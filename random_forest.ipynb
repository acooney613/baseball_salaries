{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseball = pd.read_csv('data/baseball.csv')\n",
    "\n",
    "baseball = baseball.drop(['Name', 'Age', 'Name-additional'], axis = 1)\n",
    "baseball['Salary'] = baseball['Salary'].str.replace('$', '').astype(float)\n",
    "\n",
    "baseball['C'] = baseball['Position'].apply(lambda x: 1 if 'C' in x else 0)\n",
    "baseball['1B'] = baseball['Position'].apply(lambda x: 1 if '1B' in x else 0)\n",
    "baseball['2B'] = baseball['Position'].apply(lambda x: 1 if '2B' in x else 0)\n",
    "baseball['3B'] = baseball['Position'].apply(lambda x: 1 if '3B' in x else 0)\n",
    "baseball['SS'] = baseball['Position'].apply(lambda x: 1 if 'SS' in x else 0)\n",
    "baseball['OF'] = baseball['Position'].apply(lambda x: 1 if 'OF' in x else 0)\n",
    "\n",
    "baseball['Num_Pos'] = baseball[['C', '1B', '2B', '3B', 'SS', 'OF']].sum(axis = 1)\n",
    "baseball = baseball.drop(['Position'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = baseball.drop(['Salary'], axis = 1)\n",
    "y = baseball['Salary']\n",
    "#y = (baseball['Salary'] - np.mean(baseball['Salary'])) / np.std(baseball['Salary'])\n",
    "\n",
    "cat_columns = ['Tm', 'Lg', 'Acquired', 'Bat']\n",
    "num_columns = [col for col in X.columns if col not in cat_columns + ['C', '1B', '2B', '3B', 'SS', 'OF']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#num_columns = ['Def-Inn', 'PO', 'A', 'E', 'DP', 'Fld%', 'Rdrs', 'RAA', 'WAA', 'RAR',\n",
    "#               'WAR', 'PA', 'AB', 'R', 'H', 'HR', 'RBI', 'SB', 'CS', 'BB', 'SO', 'BA', 'OBP',\n",
    "#               'SLG', 'OPS', 'OPS+', 'TB', 'GDP', 'HBP', 'SH', 'SF', 'IBB', 'Num_Pos']\n",
    "#cat_columns = ['Tm', 'Acquired']\n",
    "\n",
    "#X = X[num_columns + cat_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_transformer = Pipeline(\n",
    "    steps = [\n",
    "        ('onehot', OneHotEncoder(handle_unknown = 'ignore'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "num_transformer = Pipeline(\n",
    "    steps = [\n",
    "        ('scale', StandardScaler())\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers = [\n",
    "        ('cont', num_transformer, num_columns),\n",
    "        ('cat', cat_transformer, cat_columns)\n",
    "    ], remainder = 'passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .25, random_state = 621)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Fit Pipeline Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(\n",
    "    steps = [\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', RandomForestRegressor(n_estimators = 150, min_samples_leaf = 10))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-16 {color: black;background-color: white;}#sk-container-id-16 pre{padding: 0;}#sk-container-id-16 div.sk-toggleable {background-color: white;}#sk-container-id-16 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-16 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-16 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-16 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-16 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-16 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-16 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-16 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-16 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-16 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-16 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-16 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-16 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-16 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-16 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-16 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-16 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-16 div.sk-item {position: relative;z-index: 1;}#sk-container-id-16 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-16 div.sk-item::before, #sk-container-id-16 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-16 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-16 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-16 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-16 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-16 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-16 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-16 div.sk-label-container {text-align: center;}#sk-container-id-16 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-16 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-16\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;cont&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;scale&#x27;,\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  [&#x27;Def-Inn&#x27;, &#x27;PO&#x27;, &#x27;A&#x27;, &#x27;E&#x27;,\n",
       "                                                   &#x27;DP&#x27;, &#x27;Fld%&#x27;, &#x27;Rdrs&#x27;,\n",
       "                                                   &#x27;Season&#x27;, &#x27;RAA&#x27;, &#x27;WAA&#x27;,\n",
       "                                                   &#x27;RAR&#x27;, &#x27;WAR&#x27;, &#x27;PA&#x27;, &#x27;AB&#x27;,\n",
       "                                                   &#x27;R&#x27;, &#x27;H&#x27;, &#x27;HR&#x27;, &#x27;RBI&#x27;, &#x27;SB&#x27;,\n",
       "                                                   &#x27;CS&#x27;, &#x27;BB&#x27;, &#x27;SO&#x27;, &#x27;BA&#x27;,\n",
       "                                                   &#x27;OBP&#x27;, &#x27;SLG&#x27;, &#x27;OPS&#x27;, &#x27;OPS+&#x27;,\n",
       "                                                   &#x27;TB&#x27;, &#x27;GDP&#x27;, &#x27;HBP&#x27;, ...]),\n",
       "                                                 (&#x27;cat&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;onehot&#x27;,\n",
       "                                                                   OneHotEncoder(handle_unknown=&#x27;ignore&#x27;))]),\n",
       "                                                  [&#x27;Tm&#x27;, &#x27;Lg&#x27;, &#x27;Acquired&#x27;,\n",
       "                                                   &#x27;Bat&#x27;])])),\n",
       "                (&#x27;model&#x27;,\n",
       "                 RandomForestRegressor(min_samples_leaf=10, n_estimators=150))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-136\" type=\"checkbox\" ><label for=\"sk-estimator-id-136\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;cont&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;scale&#x27;,\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  [&#x27;Def-Inn&#x27;, &#x27;PO&#x27;, &#x27;A&#x27;, &#x27;E&#x27;,\n",
       "                                                   &#x27;DP&#x27;, &#x27;Fld%&#x27;, &#x27;Rdrs&#x27;,\n",
       "                                                   &#x27;Season&#x27;, &#x27;RAA&#x27;, &#x27;WAA&#x27;,\n",
       "                                                   &#x27;RAR&#x27;, &#x27;WAR&#x27;, &#x27;PA&#x27;, &#x27;AB&#x27;,\n",
       "                                                   &#x27;R&#x27;, &#x27;H&#x27;, &#x27;HR&#x27;, &#x27;RBI&#x27;, &#x27;SB&#x27;,\n",
       "                                                   &#x27;CS&#x27;, &#x27;BB&#x27;, &#x27;SO&#x27;, &#x27;BA&#x27;,\n",
       "                                                   &#x27;OBP&#x27;, &#x27;SLG&#x27;, &#x27;OPS&#x27;, &#x27;OPS+&#x27;,\n",
       "                                                   &#x27;TB&#x27;, &#x27;GDP&#x27;, &#x27;HBP&#x27;, ...]),\n",
       "                                                 (&#x27;cat&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;onehot&#x27;,\n",
       "                                                                   OneHotEncoder(handle_unknown=&#x27;ignore&#x27;))]),\n",
       "                                                  [&#x27;Tm&#x27;, &#x27;Lg&#x27;, &#x27;Acquired&#x27;,\n",
       "                                                   &#x27;Bat&#x27;])])),\n",
       "                (&#x27;model&#x27;,\n",
       "                 RandomForestRegressor(min_samples_leaf=10, n_estimators=150))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-137\" type=\"checkbox\" ><label for=\"sk-estimator-id-137\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessor: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                  transformers=[(&#x27;cont&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;scale&#x27;, StandardScaler())]),\n",
       "                                 [&#x27;Def-Inn&#x27;, &#x27;PO&#x27;, &#x27;A&#x27;, &#x27;E&#x27;, &#x27;DP&#x27;, &#x27;Fld%&#x27;,\n",
       "                                  &#x27;Rdrs&#x27;, &#x27;Season&#x27;, &#x27;RAA&#x27;, &#x27;WAA&#x27;, &#x27;RAR&#x27;, &#x27;WAR&#x27;,\n",
       "                                  &#x27;PA&#x27;, &#x27;AB&#x27;, &#x27;R&#x27;, &#x27;H&#x27;, &#x27;HR&#x27;, &#x27;RBI&#x27;, &#x27;SB&#x27;, &#x27;CS&#x27;,\n",
       "                                  &#x27;BB&#x27;, &#x27;SO&#x27;, &#x27;BA&#x27;, &#x27;OBP&#x27;, &#x27;SLG&#x27;, &#x27;OPS&#x27;, &#x27;OPS+&#x27;,\n",
       "                                  &#x27;TB&#x27;, &#x27;GDP&#x27;, &#x27;HBP&#x27;, ...]),\n",
       "                                (&#x27;cat&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;onehot&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;))]),\n",
       "                                 [&#x27;Tm&#x27;, &#x27;Lg&#x27;, &#x27;Acquired&#x27;, &#x27;Bat&#x27;])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-138\" type=\"checkbox\" ><label for=\"sk-estimator-id-138\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">cont</label><div class=\"sk-toggleable__content\"><pre>[&#x27;Def-Inn&#x27;, &#x27;PO&#x27;, &#x27;A&#x27;, &#x27;E&#x27;, &#x27;DP&#x27;, &#x27;Fld%&#x27;, &#x27;Rdrs&#x27;, &#x27;Season&#x27;, &#x27;RAA&#x27;, &#x27;WAA&#x27;, &#x27;RAR&#x27;, &#x27;WAR&#x27;, &#x27;PA&#x27;, &#x27;AB&#x27;, &#x27;R&#x27;, &#x27;H&#x27;, &#x27;HR&#x27;, &#x27;RBI&#x27;, &#x27;SB&#x27;, &#x27;CS&#x27;, &#x27;BB&#x27;, &#x27;SO&#x27;, &#x27;BA&#x27;, &#x27;OBP&#x27;, &#x27;SLG&#x27;, &#x27;OPS&#x27;, &#x27;OPS+&#x27;, &#x27;TB&#x27;, &#x27;GDP&#x27;, &#x27;HBP&#x27;, &#x27;SH&#x27;, &#x27;SF&#x27;, &#x27;IBB&#x27;, &#x27;Num_Pos&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-139\" type=\"checkbox\" ><label for=\"sk-estimator-id-139\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-140\" type=\"checkbox\" ><label for=\"sk-estimator-id-140\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">cat</label><div class=\"sk-toggleable__content\"><pre>[&#x27;Tm&#x27;, &#x27;Lg&#x27;, &#x27;Acquired&#x27;, &#x27;Bat&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-141\" type=\"checkbox\" ><label for=\"sk-estimator-id-141\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-142\" type=\"checkbox\" ><label for=\"sk-estimator-id-142\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">remainder</label><div class=\"sk-toggleable__content\"><pre>[&#x27;2B&#x27;, &#x27;3B&#x27;, &#x27;C&#x27;, &#x27;1B&#x27;, &#x27;SS&#x27;, &#x27;OF&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-143\" type=\"checkbox\" ><label for=\"sk-estimator-id-143\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">passthrough</label><div class=\"sk-toggleable__content\"><pre>passthrough</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-144\" type=\"checkbox\" ><label for=\"sk-estimator-id-144\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(min_samples_leaf=10, n_estimators=150)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('cont',\n",
       "                                                  Pipeline(steps=[('scale',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  ['Def-Inn', 'PO', 'A', 'E',\n",
       "                                                   'DP', 'Fld%', 'Rdrs',\n",
       "                                                   'Season', 'RAA', 'WAA',\n",
       "                                                   'RAR', 'WAR', 'PA', 'AB',\n",
       "                                                   'R', 'H', 'HR', 'RBI', 'SB',\n",
       "                                                   'CS', 'BB', 'SO', 'BA',\n",
       "                                                   'OBP', 'SLG', 'OPS', 'OPS+',\n",
       "                                                   'TB', 'GDP', 'HBP', ...]),\n",
       "                                                 ('cat',\n",
       "                                                  Pipeline(steps=[('onehot',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                  ['Tm', 'Lg', 'Acquired',\n",
       "                                                   'Bat'])])),\n",
       "                ('model',\n",
       "                 RandomForestRegressor(min_samples_leaf=10, n_estimators=150))])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mse = mean_squared_error(y_train, pipe.predict(X_train))\n",
    "test_mse = mean_squared_error(y_test, pipe.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 3990549.7988496535\n",
      "Test MSE: 5360333.8405447425\n",
      "Mean of Y: 6395365.8881033715\n"
     ]
    }
   ],
   "source": [
    "print(f'Train MSE: {np.sqrt(train_mse)}')\n",
    "print(f'Test MSE: {np.sqrt(test_mse)}')\n",
    "print(f'Mean of Y: {np.std(y)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Fit Pipeline XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(\n",
    "    steps = [\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', XGBRegressor())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-17 {color: black;background-color: white;}#sk-container-id-17 pre{padding: 0;}#sk-container-id-17 div.sk-toggleable {background-color: white;}#sk-container-id-17 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-17 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-17 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-17 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-17 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-17 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-17 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-17 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-17 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-17 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-17 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-17 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-17 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-17 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-17 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-17 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-17 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-17 div.sk-item {position: relative;z-index: 1;}#sk-container-id-17 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-17 div.sk-item::before, #sk-container-id-17 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-17 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-17 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-17 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-17 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-17 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-17 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-17 div.sk-label-container {text-align: center;}#sk-container-id-17 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-17 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-17\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;cont&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;scale&#x27;,\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  [&#x27;Def-Inn&#x27;, &#x27;PO&#x27;, &#x27;A&#x27;, &#x27;E&#x27;,\n",
       "                                                   &#x27;DP&#x27;, &#x27;Fld%&#x27;, &#x27;Rdrs&#x27;,\n",
       "                                                   &#x27;Season&#x27;, &#x27;RAA&#x27;, &#x27;WAA&#x27;,\n",
       "                                                   &#x27;RAR&#x27;, &#x27;WAR&#x27;, &#x27;PA&#x27;, &#x27;AB&#x27;,\n",
       "                                                   &#x27;R&#x27;, &#x27;H&#x27;, &#x27;HR&#x27;, &#x27;RBI&#x27;, &#x27;SB&#x27;,\n",
       "                                                   &#x27;CS&#x27;, &#x27;BB&#x27;, &#x27;SO&#x27;, &#x27;BA&#x27;,\n",
       "                                                   &#x27;OBP&#x27;, &#x27;SLG&#x27;, &#x27;OPS&#x27;, &#x27;OPS+&#x27;,\n",
       "                                                   &#x27;TB&#x27;, &#x27;GDP&#x27;, &#x27;HBP&#x27;, ...]),\n",
       "                                                 (&#x27;cat&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;onehot...\n",
       "                              feature_types=None, gamma=None, gpu_id=None,\n",
       "                              grow_policy=None, importance_type=None,\n",
       "                              interaction_constraints=None, learning_rate=None,\n",
       "                              max_bin=None, max_cat_threshold=None,\n",
       "                              max_cat_to_onehot=None, max_delta_step=None,\n",
       "                              max_depth=None, max_leaves=None,\n",
       "                              min_child_weight=None, missing=nan,\n",
       "                              monotone_constraints=None, n_estimators=100,\n",
       "                              n_jobs=None, num_parallel_tree=None,\n",
       "                              predictor=None, random_state=None, ...))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-145\" type=\"checkbox\" ><label for=\"sk-estimator-id-145\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                 ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                                   transformers=[(&#x27;cont&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;scale&#x27;,\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  [&#x27;Def-Inn&#x27;, &#x27;PO&#x27;, &#x27;A&#x27;, &#x27;E&#x27;,\n",
       "                                                   &#x27;DP&#x27;, &#x27;Fld%&#x27;, &#x27;Rdrs&#x27;,\n",
       "                                                   &#x27;Season&#x27;, &#x27;RAA&#x27;, &#x27;WAA&#x27;,\n",
       "                                                   &#x27;RAR&#x27;, &#x27;WAR&#x27;, &#x27;PA&#x27;, &#x27;AB&#x27;,\n",
       "                                                   &#x27;R&#x27;, &#x27;H&#x27;, &#x27;HR&#x27;, &#x27;RBI&#x27;, &#x27;SB&#x27;,\n",
       "                                                   &#x27;CS&#x27;, &#x27;BB&#x27;, &#x27;SO&#x27;, &#x27;BA&#x27;,\n",
       "                                                   &#x27;OBP&#x27;, &#x27;SLG&#x27;, &#x27;OPS&#x27;, &#x27;OPS+&#x27;,\n",
       "                                                   &#x27;TB&#x27;, &#x27;GDP&#x27;, &#x27;HBP&#x27;, ...]),\n",
       "                                                 (&#x27;cat&#x27;,\n",
       "                                                  Pipeline(steps=[(&#x27;onehot...\n",
       "                              feature_types=None, gamma=None, gpu_id=None,\n",
       "                              grow_policy=None, importance_type=None,\n",
       "                              interaction_constraints=None, learning_rate=None,\n",
       "                              max_bin=None, max_cat_threshold=None,\n",
       "                              max_cat_to_onehot=None, max_delta_step=None,\n",
       "                              max_depth=None, max_leaves=None,\n",
       "                              min_child_weight=None, missing=nan,\n",
       "                              monotone_constraints=None, n_estimators=100,\n",
       "                              n_jobs=None, num_parallel_tree=None,\n",
       "                              predictor=None, random_state=None, ...))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-146\" type=\"checkbox\" ><label for=\"sk-estimator-id-146\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">preprocessor: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(remainder=&#x27;passthrough&#x27;,\n",
       "                  transformers=[(&#x27;cont&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;scale&#x27;, StandardScaler())]),\n",
       "                                 [&#x27;Def-Inn&#x27;, &#x27;PO&#x27;, &#x27;A&#x27;, &#x27;E&#x27;, &#x27;DP&#x27;, &#x27;Fld%&#x27;,\n",
       "                                  &#x27;Rdrs&#x27;, &#x27;Season&#x27;, &#x27;RAA&#x27;, &#x27;WAA&#x27;, &#x27;RAR&#x27;, &#x27;WAR&#x27;,\n",
       "                                  &#x27;PA&#x27;, &#x27;AB&#x27;, &#x27;R&#x27;, &#x27;H&#x27;, &#x27;HR&#x27;, &#x27;RBI&#x27;, &#x27;SB&#x27;, &#x27;CS&#x27;,\n",
       "                                  &#x27;BB&#x27;, &#x27;SO&#x27;, &#x27;BA&#x27;, &#x27;OBP&#x27;, &#x27;SLG&#x27;, &#x27;OPS&#x27;, &#x27;OPS+&#x27;,\n",
       "                                  &#x27;TB&#x27;, &#x27;GDP&#x27;, &#x27;HBP&#x27;, ...]),\n",
       "                                (&#x27;cat&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;onehot&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;))]),\n",
       "                                 [&#x27;Tm&#x27;, &#x27;Lg&#x27;, &#x27;Acquired&#x27;, &#x27;Bat&#x27;])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-147\" type=\"checkbox\" ><label for=\"sk-estimator-id-147\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">cont</label><div class=\"sk-toggleable__content\"><pre>[&#x27;Def-Inn&#x27;, &#x27;PO&#x27;, &#x27;A&#x27;, &#x27;E&#x27;, &#x27;DP&#x27;, &#x27;Fld%&#x27;, &#x27;Rdrs&#x27;, &#x27;Season&#x27;, &#x27;RAA&#x27;, &#x27;WAA&#x27;, &#x27;RAR&#x27;, &#x27;WAR&#x27;, &#x27;PA&#x27;, &#x27;AB&#x27;, &#x27;R&#x27;, &#x27;H&#x27;, &#x27;HR&#x27;, &#x27;RBI&#x27;, &#x27;SB&#x27;, &#x27;CS&#x27;, &#x27;BB&#x27;, &#x27;SO&#x27;, &#x27;BA&#x27;, &#x27;OBP&#x27;, &#x27;SLG&#x27;, &#x27;OPS&#x27;, &#x27;OPS+&#x27;, &#x27;TB&#x27;, &#x27;GDP&#x27;, &#x27;HBP&#x27;, &#x27;SH&#x27;, &#x27;SF&#x27;, &#x27;IBB&#x27;, &#x27;Num_Pos&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-148\" type=\"checkbox\" ><label for=\"sk-estimator-id-148\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-149\" type=\"checkbox\" ><label for=\"sk-estimator-id-149\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">cat</label><div class=\"sk-toggleable__content\"><pre>[&#x27;Tm&#x27;, &#x27;Lg&#x27;, &#x27;Acquired&#x27;, &#x27;Bat&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-150\" type=\"checkbox\" ><label for=\"sk-estimator-id-150\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-151\" type=\"checkbox\" ><label for=\"sk-estimator-id-151\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">remainder</label><div class=\"sk-toggleable__content\"><pre>[&#x27;2B&#x27;, &#x27;3B&#x27;, &#x27;C&#x27;, &#x27;1B&#x27;, &#x27;SS&#x27;, &#x27;OF&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-152\" type=\"checkbox\" ><label for=\"sk-estimator-id-152\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">passthrough</label><div class=\"sk-toggleable__content\"><pre>passthrough</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-153\" type=\"checkbox\" ><label for=\"sk-estimator-id-153\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "             predictor=None, random_state=None, ...)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocessor',\n",
       "                 ColumnTransformer(remainder='passthrough',\n",
       "                                   transformers=[('cont',\n",
       "                                                  Pipeline(steps=[('scale',\n",
       "                                                                   StandardScaler())]),\n",
       "                                                  ['Def-Inn', 'PO', 'A', 'E',\n",
       "                                                   'DP', 'Fld%', 'Rdrs',\n",
       "                                                   'Season', 'RAA', 'WAA',\n",
       "                                                   'RAR', 'WAR', 'PA', 'AB',\n",
       "                                                   'R', 'H', 'HR', 'RBI', 'SB',\n",
       "                                                   'CS', 'BB', 'SO', 'BA',\n",
       "                                                   'OBP', 'SLG', 'OPS', 'OPS+',\n",
       "                                                   'TB', 'GDP', 'HBP', ...]),\n",
       "                                                 ('cat',\n",
       "                                                  Pipeline(steps=[('onehot...\n",
       "                              feature_types=None, gamma=None, gpu_id=None,\n",
       "                              grow_policy=None, importance_type=None,\n",
       "                              interaction_constraints=None, learning_rate=None,\n",
       "                              max_bin=None, max_cat_threshold=None,\n",
       "                              max_cat_to_onehot=None, max_delta_step=None,\n",
       "                              max_depth=None, max_leaves=None,\n",
       "                              min_child_weight=None, missing=nan,\n",
       "                              monotone_constraints=None, n_estimators=100,\n",
       "                              n_jobs=None, num_parallel_tree=None,\n",
       "                              predictor=None, random_state=None, ...))])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mse = mean_squared_error(y_train, pipe.predict(X_train))\n",
    "test_mse = mean_squared_error(y_test, pipe.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 996000.2897536749\n",
      "Test MSE: 5174242.088274057\n",
      "Mean of Y: 6395365.8881033715\n"
     ]
    }
   ],
   "source": [
    "print(f'Train MSE: {np.sqrt(train_mse)}')\n",
    "print(f'Test MSE: {np.sqrt(test_mse)}')\n",
    "print(f'Mean of Y: {np.std(y)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Fit CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseball = pd.read_csv('data/baseball.csv')\n",
    "\n",
    "baseball = baseball.drop(['Name', 'Age', 'Name-additional'], axis = 1)\n",
    "baseball['Salary'] = baseball['Salary'].str.replace('$', '').astype(float)\n",
    "\n",
    "baseball['C'] = baseball['Position'].apply(lambda x: 1 if 'C' in x else 0)\n",
    "baseball['1B'] = baseball['Position'].apply(lambda x: 1 if '1B' in x else 0)\n",
    "baseball['2B'] = baseball['Position'].apply(lambda x: 1 if '2B' in x else 0)\n",
    "baseball['3B'] = baseball['Position'].apply(lambda x: 1 if '3B' in x else 0)\n",
    "baseball['SS'] = baseball['Position'].apply(lambda x: 1 if 'SS' in x else 0)\n",
    "baseball['OF'] = baseball['Position'].apply(lambda x: 1 if 'OF' in x else 0)\n",
    "\n",
    "baseball['Num_Pos'] = baseball[['C', '1B', '2B', '3B', 'SS', 'OF']].sum(axis = 1)\n",
    "baseball = baseball.drop(['Position'], axis = 1)\n",
    "\n",
    "cat_columns = ['Tm', 'Lg', 'Acquired', 'Bat']\n",
    "num_columns = [col for col in X.columns if col not in cat_columns + ['C', '1B', '2B', '3B', 'SS', 'OF']]\n",
    "\n",
    "baseball = pd.get_dummies(baseball, columns = cat_columns)\n",
    "#baseball.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = baseball['Salary']\n",
    "X = baseball.drop(['Salary'], axis = 1)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, random_state = 621)\n",
    "\n",
    "min_max_scalar = MinMaxScaler()\n",
    "X_train = min_max_scalar.fit_transform(X_train)\n",
    "X_test = min_max_scalar.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 1000)              88000     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 500)               500500    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 500)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 250)               125250    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 250)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 251       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 714001 (2.72 MB)\n",
      "Trainable params: 714001 (2.72 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1000, input_shape = (X_train.shape[1], ), activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(500, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(250, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation = 'linear'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'mse', metrics = ['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor = 'val_loss', mode = 'min', patience = 50, restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000\n",
      "81/81 [==============================] - 3s 21ms/step - loss: 65458166300672.0000 - mae: 4985864.5000 - val_loss: 65352918630400.0000 - val_mae: 4849592.0000\n",
      "Epoch 2/5000\n",
      "81/81 [==============================] - 1s 17ms/step - loss: 52406767321088.0000 - mae: 4311260.0000 - val_loss: 39986158108672.0000 - val_mae: 4401210.5000\n",
      "Epoch 3/5000\n",
      "81/81 [==============================] - 1s 17ms/step - loss: 37548873220096.0000 - mae: 4506814.5000 - val_loss: 38112038223872.0000 - val_mae: 4615650.0000\n",
      "Epoch 4/5000\n",
      "81/81 [==============================] - 1s 17ms/step - loss: 36517028298752.0000 - mae: 4466976.5000 - val_loss: 37347563405312.0000 - val_mae: 4535641.5000\n",
      "Epoch 5/5000\n",
      "81/81 [==============================] - 2s 22ms/step - loss: 35711050842112.0000 - mae: 4419239.0000 - val_loss: 36618111025152.0000 - val_mae: 4538241.5000\n",
      "Epoch 6/5000\n",
      "81/81 [==============================] - 1s 18ms/step - loss: 35180223922176.0000 - mae: 4383656.5000 - val_loss: 35963971567616.0000 - val_mae: 4427062.0000\n",
      "Epoch 7/5000\n",
      "81/81 [==============================] - 2s 21ms/step - loss: 34548224098304.0000 - mae: 4300473.0000 - val_loss: 35271974322176.0000 - val_mae: 4394234.5000\n",
      "Epoch 8/5000\n",
      "81/81 [==============================] - 2s 19ms/step - loss: 33790548246528.0000 - mae: 4269873.5000 - val_loss: 34646555361280.0000 - val_mae: 4271401.0000\n",
      "Epoch 9/5000\n",
      "81/81 [==============================] - 2s 20ms/step - loss: 33210505363456.0000 - mae: 4192002.5000 - val_loss: 33891127656448.0000 - val_mae: 4243382.0000\n",
      "Epoch 10/5000\n",
      "81/81 [==============================] - 2s 22ms/step - loss: 32633620791296.0000 - mae: 4167841.0000 - val_loss: 33262676213760.0000 - val_mae: 4114097.0000\n",
      "Epoch 11/5000\n",
      "81/81 [==============================] - 2s 20ms/step - loss: 32116073037824.0000 - mae: 4041883.0000 - val_loss: 32500508262400.0000 - val_mae: 4095187.0000\n",
      "Epoch 12/5000\n",
      "81/81 [==============================] - 2s 22ms/step - loss: 31431839449088.0000 - mae: 3988819.0000 - val_loss: 31821590953984.0000 - val_mae: 4053973.2500\n",
      "Epoch 13/5000\n",
      "81/81 [==============================] - 2s 21ms/step - loss: 30737076060160.0000 - mae: 3956611.5000 - val_loss: 31208784265216.0000 - val_mae: 3925120.2500\n",
      "Epoch 14/5000\n",
      "81/81 [==============================] - 2s 21ms/step - loss: 30028964298752.0000 - mae: 3830216.7500 - val_loss: 30576325165056.0000 - val_mae: 3961811.5000\n",
      "Epoch 15/5000\n",
      "81/81 [==============================] - 2s 23ms/step - loss: 29567571984384.0000 - mae: 3801741.5000 - val_loss: 30008418500608.0000 - val_mae: 3843532.7500\n",
      "Epoch 16/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 28935165313024.0000 - mae: 3771700.5000 - val_loss: 29491975946240.0000 - val_mae: 3808105.5000\n",
      "Epoch 17/5000\n",
      "81/81 [==============================] - 2s 21ms/step - loss: 28952538120192.0000 - mae: 3728790.2500 - val_loss: 29079623434240.0000 - val_mae: 3734292.2500\n",
      "Epoch 18/5000\n",
      "81/81 [==============================] - 2s 25ms/step - loss: 28486437699584.0000 - mae: 3661595.0000 - val_loss: 28674900361216.0000 - val_mae: 3713829.5000\n",
      "Epoch 19/5000\n",
      "81/81 [==============================] - 2s 23ms/step - loss: 28091164393472.0000 - mae: 3658371.7500 - val_loss: 28351630671872.0000 - val_mae: 3657476.2500\n",
      "Epoch 20/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 27787211571200.0000 - mae: 3622747.0000 - val_loss: 28007867613184.0000 - val_mae: 3675815.0000\n",
      "Epoch 21/5000\n",
      "81/81 [==============================] - 2s 22ms/step - loss: 27492431691776.0000 - mae: 3610169.5000 - val_loss: 27754026237952.0000 - val_mae: 3632165.2500\n",
      "Epoch 22/5000\n",
      "81/81 [==============================] - 2s 23ms/step - loss: 27306380754944.0000 - mae: 3593127.0000 - val_loss: 27552521388032.0000 - val_mae: 3556319.0000\n",
      "Epoch 23/5000\n",
      "81/81 [==============================] - 2s 25ms/step - loss: 27074993586176.0000 - mae: 3581661.2500 - val_loss: 27402562437120.0000 - val_mae: 3514669.7500\n",
      "Epoch 24/5000\n",
      "81/81 [==============================] - 2s 23ms/step - loss: 26772871577600.0000 - mae: 3520535.0000 - val_loss: 27066573520896.0000 - val_mae: 3559622.2500\n",
      "Epoch 25/5000\n",
      "81/81 [==============================] - 2s 23ms/step - loss: 26844397043712.0000 - mae: 3531377.7500 - val_loss: 26901859008512.0000 - val_mae: 3556721.7500\n",
      "Epoch 26/5000\n",
      "81/81 [==============================] - 2s 25ms/step - loss: 26480100769792.0000 - mae: 3523551.0000 - val_loss: 26764803833856.0000 - val_mae: 3509392.2500\n",
      "Epoch 27/5000\n",
      "81/81 [==============================] - 2s 23ms/step - loss: 26392855052288.0000 - mae: 3505734.2500 - val_loss: 26615255924736.0000 - val_mae: 3554881.2500\n",
      "Epoch 28/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 26083227336704.0000 - mae: 3479042.7500 - val_loss: 26455129980928.0000 - val_mae: 3532183.7500\n",
      "Epoch 29/5000\n",
      "81/81 [==============================] - 2s 23ms/step - loss: 25995352473600.0000 - mae: 3494510.5000 - val_loss: 26335286132736.0000 - val_mae: 3497945.2500\n",
      "Epoch 30/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 26144189448192.0000 - mae: 3491322.0000 - val_loss: 26273455800320.0000 - val_mae: 3456271.2500\n",
      "Epoch 31/5000\n",
      "81/81 [==============================] - 2s 25ms/step - loss: 25738851909632.0000 - mae: 3473770.2500 - val_loss: 26148306157568.0000 - val_mae: 3470078.7500\n",
      "Epoch 32/5000\n",
      "81/81 [==============================] - 2s 25ms/step - loss: 25739233591296.0000 - mae: 3459210.0000 - val_loss: 26076822634496.0000 - val_mae: 3445403.5000\n",
      "Epoch 33/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 25804534710272.0000 - mae: 3462751.7500 - val_loss: 25960260829184.0000 - val_mae: 3473395.0000\n",
      "Epoch 34/5000\n",
      "81/81 [==============================] - 2s 25ms/step - loss: 25379790127104.0000 - mae: 3458428.2500 - val_loss: 25902253604864.0000 - val_mae: 3438805.0000\n",
      "Epoch 35/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 25549682507776.0000 - mae: 3446555.2500 - val_loss: 25811941851136.0000 - val_mae: 3452420.2500\n",
      "Epoch 36/5000\n",
      "81/81 [==============================] - 2s 25ms/step - loss: 25331234766848.0000 - mae: 3449020.2500 - val_loss: 25794665512960.0000 - val_mae: 3408722.0000\n",
      "Epoch 37/5000\n",
      "81/81 [==============================] - 2s 24ms/step - loss: 25452261408768.0000 - mae: 3411939.7500 - val_loss: 25685632483328.0000 - val_mae: 3469468.0000\n",
      "Epoch 38/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 25145190121472.0000 - mae: 3434684.5000 - val_loss: 25734814892032.0000 - val_mae: 3391559.2500\n",
      "Epoch 39/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 25261701595136.0000 - mae: 3428611.0000 - val_loss: 25556059947008.0000 - val_mae: 3446044.2500\n",
      "Epoch 40/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 25057256538112.0000 - mae: 3418997.0000 - val_loss: 25554057166848.0000 - val_mae: 3407692.7500\n",
      "Epoch 41/5000\n",
      "81/81 [==============================] - 2s 31ms/step - loss: 24953493651456.0000 - mae: 3404499.0000 - val_loss: 25474994536448.0000 - val_mae: 3416225.5000\n",
      "Epoch 42/5000\n",
      "81/81 [==============================] - 3s 31ms/step - loss: 25177635160064.0000 - mae: 3426286.0000 - val_loss: 25433317834752.0000 - val_mae: 3413963.5000\n",
      "Epoch 43/5000\n",
      "81/81 [==============================] - 2s 25ms/step - loss: 24939669225472.0000 - mae: 3421023.0000 - val_loss: 25380001939456.0000 - val_mae: 3416379.2500\n",
      "Epoch 44/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 24892351184896.0000 - mae: 3406382.2500 - val_loss: 25329779343360.0000 - val_mae: 3444018.7500\n",
      "Epoch 45/5000\n",
      "81/81 [==============================] - 2s 25ms/step - loss: 24811642290176.0000 - mae: 3425387.5000 - val_loss: 25382373818368.0000 - val_mae: 3369084.2500\n",
      "Epoch 46/5000\n",
      "81/81 [==============================] - 2s 25ms/step - loss: 24706046492672.0000 - mae: 3381343.7500 - val_loss: 25237714370560.0000 - val_mae: 3440113.0000\n",
      "Epoch 47/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 24809469640704.0000 - mae: 3406748.2500 - val_loss: 25223774601216.0000 - val_mae: 3392392.0000\n",
      "Epoch 48/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 24727242407936.0000 - mae: 3377677.7500 - val_loss: 25173042397184.0000 - val_mae: 3452337.7500\n",
      "Epoch 49/5000\n",
      "81/81 [==============================] - 3s 35ms/step - loss: 24598793945088.0000 - mae: 3413106.5000 - val_loss: 25122964504576.0000 - val_mae: 3434234.2500\n",
      "Epoch 50/5000\n",
      "81/81 [==============================] - 3s 37ms/step - loss: 24555846369280.0000 - mae: 3387549.5000 - val_loss: 25070191771648.0000 - val_mae: 3400451.0000\n",
      "Epoch 51/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 24643675095040.0000 - mae: 3382256.5000 - val_loss: 25031446888448.0000 - val_mae: 3402123.0000\n",
      "Epoch 52/5000\n",
      "81/81 [==============================] - 2s 30ms/step - loss: 24520702296064.0000 - mae: 3388300.2500 - val_loss: 24988084076544.0000 - val_mae: 3401192.7500\n",
      "Epoch 53/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 24629976498176.0000 - mae: 3401804.2500 - val_loss: 24961001455616.0000 - val_mae: 3401054.5000\n",
      "Epoch 54/5000\n",
      "81/81 [==============================] - 2s 25ms/step - loss: 24730587365376.0000 - mae: 3395004.7500 - val_loss: 24940757647360.0000 - val_mae: 3400716.7500\n",
      "Epoch 55/5000\n",
      "81/81 [==============================] - 2s 30ms/step - loss: 24439645274112.0000 - mae: 3373026.2500 - val_loss: 24901631082496.0000 - val_mae: 3388131.0000\n",
      "Epoch 56/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 24612247175168.0000 - mae: 3392059.2500 - val_loss: 24889473892352.0000 - val_mae: 3378896.0000\n",
      "Epoch 57/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 24203275272192.0000 - mae: 3355773.0000 - val_loss: 24825191989248.0000 - val_mae: 3406183.0000\n",
      "Epoch 58/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 23928040849408.0000 - mae: 3370219.0000 - val_loss: 24868567384064.0000 - val_mae: 3349506.5000\n",
      "Epoch 59/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 24328311668736.0000 - mae: 3369220.7500 - val_loss: 24840798994432.0000 - val_mae: 3343328.0000\n",
      "Epoch 60/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 24182473621504.0000 - mae: 3365489.7500 - val_loss: 24719388573696.0000 - val_mae: 3377839.5000\n",
      "Epoch 61/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 24199412318208.0000 - mae: 3361260.0000 - val_loss: 24691267862528.0000 - val_mae: 3372693.5000\n",
      "Epoch 62/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 23856259530752.0000 - mae: 3331591.7500 - val_loss: 24691924271104.0000 - val_mae: 3356299.0000\n",
      "Epoch 63/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 24059190444032.0000 - mae: 3351936.5000 - val_loss: 24658560679936.0000 - val_mae: 3357766.0000\n",
      "Epoch 64/5000\n",
      "81/81 [==============================] - 3s 39ms/step - loss: 23816730312704.0000 - mae: 3338337.2500 - val_loss: 24631817797632.0000 - val_mae: 3413304.7500\n",
      "Epoch 65/5000\n",
      "81/81 [==============================] - 3s 39ms/step - loss: 23718791217152.0000 - mae: 3349085.7500 - val_loss: 24596438843392.0000 - val_mae: 3368539.2500\n",
      "Epoch 66/5000\n",
      "81/81 [==============================] - 3s 43ms/step - loss: 23590474874880.0000 - mae: 3341395.0000 - val_loss: 24582436159488.0000 - val_mae: 3394630.0000\n",
      "Epoch 67/5000\n",
      "81/81 [==============================] - 2s 31ms/step - loss: 23975637811200.0000 - mae: 3360548.5000 - val_loss: 24592198402048.0000 - val_mae: 3335271.7500\n",
      "Epoch 68/5000\n",
      "81/81 [==============================] - 3s 32ms/step - loss: 23762497961984.0000 - mae: 3316317.7500 - val_loss: 24517265063936.0000 - val_mae: 3372219.0000\n",
      "Epoch 69/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 23939378053120.0000 - mae: 3355870.5000 - val_loss: 24478885085184.0000 - val_mae: 3372839.5000\n",
      "Epoch 70/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 23830416326656.0000 - mae: 3341504.7500 - val_loss: 24482949365760.0000 - val_mae: 3343967.2500\n",
      "Epoch 71/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 23421633167360.0000 - mae: 3316979.7500 - val_loss: 24449841627136.0000 - val_mae: 3394205.5000\n",
      "Epoch 72/5000\n",
      "81/81 [==============================] - 2s 31ms/step - loss: 23641857196032.0000 - mae: 3337609.0000 - val_loss: 24430195507200.0000 - val_mae: 3344586.0000\n",
      "Epoch 73/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 23617064665088.0000 - mae: 3310152.2500 - val_loss: 24384632782848.0000 - val_mae: 3345793.0000\n",
      "Epoch 74/5000\n",
      "81/81 [==============================] - 2s 30ms/step - loss: 23639934107648.0000 - mae: 3317103.2500 - val_loss: 24397903560704.0000 - val_mae: 3339815.7500\n",
      "Epoch 75/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 23313950703616.0000 - mae: 3311286.0000 - val_loss: 24364084887552.0000 - val_mae: 3341158.2500\n",
      "Epoch 76/5000\n",
      "81/81 [==============================] - 2s 30ms/step - loss: 23551392350208.0000 - mae: 3321270.0000 - val_loss: 24314954907648.0000 - val_mae: 3377653.5000\n",
      "Epoch 77/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 23574182100992.0000 - mae: 3328318.0000 - val_loss: 24417281245184.0000 - val_mae: 3305754.0000\n",
      "Epoch 78/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 23431038894080.0000 - mae: 3309517.7500 - val_loss: 24254978457600.0000 - val_mae: 3349487.7500\n",
      "Epoch 79/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 23452698279936.0000 - mae: 3337939.7500 - val_loss: 24306866192384.0000 - val_mae: 3313711.7500\n",
      "Epoch 80/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 23283040780288.0000 - mae: 3292028.0000 - val_loss: 24221107355648.0000 - val_mae: 3330402.2500\n",
      "Epoch 81/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 23357537910784.0000 - mae: 3298038.2500 - val_loss: 24207784148992.0000 - val_mae: 3386821.0000\n",
      "Epoch 82/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 23276149538816.0000 - mae: 3312375.7500 - val_loss: 24174135345152.0000 - val_mae: 3335579.5000\n",
      "Epoch 83/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 23104147423232.0000 - mae: 3279479.2500 - val_loss: 24136279654400.0000 - val_mae: 3336331.5000\n",
      "Epoch 84/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 23430808207360.0000 - mae: 3317136.5000 - val_loss: 24147635732480.0000 - val_mae: 3377308.7500\n",
      "Epoch 85/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 23219371245568.0000 - mae: 3288813.2500 - val_loss: 24060490678272.0000 - val_mae: 3344217.5000\n",
      "Epoch 86/5000\n",
      "81/81 [==============================] - 2s 30ms/step - loss: 23061359230976.0000 - mae: 3291403.7500 - val_loss: 24028723019776.0000 - val_mae: 3339538.5000\n",
      "Epoch 87/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 23100127182848.0000 - mae: 3304537.2500 - val_loss: 24015003451392.0000 - val_mae: 3325915.5000\n",
      "Epoch 88/5000\n",
      "81/81 [==============================] - 2s 30ms/step - loss: 23134174445568.0000 - mae: 3286818.0000 - val_loss: 24010316316672.0000 - val_mae: 3306408.0000\n",
      "Epoch 89/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 23065457065984.0000 - mae: 3293653.5000 - val_loss: 24019204046848.0000 - val_mae: 3369143.5000\n",
      "Epoch 90/5000\n",
      "81/81 [==============================] - 2s 30ms/step - loss: 23113989357568.0000 - mae: 3284360.7500 - val_loss: 23940296605696.0000 - val_mae: 3316852.0000\n",
      "Epoch 91/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 22964714078208.0000 - mae: 3279973.0000 - val_loss: 23953015832576.0000 - val_mae: 3365736.2500\n",
      "Epoch 92/5000\n",
      "81/81 [==============================] - 2s 30ms/step - loss: 23020443795456.0000 - mae: 3292742.2500 - val_loss: 23871518408704.0000 - val_mae: 3314309.0000\n",
      "Epoch 93/5000\n",
      "81/81 [==============================] - 2s 31ms/step - loss: 22992948035584.0000 - mae: 3264221.2500 - val_loss: 23887368683520.0000 - val_mae: 3300346.2500\n",
      "Epoch 94/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 22821921095680.0000 - mae: 3284247.7500 - val_loss: 23817357361152.0000 - val_mae: 3328443.7500\n",
      "Epoch 95/5000\n",
      "81/81 [==============================] - 2s 30ms/step - loss: 22898450366464.0000 - mae: 3273986.5000 - val_loss: 23805472800768.0000 - val_mae: 3320528.2500\n",
      "Epoch 96/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 22769595056128.0000 - mae: 3266817.2500 - val_loss: 23778551660544.0000 - val_mae: 3309360.5000\n",
      "Epoch 97/5000\n",
      "81/81 [==============================] - 2s 30ms/step - loss: 22788110811136.0000 - mae: 3271229.7500 - val_loss: 23774080532480.0000 - val_mae: 3310119.0000\n",
      "Epoch 98/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 22570468376576.0000 - mae: 3259851.7500 - val_loss: 23773665296384.0000 - val_mae: 3315395.5000\n",
      "Epoch 99/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 22498974367744.0000 - mae: 3232408.5000 - val_loss: 23710325014528.0000 - val_mae: 3318100.7500\n",
      "Epoch 100/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 22335901925376.0000 - mae: 3244321.2500 - val_loss: 23999578898432.0000 - val_mae: 3411920.7500\n",
      "Epoch 101/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 22396620767232.0000 - mae: 3260447.2500 - val_loss: 23703974838272.0000 - val_mae: 3273575.5000\n",
      "Epoch 102/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 22170946240512.0000 - mae: 3230955.0000 - val_loss: 23627680448512.0000 - val_mae: 3308509.7500\n",
      "Epoch 103/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 22444414861312.0000 - mae: 3250630.7500 - val_loss: 23613080076288.0000 - val_mae: 3328220.2500\n",
      "Epoch 104/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 22390132178944.0000 - mae: 3241046.7500 - val_loss: 23550857576448.0000 - val_mae: 3328606.0000\n",
      "Epoch 105/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 22455882088448.0000 - mae: 3250128.0000 - val_loss: 23536414490624.0000 - val_mae: 3293091.2500\n",
      "Epoch 106/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 22161360158720.0000 - mae: 3218020.5000 - val_loss: 23467323817984.0000 - val_mae: 3309901.7500\n",
      "Epoch 107/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 22166196191232.0000 - mae: 3245804.7500 - val_loss: 23578821001216.0000 - val_mae: 3246573.5000\n",
      "Epoch 108/5000\n",
      "81/81 [==============================] - 3s 32ms/step - loss: 22118685212672.0000 - mae: 3224344.5000 - val_loss: 23473239883776.0000 - val_mae: 3305617.2500\n",
      "Epoch 109/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 22354765807616.0000 - mae: 3226276.0000 - val_loss: 23510365765632.0000 - val_mae: 3337396.5000\n",
      "Epoch 110/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 21804252921856.0000 - mae: 3199806.0000 - val_loss: 23376791863296.0000 - val_mae: 3276978.2500\n",
      "Epoch 111/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 21731337043968.0000 - mae: 3215308.0000 - val_loss: 23389066493952.0000 - val_mae: 3313698.0000\n",
      "Epoch 112/5000\n",
      "81/81 [==============================] - 2s 30ms/step - loss: 22135093329920.0000 - mae: 3230667.0000 - val_loss: 23379149062144.0000 - val_mae: 3257751.7500\n",
      "Epoch 113/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 21867217813504.0000 - mae: 3211585.0000 - val_loss: 23352666226688.0000 - val_mae: 3267125.2500\n",
      "Epoch 114/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 21725983014912.0000 - mae: 3200106.0000 - val_loss: 23278678704128.0000 - val_mae: 3281063.7500\n",
      "Epoch 115/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 21922184167424.0000 - mae: 3222352.5000 - val_loss: 23344856432640.0000 - val_mae: 3272262.0000\n",
      "Epoch 116/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 21473001472000.0000 - mae: 3170673.0000 - val_loss: 23238159630336.0000 - val_mae: 3255122.5000\n",
      "Epoch 117/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 21462377299968.0000 - mae: 3177295.2500 - val_loss: 23237245272064.0000 - val_mae: 3269292.2500\n",
      "Epoch 118/5000\n",
      "81/81 [==============================] - 3s 31ms/step - loss: 21737917906944.0000 - mae: 3177944.0000 - val_loss: 23222059794432.0000 - val_mae: 3295629.2500\n",
      "Epoch 119/5000\n",
      "81/81 [==============================] - 2s 31ms/step - loss: 21574950322176.0000 - mae: 3208700.5000 - val_loss: 23215506194432.0000 - val_mae: 3238428.0000\n",
      "Epoch 120/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 21157268946944.0000 - mae: 3152668.7500 - val_loss: 23216263266304.0000 - val_mae: 3226121.5000\n",
      "Epoch 121/5000\n",
      "81/81 [==============================] - 2s 31ms/step - loss: 21519184953344.0000 - mae: 3180902.7500 - val_loss: 23112064172032.0000 - val_mae: 3267964.5000\n",
      "Epoch 122/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 21272283054080.0000 - mae: 3170509.5000 - val_loss: 23128174493696.0000 - val_mae: 3265070.7500\n",
      "Epoch 123/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 21253148639232.0000 - mae: 3171064.2500 - val_loss: 23144188346368.0000 - val_mae: 3230061.7500\n",
      "Epoch 124/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 21205297922048.0000 - mae: 3153966.5000 - val_loss: 23062791585792.0000 - val_mae: 3239607.2500\n",
      "Epoch 125/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 21092305469440.0000 - mae: 3171951.0000 - val_loss: 23043246129152.0000 - val_mae: 3233986.7500\n",
      "Epoch 126/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 20956636512256.0000 - mae: 3128698.2500 - val_loss: 23040419168256.0000 - val_mae: 3275622.0000\n",
      "Epoch 127/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 20858212974592.0000 - mae: 3142470.2500 - val_loss: 23031554506752.0000 - val_mae: 3279768.7500\n",
      "Epoch 128/5000\n",
      "81/81 [==============================] - 2s 30ms/step - loss: 20869768282112.0000 - mae: 3141841.5000 - val_loss: 22983873658880.0000 - val_mae: 3220479.2500\n",
      "Epoch 129/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 21020975038464.0000 - mae: 3138716.0000 - val_loss: 22932153696256.0000 - val_mae: 3224797.7500\n",
      "Epoch 130/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 20600139546624.0000 - mae: 3132410.5000 - val_loss: 22956944130048.0000 - val_mae: 3216293.7500\n",
      "Epoch 131/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 20772603035648.0000 - mae: 3112055.2500 - val_loss: 22946932326400.0000 - val_mae: 3268579.5000\n",
      "Epoch 132/5000\n",
      "81/81 [==============================] - 2s 30ms/step - loss: 20686984708096.0000 - mae: 3135474.0000 - val_loss: 22978697887744.0000 - val_mae: 3189129.0000\n",
      "Epoch 133/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 20480608174080.0000 - mae: 3106046.0000 - val_loss: 22819209478144.0000 - val_mae: 3250979.5000\n",
      "Epoch 134/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 20473008095232.0000 - mae: 3120467.2500 - val_loss: 22834390761472.0000 - val_mae: 3229449.0000\n",
      "Epoch 135/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 20561595990016.0000 - mae: 3093093.7500 - val_loss: 22788863688704.0000 - val_mae: 3220189.7500\n",
      "Epoch 136/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 20770658975744.0000 - mae: 3123487.7500 - val_loss: 22840854183936.0000 - val_mae: 3197275.2500\n",
      "Epoch 137/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 20290975301632.0000 - mae: 3089672.7500 - val_loss: 22828478889984.0000 - val_mae: 3265079.2500\n",
      "Epoch 138/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 20340245790720.0000 - mae: 3103922.2500 - val_loss: 22747560280064.0000 - val_mae: 3217527.2500\n",
      "Epoch 139/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 20200147648512.0000 - mae: 3077695.0000 - val_loss: 22708299497472.0000 - val_mae: 3217164.5000\n",
      "Epoch 140/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 20496877879296.0000 - mae: 3109955.0000 - val_loss: 22684693954560.0000 - val_mae: 3220089.5000\n",
      "Epoch 141/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 20131092627456.0000 - mae: 3081835.0000 - val_loss: 22742885728256.0000 - val_mae: 3253325.7500\n",
      "Epoch 142/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 20025448595456.0000 - mae: 3080607.2500 - val_loss: 22630432243712.0000 - val_mae: 3234312.5000\n",
      "Epoch 143/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 20000729464832.0000 - mae: 3067730.0000 - val_loss: 22649130450944.0000 - val_mae: 3211090.2500\n",
      "Epoch 144/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 19567826960384.0000 - mae: 3052113.0000 - val_loss: 22632191754240.0000 - val_mae: 3212449.7500\n",
      "Epoch 145/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 20095730450432.0000 - mae: 3076001.5000 - val_loss: 22585595133952.0000 - val_mae: 3190614.0000\n",
      "Epoch 146/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 19751904477184.0000 - mae: 3053207.7500 - val_loss: 22610221989888.0000 - val_mae: 3232872.5000\n",
      "Epoch 147/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 19668083408896.0000 - mae: 3049652.0000 - val_loss: 22592891125760.0000 - val_mae: 3236568.0000\n",
      "Epoch 148/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 19839831769088.0000 - mae: 3053690.5000 - val_loss: 22510921842688.0000 - val_mae: 3217538.7500\n",
      "Epoch 149/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 19866232815616.0000 - mae: 3055260.5000 - val_loss: 22551786946560.0000 - val_mae: 3207119.0000\n",
      "Epoch 150/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 19560438693888.0000 - mae: 3033073.2500 - val_loss: 22515516702720.0000 - val_mae: 3216482.2500\n",
      "Epoch 151/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 19370757586944.0000 - mae: 3026454.0000 - val_loss: 22466541912064.0000 - val_mae: 3188193.5000\n",
      "Epoch 152/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 19474925223936.0000 - mae: 3030349.7500 - val_loss: 22441841655808.0000 - val_mae: 3190357.5000\n",
      "Epoch 153/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 19556057743360.0000 - mae: 3021619.5000 - val_loss: 22406366232576.0000 - val_mae: 3184998.5000\n",
      "Epoch 154/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 19444889812992.0000 - mae: 3031508.2500 - val_loss: 22388116815872.0000 - val_mae: 3182675.5000\n",
      "Epoch 155/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 19078775308288.0000 - mae: 3006209.5000 - val_loss: 22315846860800.0000 - val_mae: 3164858.0000\n",
      "Epoch 156/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 19091370803200.0000 - mae: 3001615.2500 - val_loss: 22328658362368.0000 - val_mae: 3172564.2500\n",
      "Epoch 157/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 19100518580224.0000 - mae: 3014245.2500 - val_loss: 22440275083264.0000 - val_mae: 3240580.0000\n",
      "Epoch 158/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 19197713186816.0000 - mae: 3023242.0000 - val_loss: 22312581595136.0000 - val_mae: 3181918.0000\n",
      "Epoch 159/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 19135201280000.0000 - mae: 3012558.2500 - val_loss: 22329845350400.0000 - val_mae: 3189486.2500\n",
      "Epoch 160/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 19140045701120.0000 - mae: 3002139.2500 - val_loss: 22278660161536.0000 - val_mae: 3171668.0000\n",
      "Epoch 161/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 19220347748352.0000 - mae: 2998119.2500 - val_loss: 22291184353280.0000 - val_mae: 3184310.7500\n",
      "Epoch 162/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 18966384738304.0000 - mae: 2994561.5000 - val_loss: 22283397627904.0000 - val_mae: 3165989.2500\n",
      "Epoch 163/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 18626910355456.0000 - mae: 2959065.2500 - val_loss: 22286920843264.0000 - val_mae: 3211120.2500\n",
      "Epoch 164/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 18841522405376.0000 - mae: 2982324.5000 - val_loss: 22229079293952.0000 - val_mae: 3195242.5000\n",
      "Epoch 165/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 18642586566656.0000 - mae: 2971395.0000 - val_loss: 22258923864064.0000 - val_mae: 3168470.0000\n",
      "Epoch 166/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 18696208646144.0000 - mae: 2979724.2500 - val_loss: 22305648410624.0000 - val_mae: 3203720.7500\n",
      "Epoch 167/5000\n",
      "81/81 [==============================] - 3s 39ms/step - loss: 18485325332480.0000 - mae: 2975759.7500 - val_loss: 22214883672064.0000 - val_mae: 3178799.7500\n",
      "Epoch 168/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 18583115530240.0000 - mae: 2964963.5000 - val_loss: 22228664057856.0000 - val_mae: 3211083.7500\n",
      "Epoch 169/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 18472994078720.0000 - mae: 2941982.0000 - val_loss: 22116820844544.0000 - val_mae: 3184660.5000\n",
      "Epoch 170/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 18260848279552.0000 - mae: 2952975.2500 - val_loss: 22235152646144.0000 - val_mae: 3224688.5000\n",
      "Epoch 171/5000\n",
      "81/81 [==============================] - 3s 42ms/step - loss: 18206397825024.0000 - mae: 2930463.0000 - val_loss: 22154911416320.0000 - val_mae: 3183938.7500\n",
      "Epoch 172/5000\n",
      "81/81 [==============================] - 3s 40ms/step - loss: 18218991222784.0000 - mae: 2935573.0000 - val_loss: 22360853839872.0000 - val_mae: 3249086.0000\n",
      "Epoch 173/5000\n",
      "81/81 [==============================] - 2s 24ms/step - loss: 18189444448256.0000 - mae: 2932036.7500 - val_loss: 22120362934272.0000 - val_mae: 3159234.0000\n",
      "Epoch 174/5000\n",
      "81/81 [==============================] - 2s 24ms/step - loss: 17918632919040.0000 - mae: 2919270.7500 - val_loss: 22069813182464.0000 - val_mae: 3163800.2500\n",
      "Epoch 175/5000\n",
      "81/81 [==============================] - 2s 22ms/step - loss: 17995879415808.0000 - mae: 2910999.0000 - val_loss: 22040224464896.0000 - val_mae: 3156483.0000\n",
      "Epoch 176/5000\n",
      "81/81 [==============================] - 2s 23ms/step - loss: 17710310227968.0000 - mae: 2908392.0000 - val_loss: 22024101560320.0000 - val_mae: 3150891.2500\n",
      "Epoch 177/5000\n",
      "81/81 [==============================] - 2s 22ms/step - loss: 17680788619264.0000 - mae: 2889639.7500 - val_loss: 22071994220544.0000 - val_mae: 3169165.0000\n",
      "Epoch 178/5000\n",
      "81/81 [==============================] - 3s 36ms/step - loss: 17948074835968.0000 - mae: 2914213.7500 - val_loss: 22046429937664.0000 - val_mae: 3170063.5000\n",
      "Epoch 179/5000\n",
      "81/81 [==============================] - 3s 32ms/step - loss: 17879141449728.0000 - mae: 2908849.5000 - val_loss: 22140627714048.0000 - val_mae: 3213782.5000\n",
      "Epoch 180/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 17714282233856.0000 - mae: 2907691.5000 - val_loss: 22066090737664.0000 - val_mae: 3176813.5000\n",
      "Epoch 181/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 18093424246784.0000 - mae: 2909623.5000 - val_loss: 22038995533824.0000 - val_mae: 3186635.7500\n",
      "Epoch 182/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 17633642545152.0000 - mae: 2898605.0000 - val_loss: 21970116673536.0000 - val_mae: 3152414.0000\n",
      "Epoch 183/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 17732617633792.0000 - mae: 2883771.5000 - val_loss: 22200644009984.0000 - val_mae: 3227110.7500\n",
      "Epoch 184/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 17565017440256.0000 - mae: 2874868.7500 - val_loss: 21958169198592.0000 - val_mae: 3150434.7500\n",
      "Epoch 185/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 17931356340224.0000 - mae: 2903529.5000 - val_loss: 21934435729408.0000 - val_mae: 3150033.7500\n",
      "Epoch 186/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 17715190300672.0000 - mae: 2890727.7500 - val_loss: 21886769561600.0000 - val_mae: 3140155.7500\n",
      "Epoch 187/5000\n",
      "81/81 [==============================] - 2s 30ms/step - loss: 17226112434176.0000 - mae: 2852181.5000 - val_loss: 21899065163776.0000 - val_mae: 3178142.7500\n",
      "Epoch 188/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 17657942245376.0000 - mae: 2893826.2500 - val_loss: 21925027905536.0000 - val_mae: 3165363.5000\n",
      "Epoch 189/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 17469012967424.0000 - mae: 2868372.0000 - val_loss: 21910708551680.0000 - val_mae: 3143835.5000\n",
      "Epoch 190/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 17045371486208.0000 - mae: 2839973.7500 - val_loss: 21943940022272.0000 - val_mae: 3169005.5000\n",
      "Epoch 191/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 17128573894656.0000 - mae: 2857003.5000 - val_loss: 21861423382528.0000 - val_mae: 3143476.2500\n",
      "Epoch 192/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 16963340337152.0000 - mae: 2829559.2500 - val_loss: 21859294773248.0000 - val_mae: 3148589.0000\n",
      "Epoch 193/5000\n",
      "81/81 [==============================] - 3s 34ms/step - loss: 17093902729216.0000 - mae: 2849982.7500 - val_loss: 21838944010240.0000 - val_mae: 3131397.5000\n",
      "Epoch 194/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 16494482161664.0000 - mae: 2807873.5000 - val_loss: 21990199001088.0000 - val_mae: 3173908.7500\n",
      "Epoch 195/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 17100907216896.0000 - mae: 2855053.0000 - val_loss: 22068789772288.0000 - val_mae: 3187715.7500\n",
      "Epoch 196/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 16998868189184.0000 - mae: 2829290.2500 - val_loss: 21838946107392.0000 - val_mae: 3142844.0000\n",
      "Epoch 197/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 16908984254464.0000 - mae: 2806668.2500 - val_loss: 21883739176960.0000 - val_mae: 3154364.7500\n",
      "Epoch 198/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 16740467605504.0000 - mae: 2816386.5000 - val_loss: 21937109598208.0000 - val_mae: 3166175.2500\n",
      "Epoch 199/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 16681538682880.0000 - mae: 2802368.2500 - val_loss: 21985228750848.0000 - val_mae: 3164004.7500\n",
      "Epoch 200/5000\n",
      "81/81 [==============================] - 2s 31ms/step - loss: 16419002515456.0000 - mae: 2777723.2500 - val_loss: 21843058622464.0000 - val_mae: 3149066.5000\n",
      "Epoch 201/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 16370589761536.0000 - mae: 2815324.2500 - val_loss: 21773921812480.0000 - val_mae: 3115893.0000\n",
      "Epoch 202/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 16188234006528.0000 - mae: 2771734.2500 - val_loss: 21874973081600.0000 - val_mae: 3089221.5000\n",
      "Epoch 203/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 16695306485760.0000 - mae: 2801739.2500 - val_loss: 21876006977536.0000 - val_mae: 3152500.2500\n",
      "Epoch 204/5000\n",
      "81/81 [==============================] - 3s 32ms/step - loss: 16380452667392.0000 - mae: 2773272.0000 - val_loss: 21778258722816.0000 - val_mae: 3139050.5000\n",
      "Epoch 205/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 16418704719872.0000 - mae: 2784652.0000 - val_loss: 21905868324864.0000 - val_mae: 3155125.5000\n",
      "Epoch 206/5000\n",
      "81/81 [==============================] - 2s 25ms/step - loss: 16344576688128.0000 - mae: 2797139.0000 - val_loss: 21788864020480.0000 - val_mae: 3114638.5000\n",
      "Epoch 207/5000\n",
      "81/81 [==============================] - 2s 30ms/step - loss: 16297015377920.0000 - mae: 2788663.5000 - val_loss: 21751903813632.0000 - val_mae: 3109579.2500\n",
      "Epoch 208/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 16243180437504.0000 - mae: 2761039.7500 - val_loss: 21801037987840.0000 - val_mae: 3165651.2500\n",
      "Epoch 209/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 16044182732800.0000 - mae: 2776042.2500 - val_loss: 21732245110784.0000 - val_mae: 3122291.7500\n",
      "Epoch 210/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 16145229807616.0000 - mae: 2762420.2500 - val_loss: 21606585860096.0000 - val_mae: 3114913.5000\n",
      "Epoch 211/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 15740455354368.0000 - mae: 2743225.0000 - val_loss: 21735485210624.0000 - val_mae: 3128347.0000\n",
      "Epoch 212/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 15990322626560.0000 - mae: 2762366.2500 - val_loss: 21709767835648.0000 - val_mae: 3131122.5000\n",
      "Epoch 213/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 16309861482496.0000 - mae: 2762166.2500 - val_loss: 21708222234624.0000 - val_mae: 3102737.5000\n",
      "Epoch 214/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 16330779525120.0000 - mae: 2758838.7500 - val_loss: 21686111961088.0000 - val_mae: 3119071.2500\n",
      "Epoch 215/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 15788302925824.0000 - mae: 2734112.5000 - val_loss: 21720530419712.0000 - val_mae: 3145051.2500\n",
      "Epoch 216/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 15650644819968.0000 - mae: 2720823.5000 - val_loss: 21721889374208.0000 - val_mae: 3126737.5000\n",
      "Epoch 217/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 15661048791040.0000 - mae: 2720798.5000 - val_loss: 21574180667392.0000 - val_mae: 3100598.7500\n",
      "Epoch 218/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 15575811096576.0000 - mae: 2697176.5000 - val_loss: 21638127026176.0000 - val_mae: 3130874.2500\n",
      "Epoch 219/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 15718594641920.0000 - mae: 2730896.0000 - val_loss: 21618841616384.0000 - val_mae: 3107495.5000\n",
      "Epoch 220/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 15528199454720.0000 - mae: 2709006.5000 - val_loss: 21643571232768.0000 - val_mae: 3128885.2500\n",
      "Epoch 221/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 15646958026752.0000 - mae: 2729750.0000 - val_loss: 21593979879424.0000 - val_mae: 3098176.2500\n",
      "Epoch 222/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 15751610105856.0000 - mae: 2722863.0000 - val_loss: 21710329872384.0000 - val_mae: 3066970.2500\n",
      "Epoch 223/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 15469017825280.0000 - mae: 2709260.2500 - val_loss: 21742051393536.0000 - val_mae: 3101409.7500\n",
      "Epoch 224/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 15467654676480.0000 - mae: 2694655.0000 - val_loss: 21580453249024.0000 - val_mae: 3094958.0000\n",
      "Epoch 225/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 15146662494208.0000 - mae: 2691536.7500 - val_loss: 21628408823808.0000 - val_mae: 3108599.7500\n",
      "Epoch 226/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 15143532494848.0000 - mae: 2677315.7500 - val_loss: 21624069816320.0000 - val_mae: 3083659.0000\n",
      "Epoch 227/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 15288700502016.0000 - mae: 2690190.7500 - val_loss: 21584899211264.0000 - val_mae: 3114603.7500\n",
      "Epoch 228/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 15090521735168.0000 - mae: 2673540.2500 - val_loss: 21602318155776.0000 - val_mae: 3121461.0000\n",
      "Epoch 229/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 14926293762048.0000 - mae: 2666495.5000 - val_loss: 21541672714240.0000 - val_mae: 3109537.5000\n",
      "Epoch 230/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 14974110924800.0000 - mae: 2672345.5000 - val_loss: 21517670809600.0000 - val_mae: 3109794.2500\n",
      "Epoch 231/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 15227889385472.0000 - mae: 2685462.5000 - val_loss: 21647253831680.0000 - val_mae: 3126804.2500\n",
      "Epoch 232/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 15324913074176.0000 - mae: 2680354.5000 - val_loss: 21541987287040.0000 - val_mae: 3107005.5000\n",
      "Epoch 233/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 15092276002816.0000 - mae: 2657004.0000 - val_loss: 21460328382464.0000 - val_mae: 3098100.5000\n",
      "Epoch 234/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 14833594400768.0000 - mae: 2652929.2500 - val_loss: 21800259944448.0000 - val_mae: 3151960.0000\n",
      "Epoch 235/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 14518559178752.0000 - mae: 2634907.7500 - val_loss: 21583468953600.0000 - val_mae: 3100596.2500\n",
      "Epoch 236/5000\n",
      "81/81 [==============================] - 2s 25ms/step - loss: 14659944972288.0000 - mae: 2629865.5000 - val_loss: 21554249334784.0000 - val_mae: 3100448.0000\n",
      "Epoch 237/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 14755453468672.0000 - mae: 2638124.2500 - val_loss: 21570011529216.0000 - val_mae: 3113006.5000\n",
      "Epoch 238/5000\n",
      "81/81 [==============================] - 2s 30ms/step - loss: 14524760457216.0000 - mae: 2608523.2500 - val_loss: 21435269513216.0000 - val_mae: 3067752.0000\n",
      "Epoch 239/5000\n",
      "81/81 [==============================] - 4s 49ms/step - loss: 14748536012800.0000 - mae: 2639239.5000 - val_loss: 21432625004544.0000 - val_mae: 3099495.7500\n",
      "Epoch 240/5000\n",
      "81/81 [==============================] - 2s 30ms/step - loss: 14709174566912.0000 - mae: 2639892.0000 - val_loss: 21501736648704.0000 - val_mae: 3101958.5000\n",
      "Epoch 241/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 14731545935872.0000 - mae: 2615394.7500 - val_loss: 21508732747776.0000 - val_mae: 3125429.0000\n",
      "Epoch 242/5000\n",
      "81/81 [==============================] - 2s 25ms/step - loss: 14348497977344.0000 - mae: 2614571.7500 - val_loss: 21487725576192.0000 - val_mae: 3095302.7500\n",
      "Epoch 243/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 14502925959168.0000 - mae: 2615562.0000 - val_loss: 21411806576640.0000 - val_mae: 3072773.5000\n",
      "Epoch 244/5000\n",
      "81/81 [==============================] - 3s 36ms/step - loss: 14217684975616.0000 - mae: 2592830.5000 - val_loss: 21402002391040.0000 - val_mae: 3085113.5000\n",
      "Epoch 245/5000\n",
      "81/81 [==============================] - 3s 31ms/step - loss: 14140824354816.0000 - mae: 2590286.0000 - val_loss: 21490466553856.0000 - val_mae: 3063595.2500\n",
      "Epoch 246/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 14083731488768.0000 - mae: 2588063.2500 - val_loss: 21641648144384.0000 - val_mae: 3146586.5000\n",
      "Epoch 247/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 14096400384000.0000 - mae: 2588184.0000 - val_loss: 21344729169920.0000 - val_mae: 3063548.5000\n",
      "Epoch 248/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 14294648356864.0000 - mae: 2591818.0000 - val_loss: 21410227421184.0000 - val_mae: 3102760.7500\n",
      "Epoch 249/5000\n",
      "81/81 [==============================] - 2s 21ms/step - loss: 13760938901504.0000 - mae: 2538250.2500 - val_loss: 21402736394240.0000 - val_mae: 3086471.7500\n",
      "Epoch 250/5000\n",
      "81/81 [==============================] - 3s 43ms/step - loss: 14111702253568.0000 - mae: 2568328.5000 - val_loss: 21374645043200.0000 - val_mae: 3101165.7500\n",
      "Epoch 251/5000\n",
      "81/81 [==============================] - 3s 38ms/step - loss: 13757893836800.0000 - mae: 2562602.7500 - val_loss: 21377077739520.0000 - val_mae: 3080831.2500\n",
      "Epoch 252/5000\n",
      "81/81 [==============================] - 2s 30ms/step - loss: 13565294542848.0000 - mae: 2548061.2500 - val_loss: 21250975989760.0000 - val_mae: 3063068.2500\n",
      "Epoch 253/5000\n",
      "81/81 [==============================] - 3s 34ms/step - loss: 13887988563968.0000 - mae: 2544766.5000 - val_loss: 21508801953792.0000 - val_mae: 3108040.2500\n",
      "Epoch 254/5000\n",
      "81/81 [==============================] - 3s 32ms/step - loss: 13575105019904.0000 - mae: 2531141.7500 - val_loss: 21420425871360.0000 - val_mae: 3085523.0000\n",
      "Epoch 255/5000\n",
      "81/81 [==============================] - 2s 31ms/step - loss: 13662747099136.0000 - mae: 2546425.2500 - val_loss: 21386967908352.0000 - val_mae: 3088328.7500\n",
      "Epoch 256/5000\n",
      "81/81 [==============================] - 3s 36ms/step - loss: 13652479442944.0000 - mae: 2536187.0000 - val_loss: 21475784392704.0000 - val_mae: 3082458.2500\n",
      "Epoch 257/5000\n",
      "81/81 [==============================] - 3s 32ms/step - loss: 13465144000512.0000 - mae: 2524378.0000 - val_loss: 21460932362240.0000 - val_mae: 3082477.5000\n",
      "Epoch 258/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 13430015655936.0000 - mae: 2507130.5000 - val_loss: 21299130793984.0000 - val_mae: 3071817.5000\n",
      "Epoch 259/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 13677458620416.0000 - mae: 2544442.0000 - val_loss: 21364608073728.0000 - val_mae: 3072586.0000\n",
      "Epoch 260/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 13677389414400.0000 - mae: 2524926.5000 - val_loss: 21242656587776.0000 - val_mae: 3057518.0000\n",
      "Epoch 261/5000\n",
      "81/81 [==============================] - 3s 34ms/step - loss: 13416353759232.0000 - mae: 2511912.7500 - val_loss: 21322912497664.0000 - val_mae: 3086430.2500\n",
      "Epoch 262/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 13685736079360.0000 - mae: 2517565.0000 - val_loss: 21240060313600.0000 - val_mae: 3087198.0000\n",
      "Epoch 263/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 13479128858624.0000 - mae: 2521939.0000 - val_loss: 21182363467776.0000 - val_mae: 3064916.2500\n",
      "Epoch 264/5000\n",
      "81/81 [==============================] - 3s 31ms/step - loss: 13471939821568.0000 - mae: 2494288.2500 - val_loss: 21297167859712.0000 - val_mae: 3088968.7500\n",
      "Epoch 265/5000\n",
      "81/81 [==============================] - 3s 33ms/step - loss: 13536053952512.0000 - mae: 2530580.2500 - val_loss: 21540471046144.0000 - val_mae: 3133405.2500\n",
      "Epoch 266/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 12997539921920.0000 - mae: 2486980.2500 - val_loss: 21357446299648.0000 - val_mae: 3068367.5000\n",
      "Epoch 267/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 13426157944832.0000 - mae: 2496142.2500 - val_loss: 21282338897920.0000 - val_mae: 3046187.5000\n",
      "Epoch 268/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 12963649945600.0000 - mae: 2470915.7500 - val_loss: 21287552417792.0000 - val_mae: 3055125.5000\n",
      "Epoch 269/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 13100755451904.0000 - mae: 2477345.2500 - val_loss: 21348625678336.0000 - val_mae: 3086634.7500\n",
      "Epoch 270/5000\n",
      "81/81 [==============================] - 2s 22ms/step - loss: 12949670330368.0000 - mae: 2483758.0000 - val_loss: 21293762084864.0000 - val_mae: 3078768.5000\n",
      "Epoch 271/5000\n",
      "81/81 [==============================] - 2s 25ms/step - loss: 12950830055424.0000 - mae: 2476500.0000 - val_loss: 21221777342464.0000 - val_mae: 3065305.7500\n",
      "Epoch 272/5000\n",
      "81/81 [==============================] - 2s 24ms/step - loss: 12717619412992.0000 - mae: 2448518.0000 - val_loss: 21260826312704.0000 - val_mae: 3063964.0000\n",
      "Epoch 273/5000\n",
      "81/81 [==============================] - 2s 22ms/step - loss: 12737735294976.0000 - mae: 2465399.2500 - val_loss: 21228840550400.0000 - val_mae: 3043427.7500\n",
      "Epoch 274/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 12866274983936.0000 - mae: 2455589.0000 - val_loss: 21445279219712.0000 - val_mae: 3087576.0000\n",
      "Epoch 275/5000\n",
      "81/81 [==============================] - 2s 25ms/step - loss: 12623754035200.0000 - mae: 2434489.5000 - val_loss: 21238846062592.0000 - val_mae: 3068501.7500\n",
      "Epoch 276/5000\n",
      "81/81 [==============================] - 2s 24ms/step - loss: 12900035985408.0000 - mae: 2446041.7500 - val_loss: 21302219898880.0000 - val_mae: 3058207.5000\n",
      "Epoch 277/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 12772239736832.0000 - mae: 2444290.2500 - val_loss: 21312502235136.0000 - val_mae: 3074991.5000\n",
      "Epoch 278/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 12472473878528.0000 - mae: 2440946.7500 - val_loss: 21433163972608.0000 - val_mae: 3089208.2500\n",
      "Epoch 279/5000\n",
      "81/81 [==============================] - 2s 23ms/step - loss: 12303547236352.0000 - mae: 2409001.0000 - val_loss: 21083805712384.0000 - val_mae: 3060260.0000\n",
      "Epoch 280/5000\n",
      "81/81 [==============================] - 2s 23ms/step - loss: 12466942640128.0000 - mae: 2408165.7500 - val_loss: 21075987529728.0000 - val_mae: 3049944.7500\n",
      "Epoch 281/5000\n",
      "81/81 [==============================] - 2s 22ms/step - loss: 12309989687296.0000 - mae: 2404740.5000 - val_loss: 21180117417984.0000 - val_mae: 3052143.2500\n",
      "Epoch 282/5000\n",
      "81/81 [==============================] - 2s 21ms/step - loss: 12535936843776.0000 - mae: 2418739.2500 - val_loss: 21056880377856.0000 - val_mae: 3050801.5000\n",
      "Epoch 283/5000\n",
      "81/81 [==============================] - 2s 22ms/step - loss: 12512910114816.0000 - mae: 2417481.2500 - val_loss: 21235517882368.0000 - val_mae: 3024829.7500\n",
      "Epoch 284/5000\n",
      "81/81 [==============================] - 2s 22ms/step - loss: 12148930510848.0000 - mae: 2387263.2500 - val_loss: 21196036898816.0000 - val_mae: 3066348.0000\n",
      "Epoch 285/5000\n",
      "81/81 [==============================] - 2s 23ms/step - loss: 12199705706496.0000 - mae: 2408712.5000 - val_loss: 21339647770624.0000 - val_mae: 3082230.2500\n",
      "Epoch 286/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 12290839543808.0000 - mae: 2406119.5000 - val_loss: 21142815375360.0000 - val_mae: 3034945.7500\n",
      "Epoch 287/5000\n",
      "81/81 [==============================] - 2s 25ms/step - loss: 12284032188416.0000 - mae: 2381749.5000 - val_loss: 21074850873344.0000 - val_mae: 3046767.2500\n",
      "Epoch 288/5000\n",
      "81/81 [==============================] - 2s 23ms/step - loss: 12045062766592.0000 - mae: 2381423.7500 - val_loss: 21166628536320.0000 - val_mae: 3037034.5000\n",
      "Epoch 289/5000\n",
      "81/81 [==============================] - 2s 20ms/step - loss: 12017708564480.0000 - mae: 2382226.0000 - val_loss: 21105978900480.0000 - val_mae: 3030929.7500\n",
      "Epoch 290/5000\n",
      "81/81 [==============================] - 2s 23ms/step - loss: 12131010347008.0000 - mae: 2368173.0000 - val_loss: 21072445440000.0000 - val_mae: 3022004.0000\n",
      "Epoch 291/5000\n",
      "81/81 [==============================] - 2s 25ms/step - loss: 11914521346048.0000 - mae: 2367593.5000 - val_loss: 21058801369088.0000 - val_mae: 3024544.7500\n",
      "Epoch 292/5000\n",
      "81/81 [==============================] - 2s 23ms/step - loss: 11652210622464.0000 - mae: 2336414.2500 - val_loss: 21208521244672.0000 - val_mae: 3087858.0000\n",
      "Epoch 293/5000\n",
      "81/81 [==============================] - 2s 24ms/step - loss: 11783972585472.0000 - mae: 2358834.0000 - val_loss: 21127044792320.0000 - val_mae: 3019319.7500\n",
      "Epoch 294/5000\n",
      "81/81 [==============================] - 3s 35ms/step - loss: 11805719003136.0000 - mae: 2351650.0000 - val_loss: 21100029280256.0000 - val_mae: 3004044.5000\n",
      "Epoch 295/5000\n",
      "81/81 [==============================] - 2s 30ms/step - loss: 11772018819072.0000 - mae: 2354003.5000 - val_loss: 21138801426432.0000 - val_mae: 3041729.7500\n",
      "Epoch 296/5000\n",
      "81/81 [==============================] - 2s 24ms/step - loss: 11767299178496.0000 - mae: 2347433.5000 - val_loss: 21244263006208.0000 - val_mae: 3084063.7500\n",
      "Epoch 297/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 11779013869568.0000 - mae: 2358085.7500 - val_loss: 21029183291392.0000 - val_mae: 3068040.5000\n",
      "Epoch 298/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 11453228646400.0000 - mae: 2305611.5000 - val_loss: 21061888376832.0000 - val_mae: 3061431.0000\n",
      "Epoch 299/5000\n",
      "81/81 [==============================] - 2s 25ms/step - loss: 11406866907136.0000 - mae: 2316147.5000 - val_loss: 21000458600448.0000 - val_mae: 3034305.5000\n",
      "Epoch 300/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 11660969377792.0000 - mae: 2331500.0000 - val_loss: 21118345805824.0000 - val_mae: 3052197.5000\n",
      "Epoch 301/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 11426046410752.0000 - mae: 2309439.5000 - val_loss: 21141213151232.0000 - val_mae: 3050828.7500\n",
      "Epoch 302/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 11345147723776.0000 - mae: 2314432.0000 - val_loss: 21276919857152.0000 - val_mae: 2996965.7500\n",
      "Epoch 303/5000\n",
      "81/81 [==============================] - 2s 25ms/step - loss: 11544107679744.0000 - mae: 2326902.2500 - val_loss: 20961856323584.0000 - val_mae: 3028121.0000\n",
      "Epoch 304/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 11578107756544.0000 - mae: 2316114.0000 - val_loss: 21013091844096.0000 - val_mae: 3013324.0000\n",
      "Epoch 305/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 11437375225856.0000 - mae: 2309034.7500 - val_loss: 21171703644160.0000 - val_mae: 3033627.5000\n",
      "Epoch 306/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 11425002029056.0000 - mae: 2296856.7500 - val_loss: 21003562385408.0000 - val_mae: 3009148.2500\n",
      "Epoch 307/5000\n",
      "81/81 [==============================] - 2s 25ms/step - loss: 11317725364224.0000 - mae: 2300572.5000 - val_loss: 20902804717568.0000 - val_mae: 3004351.2500\n",
      "Epoch 308/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 11186340888576.0000 - mae: 2286035.7500 - val_loss: 20970708402176.0000 - val_mae: 2995286.2500\n",
      "Epoch 309/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 11178749198336.0000 - mae: 2276964.7500 - val_loss: 21106666766336.0000 - val_mae: 3058635.5000\n",
      "Epoch 310/5000\n",
      "81/81 [==============================] - 2s 30ms/step - loss: 11304002650112.0000 - mae: 2289917.2500 - val_loss: 20944573693952.0000 - val_mae: 3044796.5000\n",
      "Epoch 311/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 11042764619776.0000 - mae: 2282326.5000 - val_loss: 21055729041408.0000 - val_mae: 3038074.0000\n",
      "Epoch 312/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 10776794365952.0000 - mae: 2255269.5000 - val_loss: 21100446613504.0000 - val_mae: 3061095.7500\n",
      "Epoch 313/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 10910620975104.0000 - mae: 2263745.7500 - val_loss: 21057549369344.0000 - val_mae: 3004326.2500\n",
      "Epoch 314/5000\n",
      "81/81 [==============================] - 3s 37ms/step - loss: 11040687390720.0000 - mae: 2270638.0000 - val_loss: 21007412756480.0000 - val_mae: 2999127.5000\n",
      "Epoch 315/5000\n",
      "81/81 [==============================] - 3s 32ms/step - loss: 10908038332416.0000 - mae: 2265042.5000 - val_loss: 20944712105984.0000 - val_mae: 2999771.5000\n",
      "Epoch 316/5000\n",
      "81/81 [==============================] - 2s 25ms/step - loss: 10911870877696.0000 - mae: 2258798.0000 - val_loss: 21055060049920.0000 - val_mae: 3000344.0000\n",
      "Epoch 317/5000\n",
      "81/81 [==============================] - 2s 22ms/step - loss: 10864050569216.0000 - mae: 2267643.0000 - val_loss: 20904010579968.0000 - val_mae: 3000942.2500\n",
      "Epoch 318/5000\n",
      "81/81 [==============================] - 2s 24ms/step - loss: 11006030905344.0000 - mae: 2285035.2500 - val_loss: 20901928108032.0000 - val_mae: 3017069.0000\n",
      "Epoch 319/5000\n",
      "81/81 [==============================] - 2s 22ms/step - loss: 10555133788160.0000 - mae: 2227062.7500 - val_loss: 20953744539648.0000 - val_mae: 3025885.7500\n",
      "Epoch 320/5000\n",
      "81/81 [==============================] - 2s 24ms/step - loss: 10931967885312.0000 - mae: 2259622.5000 - val_loss: 20969185869824.0000 - val_mae: 3005990.7500\n",
      "Epoch 321/5000\n",
      "81/81 [==============================] - 2s 22ms/step - loss: 10464259997696.0000 - mae: 2227043.2500 - val_loss: 20699498414080.0000 - val_mae: 3012027.2500\n",
      "Epoch 322/5000\n",
      "81/81 [==============================] - 2s 24ms/step - loss: 10466498707456.0000 - mae: 2226054.0000 - val_loss: 20981517123584.0000 - val_mae: 3024876.7500\n",
      "Epoch 323/5000\n",
      "81/81 [==============================] - 2s 23ms/step - loss: 10561468235776.0000 - mae: 2228303.2500 - val_loss: 20866458976256.0000 - val_mae: 2982383.5000\n",
      "Epoch 324/5000\n",
      "81/81 [==============================] - 2s 21ms/step - loss: 10867074662400.0000 - mae: 2236665.2500 - val_loss: 20830545248256.0000 - val_mae: 2982664.5000\n",
      "Epoch 325/5000\n",
      "81/81 [==============================] - 2s 24ms/step - loss: 10301308141568.0000 - mae: 2198451.5000 - val_loss: 20861142695936.0000 - val_mae: 3012853.7500\n",
      "Epoch 326/5000\n",
      "81/81 [==============================] - 2s 22ms/step - loss: 10544922755072.0000 - mae: 2211333.0000 - val_loss: 20906543939584.0000 - val_mae: 2980766.2500\n",
      "Epoch 327/5000\n",
      "81/81 [==============================] - 2s 23ms/step - loss: 10312031928320.0000 - mae: 2205499.0000 - val_loss: 20756159266816.0000 - val_mae: 2980401.7500\n",
      "Epoch 328/5000\n",
      "81/81 [==============================] - 2s 24ms/step - loss: 10267902607360.0000 - mae: 2205517.5000 - val_loss: 20717005438976.0000 - val_mae: 2993594.2500\n",
      "Epoch 329/5000\n",
      "81/81 [==============================] - 3s 36ms/step - loss: 10506748297216.0000 - mae: 2221220.7500 - val_loss: 20941673332736.0000 - val_mae: 3007034.5000\n",
      "Epoch 330/5000\n",
      "81/81 [==============================] - 3s 32ms/step - loss: 10330409271296.0000 - mae: 2193431.2500 - val_loss: 20728248270848.0000 - val_mae: 2985652.0000\n",
      "Epoch 331/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 10212266213376.0000 - mae: 2195774.2500 - val_loss: 20916014678016.0000 - val_mae: 2970330.2500\n",
      "Epoch 332/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 10039097032704.0000 - mae: 2178782.2500 - val_loss: 20670719197184.0000 - val_mae: 2995320.5000\n",
      "Epoch 333/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 10273972813824.0000 - mae: 2194050.2500 - val_loss: 20745623175168.0000 - val_mae: 3019037.0000\n",
      "Epoch 334/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 10259361955840.0000 - mae: 2179368.7500 - val_loss: 20724850884608.0000 - val_mae: 3027327.2500\n",
      "Epoch 335/5000\n",
      "81/81 [==============================] - 2s 30ms/step - loss: 9914922565632.0000 - mae: 2150778.7500 - val_loss: 20664907988992.0000 - val_mae: 2995942.5000\n",
      "Epoch 336/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 10175520964608.0000 - mae: 2181830.2500 - val_loss: 20610572877824.0000 - val_mae: 2971222.5000\n",
      "Epoch 337/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 9778391678976.0000 - mae: 2152113.7500 - val_loss: 20864131137536.0000 - val_mae: 2972542.5000\n",
      "Epoch 338/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 9876176633856.0000 - mae: 2135309.2500 - val_loss: 20620035227648.0000 - val_mae: 2968000.7500\n",
      "Epoch 339/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 10249064939520.0000 - mae: 2184320.7500 - val_loss: 20651203100672.0000 - val_mae: 2956491.2500\n",
      "Epoch 340/5000\n",
      "81/81 [==============================] - 2s 25ms/step - loss: 9793558282240.0000 - mae: 2138604.2500 - val_loss: 20720713203712.0000 - val_mae: 2978132.2500\n",
      "Epoch 341/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 9914442317824.0000 - mae: 2164936.7500 - val_loss: 20787861913600.0000 - val_mae: 2970362.5000\n",
      "Epoch 342/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 9934318075904.0000 - mae: 2150875.0000 - val_loss: 20851139280896.0000 - val_mae: 3000434.7500\n",
      "Epoch 343/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 9755212906496.0000 - mae: 2138100.0000 - val_loss: 20807667417088.0000 - val_mae: 2995162.7500\n",
      "Epoch 344/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 9615901196288.0000 - mae: 2136074.5000 - val_loss: 20638957830144.0000 - val_mae: 2961652.0000\n",
      "Epoch 345/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 9716396720128.0000 - mae: 2137577.5000 - val_loss: 21156545429504.0000 - val_mae: 3053868.7500\n",
      "Epoch 346/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 9885467017216.0000 - mae: 2157808.0000 - val_loss: 20813887569920.0000 - val_mae: 2978973.5000\n",
      "Epoch 347/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 9486458683392.0000 - mae: 2100419.5000 - val_loss: 20732551626752.0000 - val_mae: 2954854.2500\n",
      "Epoch 348/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 9533608951808.0000 - mae: 2107960.0000 - val_loss: 20602675003392.0000 - val_mae: 2957717.0000\n",
      "Epoch 349/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 9494203465728.0000 - mae: 2106190.0000 - val_loss: 20812952240128.0000 - val_mae: 2952150.7500\n",
      "Epoch 350/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 9515413012480.0000 - mae: 2104226.7500 - val_loss: 20654661304320.0000 - val_mae: 2975219.7500\n",
      "Epoch 351/5000\n",
      "81/81 [==============================] - 2s 25ms/step - loss: 9443220652032.0000 - mae: 2112572.5000 - val_loss: 20493061062656.0000 - val_mae: 2953112.7500\n",
      "Epoch 352/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 9558864953344.0000 - mae: 2098962.7500 - val_loss: 20655305129984.0000 - val_mae: 2961315.7500\n",
      "Epoch 353/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 9365388001280.0000 - mae: 2090814.0000 - val_loss: 20660231340032.0000 - val_mae: 2960977.2500\n",
      "Epoch 354/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 9546256875520.0000 - mae: 2093231.7500 - val_loss: 20405647572992.0000 - val_mae: 2959219.2500\n",
      "Epoch 355/5000\n",
      "81/81 [==============================] - 2s 25ms/step - loss: 9369587548160.0000 - mae: 2101584.0000 - val_loss: 20626190368768.0000 - val_mae: 2959215.7500\n",
      "Epoch 356/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 9520320348160.0000 - mae: 2110874.5000 - val_loss: 20578058633216.0000 - val_mae: 2946071.2500\n",
      "Epoch 357/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 9337586057216.0000 - mae: 2093705.2500 - val_loss: 20481782579200.0000 - val_mae: 2989499.2500\n",
      "Epoch 358/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 8978354405376.0000 - mae: 2071985.6250 - val_loss: 20695381704704.0000 - val_mae: 2997331.2500\n",
      "Epoch 359/5000\n",
      "81/81 [==============================] - 2s 25ms/step - loss: 9121382268928.0000 - mae: 2078257.5000 - val_loss: 20666621362176.0000 - val_mae: 2952255.0000\n",
      "Epoch 360/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 9050782695424.0000 - mae: 2077069.0000 - val_loss: 20636887941120.0000 - val_mae: 2994295.0000\n",
      "Epoch 361/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 9123832791040.0000 - mae: 2070892.5000 - val_loss: 20643045179392.0000 - val_mae: 2971656.2500\n",
      "Epoch 362/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 9168414048256.0000 - mae: 2063563.2500 - val_loss: 20454307790848.0000 - val_mae: 2963175.7500\n",
      "Epoch 363/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 9257481142272.0000 - mae: 2098962.0000 - val_loss: 20521850765312.0000 - val_mae: 2961053.5000\n",
      "Epoch 364/5000\n",
      "81/81 [==============================] - 2s 25ms/step - loss: 9035872993280.0000 - mae: 2053119.0000 - val_loss: 20451717808128.0000 - val_mae: 2964400.5000\n",
      "Epoch 365/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 9060996874240.0000 - mae: 2070218.1250 - val_loss: 20511910264832.0000 - val_mae: 2946112.0000\n",
      "Epoch 366/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 9083963834368.0000 - mae: 2068203.0000 - val_loss: 20732039921664.0000 - val_mae: 2968012.5000\n",
      "Epoch 367/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 8968443265024.0000 - mae: 2045855.5000 - val_loss: 20573778345984.0000 - val_mae: 2971532.0000\n",
      "Epoch 368/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 9137342644224.0000 - mae: 2064284.3750 - val_loss: 20609538981888.0000 - val_mae: 2962380.5000\n",
      "Epoch 369/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 9169251860480.0000 - mae: 2064853.5000 - val_loss: 20647140917248.0000 - val_mae: 2939625.2500\n",
      "Epoch 370/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 8612988583936.0000 - mae: 2011298.7500 - val_loss: 20644039229440.0000 - val_mae: 2940462.7500\n",
      "Epoch 371/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 9137398218752.0000 - mae: 2059385.3750 - val_loss: 20424809250816.0000 - val_mae: 2944614.5000\n",
      "Epoch 372/5000\n",
      "81/81 [==============================] - 2s 25ms/step - loss: 8706874408960.0000 - mae: 2011000.6250 - val_loss: 20368462970880.0000 - val_mae: 2936819.7500\n",
      "Epoch 373/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 8751267971072.0000 - mae: 2033583.2500 - val_loss: 20584922611712.0000 - val_mae: 2979110.5000\n",
      "Epoch 374/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 8766481235968.0000 - mae: 2031090.1250 - val_loss: 20459311595520.0000 - val_mae: 2948133.0000\n",
      "Epoch 375/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 8646260948992.0000 - mae: 2022006.0000 - val_loss: 20648803958784.0000 - val_mae: 2973509.5000\n",
      "Epoch 376/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 8666275119104.0000 - mae: 2022062.6250 - val_loss: 20535691968512.0000 - val_mae: 2977409.0000\n",
      "Epoch 377/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 8751787016192.0000 - mae: 2027037.1250 - val_loss: 20649521184768.0000 - val_mae: 2949910.7500\n",
      "Epoch 378/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 8710995836928.0000 - mae: 2020308.3750 - val_loss: 20699072692224.0000 - val_mae: 2973427.5000\n",
      "Epoch 379/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 8419077521408.0000 - mae: 1988744.2500 - val_loss: 20577146372096.0000 - val_mae: 2959649.5000\n",
      "Epoch 380/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 8879597420544.0000 - mae: 2030465.0000 - val_loss: 20743492468736.0000 - val_mae: 2974805.5000\n",
      "Epoch 381/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 8972353404928.0000 - mae: 2033804.3750 - val_loss: 20884509163520.0000 - val_mae: 2992818.5000\n",
      "Epoch 382/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 8310331801600.0000 - mae: 1974266.8750 - val_loss: 20708539236352.0000 - val_mae: 2963929.5000\n",
      "Epoch 383/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 8550938050560.0000 - mae: 2001665.7500 - val_loss: 20414958927872.0000 - val_mae: 2954180.7500\n",
      "Epoch 384/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 8126490738688.0000 - mae: 1975249.8750 - val_loss: 20393840607232.0000 - val_mae: 2953951.7500\n",
      "Epoch 385/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 8610257043456.0000 - mae: 1999298.2500 - val_loss: 20514571550720.0000 - val_mae: 2930547.2500\n",
      "Epoch 386/5000\n",
      "81/81 [==============================] - 3s 31ms/step - loss: 8598343647232.0000 - mae: 2014100.2500 - val_loss: 20572276785152.0000 - val_mae: 2911919.5000\n",
      "Epoch 387/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 8523435999232.0000 - mae: 2009123.3750 - val_loss: 20438807740416.0000 - val_mae: 2935729.0000\n",
      "Epoch 388/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 8358681116672.0000 - mae: 1968002.3750 - val_loss: 20535557750784.0000 - val_mae: 2958732.5000\n",
      "Epoch 389/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 8445982932992.0000 - mae: 1994379.8750 - val_loss: 20335139225600.0000 - val_mae: 2941675.2500\n",
      "Epoch 390/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 8576298385408.0000 - mae: 2007159.5000 - val_loss: 20447085199360.0000 - val_mae: 2956271.2500\n",
      "Epoch 391/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 8360247164928.0000 - mae: 1979278.5000 - val_loss: 20370084069376.0000 - val_mae: 2945758.0000\n",
      "Epoch 392/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 8439478091776.0000 - mae: 2000978.3750 - val_loss: 20425543254016.0000 - val_mae: 2950389.0000\n",
      "Epoch 393/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 8271985377280.0000 - mae: 1962722.2500 - val_loss: 20311397367808.0000 - val_mae: 2923996.7500\n",
      "Epoch 394/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 8275519602688.0000 - mae: 1958139.5000 - val_loss: 20256061915136.0000 - val_mae: 2942745.2500\n",
      "Epoch 395/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 8246449930240.0000 - mae: 1966719.7500 - val_loss: 20497293115392.0000 - val_mae: 2920944.7500\n",
      "Epoch 396/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 8289918648320.0000 - mae: 1988341.6250 - val_loss: 20370839044096.0000 - val_mae: 2949013.0000\n",
      "Epoch 397/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 8068622974976.0000 - mae: 1944787.7500 - val_loss: 20374542614528.0000 - val_mae: 2965826.2500\n",
      "Epoch 398/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 7950359855104.0000 - mae: 1935207.1250 - val_loss: 20296081866752.0000 - val_mae: 2956504.0000\n",
      "Epoch 399/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 8160150028288.0000 - mae: 1980612.1250 - val_loss: 20210039914496.0000 - val_mae: 2906200.7500\n",
      "Epoch 400/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 8290356428800.0000 - mae: 1953851.0000 - val_loss: 20321077821440.0000 - val_mae: 2945822.7500\n",
      "Epoch 401/5000\n",
      "81/81 [==============================] - 2s 25ms/step - loss: 8435145375744.0000 - mae: 1982388.8750 - val_loss: 20136794783744.0000 - val_mae: 2898884.0000\n",
      "Epoch 402/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 8194910322688.0000 - mae: 1955947.0000 - val_loss: 20111991767040.0000 - val_mae: 2904669.2500\n",
      "Epoch 403/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 7953271750656.0000 - mae: 1931666.3750 - val_loss: 20184941199360.0000 - val_mae: 2940854.5000\n",
      "Epoch 404/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 8305350541312.0000 - mae: 1967986.8750 - val_loss: 20290283241472.0000 - val_mae: 2958484.7500\n",
      "Epoch 405/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 8030065786880.0000 - mae: 1945048.3750 - val_loss: 20111045951488.0000 - val_mae: 2926595.5000\n",
      "Epoch 406/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 7942432096256.0000 - mae: 1930768.2500 - val_loss: 20307540705280.0000 - val_mae: 2945935.5000\n",
      "Epoch 407/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 7704909709312.0000 - mae: 1901047.1250 - val_loss: 20259004219392.0000 - val_mae: 2924769.7500\n",
      "Epoch 408/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 7703904649216.0000 - mae: 1907739.8750 - val_loss: 20593697095680.0000 - val_mae: 2943220.7500\n",
      "Epoch 409/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 7797409316864.0000 - mae: 1921800.6250 - val_loss: 20339050414080.0000 - val_mae: 2931130.0000\n",
      "Epoch 410/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 8104831352832.0000 - mae: 1948069.8750 - val_loss: 20380360114176.0000 - val_mae: 2897112.5000\n",
      "Epoch 411/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 7697360486400.0000 - mae: 1904262.0000 - val_loss: 20239125315584.0000 - val_mae: 2943849.0000\n",
      "Epoch 412/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 7714968698880.0000 - mae: 1904468.2500 - val_loss: 20107627593728.0000 - val_mae: 2929051.2500\n",
      "Epoch 413/5000\n",
      "81/81 [==============================] - 2s 25ms/step - loss: 7720721186816.0000 - mae: 1896235.5000 - val_loss: 20478580228096.0000 - val_mae: 2914384.2500\n",
      "Epoch 414/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 7540093485056.0000 - mae: 1880132.2500 - val_loss: 20481516240896.0000 - val_mae: 2960361.7500\n",
      "Epoch 415/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 7830248095744.0000 - mae: 1905695.5000 - val_loss: 20274000953344.0000 - val_mae: 2929169.0000\n",
      "Epoch 416/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 7609295306752.0000 - mae: 1894644.3750 - val_loss: 20191446564864.0000 - val_mae: 2898412.2500\n",
      "Epoch 417/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 7606076178432.0000 - mae: 1893515.7500 - val_loss: 20511264342016.0000 - val_mae: 2933582.2500\n",
      "Epoch 418/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 7553601765376.0000 - mae: 1870341.2500 - val_loss: 20533789851648.0000 - val_mae: 2921478.7500\n",
      "Epoch 419/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 7598485536768.0000 - mae: 1881982.7500 - val_loss: 20402678005760.0000 - val_mae: 2946145.0000\n",
      "Epoch 420/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 7771915288576.0000 - mae: 1877405.0000 - val_loss: 20273719934976.0000 - val_mae: 2932105.7500\n",
      "Epoch 421/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 7647558893568.0000 - mae: 1894628.7500 - val_loss: 20561507909632.0000 - val_mae: 2929614.2500\n",
      "Epoch 422/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 7634163335168.0000 - mae: 1881668.6250 - val_loss: 20699861221376.0000 - val_mae: 2930007.0000\n",
      "Epoch 423/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 7682404646912.0000 - mae: 1891291.3750 - val_loss: 20555140956160.0000 - val_mae: 2909556.2500\n",
      "Epoch 424/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 7700345782272.0000 - mae: 1880897.8750 - val_loss: 20355156541440.0000 - val_mae: 2931667.5000\n",
      "Epoch 425/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 7523065659392.0000 - mae: 1858426.0000 - val_loss: 20339524370432.0000 - val_mae: 2915847.7500\n",
      "Epoch 426/5000\n",
      "81/81 [==============================] - 2s 25ms/step - loss: 7516624257024.0000 - mae: 1877524.8750 - val_loss: 20213296791552.0000 - val_mae: 2888392.2500\n",
      "Epoch 427/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 7505279188992.0000 - mae: 1869584.1250 - val_loss: 20523511709696.0000 - val_mae: 2945201.2500\n",
      "Epoch 428/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 7325700063232.0000 - mae: 1850292.0000 - val_loss: 20369844994048.0000 - val_mae: 2935167.7500\n",
      "Epoch 429/5000\n",
      "81/81 [==============================] - 2s 31ms/step - loss: 7415619649536.0000 - mae: 1855470.8750 - val_loss: 20391852507136.0000 - val_mae: 2919129.5000\n",
      "Epoch 430/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 7411947012096.0000 - mae: 1866995.7500 - val_loss: 20406771646464.0000 - val_mae: 2924460.2500\n",
      "Epoch 431/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 7512386437120.0000 - mae: 1848054.5000 - val_loss: 20233385410560.0000 - val_mae: 2914436.0000\n",
      "Epoch 432/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 7286501146624.0000 - mae: 1852334.5000 - val_loss: 20476262875136.0000 - val_mae: 2934359.7500\n",
      "Epoch 433/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 7491307438080.0000 - mae: 1871594.2500 - val_loss: 20423737606144.0000 - val_mae: 2909480.0000\n",
      "Epoch 434/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 7436200050688.0000 - mae: 1861568.0000 - val_loss: 20309897904128.0000 - val_mae: 2915192.7500\n",
      "Epoch 435/5000\n",
      "81/81 [==============================] - 2s 25ms/step - loss: 7416184832000.0000 - mae: 1865325.7500 - val_loss: 20330496131072.0000 - val_mae: 2941458.7500\n",
      "Epoch 436/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 7333637783552.0000 - mae: 1853528.6250 - val_loss: 20312620007424.0000 - val_mae: 2919942.2500\n",
      "Epoch 437/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 7391326765056.0000 - mae: 1868248.2500 - val_loss: 20481730150400.0000 - val_mae: 2920780.5000\n",
      "Epoch 438/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 7185998282752.0000 - mae: 1830137.2500 - val_loss: 20437010481152.0000 - val_mae: 2913107.5000\n",
      "Epoch 439/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 7388416442368.0000 - mae: 1861315.1250 - val_loss: 20513030144000.0000 - val_mae: 2929812.0000\n",
      "Epoch 440/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 6992435871744.0000 - mae: 1817981.0000 - val_loss: 20351339724800.0000 - val_mae: 2948515.2500\n",
      "Epoch 441/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 7409098031104.0000 - mae: 1864021.3750 - val_loss: 20102277758976.0000 - val_mae: 2909238.5000\n",
      "Epoch 442/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 7507415662592.0000 - mae: 1869086.2500 - val_loss: 20419954343936.0000 - val_mae: 2952891.2500\n",
      "Epoch 443/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 6986161192960.0000 - mae: 1825532.5000 - val_loss: 20377843531776.0000 - val_mae: 2912169.5000\n",
      "Epoch 444/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 7131115814912.0000 - mae: 1840345.1250 - val_loss: 20443060764672.0000 - val_mae: 2923331.2500\n",
      "Epoch 445/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 7275387813888.0000 - mae: 1834968.5000 - val_loss: 20686972125184.0000 - val_mae: 2927202.0000\n",
      "Epoch 446/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 6904218124288.0000 - mae: 1802486.0000 - val_loss: 20586745036800.0000 - val_mae: 2932815.5000\n",
      "Epoch 447/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 7279415918592.0000 - mae: 1839324.5000 - val_loss: 20640717340672.0000 - val_mae: 2943945.2500\n",
      "Epoch 448/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 7307886329856.0000 - mae: 1830442.5000 - val_loss: 20436087734272.0000 - val_mae: 2895867.0000\n",
      "Epoch 449/5000\n",
      "81/81 [==============================] - 2s 30ms/step - loss: 7192753209344.0000 - mae: 1840830.8750 - val_loss: 20369673027584.0000 - val_mae: 2915318.7500\n",
      "Epoch 450/5000\n",
      "81/81 [==============================] - 3s 32ms/step - loss: 7301676138496.0000 - mae: 1820997.0000 - val_loss: 20216555765760.0000 - val_mae: 2904635.0000\n",
      "Epoch 451/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 7418222215168.0000 - mae: 1864465.0000 - val_loss: 20344721113088.0000 - val_mae: 2898206.7500\n",
      "Epoch 452/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 6878015782912.0000 - mae: 1795040.5000 - val_loss: 20464221028352.0000 - val_mae: 2896370.7500\n",
      "Epoch 453/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 7034946191360.0000 - mae: 1815780.3750 - val_loss: 20380739698688.0000 - val_mae: 2894488.7500\n",
      "Epoch 454/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 6938682195968.0000 - mae: 1810996.1250 - val_loss: 20682928816128.0000 - val_mae: 2887350.2500\n",
      "Epoch 455/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 6880896745472.0000 - mae: 1802569.1250 - val_loss: 20329437069312.0000 - val_mae: 2875373.2500\n",
      "Epoch 456/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 7084985810944.0000 - mae: 1829841.3750 - val_loss: 20393763012608.0000 - val_mae: 2913441.5000\n",
      "Epoch 457/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 6559875203072.0000 - mae: 1760855.7500 - val_loss: 20477154164736.0000 - val_mae: 2928349.5000\n",
      "Epoch 458/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 7223995531264.0000 - mae: 1806412.5000 - val_loss: 20261254463488.0000 - val_mae: 2901860.7500\n",
      "Epoch 459/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 6970261110784.0000 - mae: 1814527.7500 - val_loss: 20365977845760.0000 - val_mae: 2901077.2500\n",
      "Epoch 460/5000\n",
      "81/81 [==============================] - 2s 25ms/step - loss: 7178353639424.0000 - mae: 1826133.8750 - val_loss: 20558125203456.0000 - val_mae: 2912051.0000\n",
      "Epoch 461/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 6886118129664.0000 - mae: 1780072.1250 - val_loss: 20525621444608.0000 - val_mae: 2867279.5000\n",
      "Epoch 462/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 6664951431168.0000 - mae: 1771803.0000 - val_loss: 20330519199744.0000 - val_mae: 2877441.5000\n",
      "Epoch 463/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 6983619444736.0000 - mae: 1795192.6250 - val_loss: 20635010990080.0000 - val_mae: 2953151.7500\n",
      "Epoch 464/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 6943847481344.0000 - mae: 1792411.6250 - val_loss: 20588697485312.0000 - val_mae: 2899994.5000\n",
      "Epoch 465/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 7363445129216.0000 - mae: 1836993.6250 - val_loss: 20589376962560.0000 - val_mae: 2934470.0000\n",
      "Epoch 466/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 6704961945600.0000 - mae: 1772159.7500 - val_loss: 20453099831296.0000 - val_mae: 2919665.5000\n",
      "Epoch 467/5000\n",
      "81/81 [==============================] - 2s 25ms/step - loss: 6661708709888.0000 - mae: 1770386.3750 - val_loss: 20540112764928.0000 - val_mae: 2879301.7500\n",
      "Epoch 468/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 6689945812992.0000 - mae: 1772694.5000 - val_loss: 20592235380736.0000 - val_mae: 2882035.0000\n",
      "Epoch 469/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 6597475565568.0000 - mae: 1765280.7500 - val_loss: 20560148955136.0000 - val_mae: 2921620.5000\n",
      "Epoch 470/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 6738039799808.0000 - mae: 1758161.2500 - val_loss: 20562841698304.0000 - val_mae: 2908765.5000\n",
      "Epoch 471/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 6911612157952.0000 - mae: 1786721.3750 - val_loss: 20564131446784.0000 - val_mae: 2883967.2500\n",
      "Epoch 472/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 6850436136960.0000 - mae: 1778129.7500 - val_loss: 20300374736896.0000 - val_mae: 2906502.2500\n",
      "Epoch 473/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 6491035140096.0000 - mae: 1764170.7500 - val_loss: 20499899875328.0000 - val_mae: 2893089.7500\n",
      "Epoch 474/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 6598054379520.0000 - mae: 1756860.2500 - val_loss: 20531371835392.0000 - val_mae: 2951977.2500\n",
      "Epoch 475/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 6553217794048.0000 - mae: 1744553.7500 - val_loss: 20626255380480.0000 - val_mae: 2923258.0000\n",
      "Epoch 476/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 6595179184128.0000 - mae: 1767345.7500 - val_loss: 20748127174656.0000 - val_mae: 2908660.5000\n",
      "Epoch 477/5000\n",
      "81/81 [==============================] - 2s 29ms/step - loss: 6678610182144.0000 - mae: 1767879.0000 - val_loss: 20731347861504.0000 - val_mae: 2912777.2500\n",
      "Epoch 478/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 7157662613504.0000 - mae: 1802394.5000 - val_loss: 20521332768768.0000 - val_mae: 2901623.2500\n",
      "Epoch 479/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 6689178779648.0000 - mae: 1768291.6250 - val_loss: 20479412797440.0000 - val_mae: 2911112.5000\n",
      "Epoch 480/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 6605697974272.0000 - mae: 1743539.2500 - val_loss: 20372065878016.0000 - val_mae: 2908538.7500\n",
      "Epoch 481/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 6907615510528.0000 - mae: 1761849.2500 - val_loss: 20340895907840.0000 - val_mae: 2883335.2500\n",
      "Epoch 482/5000\n",
      "81/81 [==============================] - 2s 26ms/step - loss: 6462855708672.0000 - mae: 1742407.1250 - val_loss: 20465301061632.0000 - val_mae: 2896559.0000\n",
      "Epoch 483/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 6923524505600.0000 - mae: 1776420.1250 - val_loss: 20571161100288.0000 - val_mae: 2895312.7500\n",
      "Epoch 484/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 6473149579264.0000 - mae: 1736908.3750 - val_loss: 20486790578176.0000 - val_mae: 2944729.0000\n",
      "Epoch 485/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 6799067447296.0000 - mae: 1784603.2500 - val_loss: 20452430839808.0000 - val_mae: 2898432.0000\n",
      "Epoch 486/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 6669424132096.0000 - mae: 1736325.0000 - val_loss: 20434554716160.0000 - val_mae: 2916506.7500\n",
      "Epoch 487/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 6333611376640.0000 - mae: 1729253.6250 - val_loss: 20280885903360.0000 - val_mae: 2870189.0000\n",
      "Epoch 488/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 6574008958976.0000 - mae: 1745301.2500 - val_loss: 20320819871744.0000 - val_mae: 2884264.5000\n",
      "Epoch 489/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 6646005235712.0000 - mae: 1750838.5000 - val_loss: 20368767057920.0000 - val_mae: 2921766.5000\n",
      "Epoch 490/5000\n",
      "81/81 [==============================] - 2s 27ms/step - loss: 6453441593344.0000 - mae: 1732529.8750 - val_loss: 20202322395136.0000 - val_mae: 2905743.7500\n",
      "Epoch 491/5000\n",
      "81/81 [==============================] - 2s 28ms/step - loss: 6214303875072.0000 - mae: 1710916.1250 - val_loss: 20321614692352.0000 - val_mae: 2889407.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, validation_data = (X_test, y_test), callbacks = [es], epochs = 5000, batch_size = 50, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4483556.670354802"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(mean_squared_error(y_test, model.predict(X_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
