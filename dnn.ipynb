{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseball = pd.read_csv('best_model/engineered_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = baseball.drop(['Salary'], axis = 1)\n",
    "y = baseball['Salary']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .25, random_state = 621)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = ['Tm', 'Lg', 'Acquired', 'Bat']\n",
    "num_columns = [col for col in X.columns if col not in cat_columns + ['Pos_C', 'Pos_1B', 'Pos_2B', 'Pos_3B', 'Pos_SS', 'Pos_OF']]\n",
    "\n",
    "cat_transformer = Pipeline(\n",
    "    steps = [\n",
    "        ('onehot', OneHotEncoder(handle_unknown = 'ignore'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "num_transformer = Pipeline(\n",
    "    steps = [\n",
    "        ('scale', StandardScaler())\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers = [\n",
    "        ('cont', num_transformer, num_columns),\n",
    "        ('cat', cat_transformer, cat_columns)\n",
    "    ], remainder = 'passthrough'\n",
    ")\n",
    "\n",
    "X_transform = preprocessor.fit_transform(X)\n",
    "\n",
    "selected_features = np.concatenate([\n",
    "    np.array(num_columns),\n",
    "    np.array(preprocessor.transformers_[1][1]['onehot'].get_feature_names_out(cat_columns)),\n",
    "    np.array(['Pos_C', 'Pos_1B', 'Pos_2B', 'Pos_3B', 'Pos_SS', 'Pos_OF'])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-17 18:29:57.978012: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Dense\n",
    "from bayes_opt import BayesianOptimization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_transform, y, test_size = .2, random_state = 621)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | batch_... | dropou... |  epochs   | layer_... | layer_... | layer_... | layer_... | layer_... | learni... |  neurons  | num_la... | patience  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m-2.119e+1\u001b[0m | \u001b[0m207.3    \u001b[0m | \u001b[0m0.4754   \u001b[0m | \u001b[0m392.8    \u001b[0m | \u001b[0m166.1    \u001b[0m | \u001b[0m66.95    \u001b[0m | \u001b[0m66.94    \u001b[0m | \u001b[0m45.01    \u001b[0m | \u001b[0m226.0    \u001b[0m | \u001b[0m0.6051   \u001b[0m | \u001b[0m190.6    \u001b[0m | \u001b[0m1.082    \u001b[0m | \u001b[0m49.1     \u001b[0m |\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "| \u001b[0m2        \u001b[0m | \u001b[0m-2.333e+1\u001b[0m | \u001b[0m421.6    \u001b[0m | \u001b[0m0.1062   \u001b[0m | \u001b[0m172.7    \u001b[0m | \u001b[0m73.08    \u001b[0m | \u001b[0m100.2    \u001b[0m | \u001b[0m149.5    \u001b[0m | \u001b[0m128.8    \u001b[0m | \u001b[0m97.24    \u001b[0m | \u001b[0m0.6157   \u001b[0m | \u001b[0m63.25    \u001b[0m | \u001b[0m2.169    \u001b[0m | \u001b[0m30.99    \u001b[0m |\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m-2.185e+1\u001b[0m | \u001b[0m245.4    \u001b[0m | \u001b[0m0.3926   \u001b[0m | \u001b[0m179.9    \u001b[0m | \u001b[0m147.2    \u001b[0m | \u001b[0m164.7    \u001b[0m | \u001b[0m42.4     \u001b[0m | \u001b[0m168.1    \u001b[0m | \u001b[0m70.2     \u001b[0m | \u001b[0m0.0744   \u001b[0m | \u001b[0m244.6    \u001b[0m | \u001b[0m4.863    \u001b[0m | \u001b[0m44.25    \u001b[0m |\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m-2.429e+1\u001b[0m | \u001b[0m174.6    \u001b[0m | \u001b[0m0.04884  \u001b[0m | \u001b[0m373.7    \u001b[0m | \u001b[0m130.6    \u001b[0m | \u001b[0m59.34    \u001b[0m | \u001b[0m142.9    \u001b[0m | \u001b[0m39.7     \u001b[0m | \u001b[0m235.7    \u001b[0m | \u001b[0m0.2662   \u001b[0m | \u001b[0m180.4    \u001b[0m | \u001b[0m2.247    \u001b[0m | \u001b[0m35.6     \u001b[0m |\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m-2.315e+1\u001b[0m | \u001b[0m287.9    \u001b[0m | \u001b[0m0.09243  \u001b[0m | \u001b[0m487.8    \u001b[0m | \u001b[0m205.6    \u001b[0m | \u001b[0m242.4    \u001b[0m | \u001b[0m232.4    \u001b[0m | \u001b[0m165.9    \u001b[0m | \u001b[0m238.5    \u001b[0m | \u001b[0m0.09761  \u001b[0m | \u001b[0m75.9     \u001b[0m | \u001b[0m1.181    \u001b[0m | \u001b[0m29.76    \u001b[0m |\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "| \u001b[0m6        \u001b[0m | \u001b[0m-2.258e+1\u001b[0m | \u001b[0m259.3    \u001b[0m | \u001b[0m0.354    \u001b[0m | \u001b[0m263.0    \u001b[0m | \u001b[0m154.4    \u001b[0m | \u001b[0m218.6    \u001b[0m | \u001b[0m143.3    \u001b[0m | \u001b[0m158.1    \u001b[0m | \u001b[0m126.6    \u001b[0m | \u001b[0m0.1988   \u001b[0m | \u001b[0m250.1    \u001b[0m | \u001b[0m1.082    \u001b[0m | \u001b[0m23.73    \u001b[0m |\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m-2.308e+1\u001b[0m | \u001b[0m353.1    \u001b[0m | \u001b[0m0.3643   \u001b[0m | \u001b[0m410.4    \u001b[0m | \u001b[0m232.1    \u001b[0m | \u001b[0m166.2    \u001b[0m | \u001b[0m254.1    \u001b[0m | \u001b[0m59.43    \u001b[0m | \u001b[0m206.1    \u001b[0m | \u001b[0m0.06139  \u001b[0m | \u001b[0m62.54    \u001b[0m | \u001b[0m3.695    \u001b[0m | \u001b[0m35.34    \u001b[0m |\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m-2.566e+1\u001b[0m | \u001b[0m439.1    \u001b[0m | \u001b[0m0.3421   \u001b[0m | \u001b[0m194.0    \u001b[0m | \u001b[0m49.28    \u001b[0m | \u001b[0m243.3    \u001b[0m | \u001b[0m170.7    \u001b[0m | \u001b[0m131.0    \u001b[0m | \u001b[0m83.59    \u001b[0m | \u001b[0m0.1348   \u001b[0m | \u001b[0m86.74    \u001b[0m | \u001b[0m4.043    \u001b[0m | \u001b[0m20.82    \u001b[0m |\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m-2.734e+1\u001b[0m | \u001b[0m451.2    \u001b[0m | \u001b[0m0.05307  \u001b[0m | \u001b[0m186.9    \u001b[0m | \u001b[0m114.3    \u001b[0m | \u001b[0m94.19    \u001b[0m | \u001b[0m165.8    \u001b[0m | \u001b[0m54.16    \u001b[0m | \u001b[0m225.9    \u001b[0m | \u001b[0m0.5165   \u001b[0m | \u001b[0m36.21    \u001b[0m | \u001b[0m4.075    \u001b[0m | \u001b[0m36.53    \u001b[0m |\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m-2.289e+1\u001b[0m | \u001b[0m407.2    \u001b[0m | \u001b[0m0.4496   \u001b[0m | \u001b[0m496.0    \u001b[0m | \u001b[0m40.1     \u001b[0m | \u001b[0m43.45    \u001b[0m | \u001b[0m212.8    \u001b[0m | \u001b[0m195.6    \u001b[0m | \u001b[0m131.4    \u001b[0m | \u001b[0m0.212    \u001b[0m | \u001b[0m171.8    \u001b[0m | \u001b[0m3.976    \u001b[0m | \u001b[0m22.69    \u001b[0m |\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m-4.781e+1\u001b[0m | \u001b[0m448.7    \u001b[0m | \u001b[0m0.4757   \u001b[0m | \u001b[0m324.8    \u001b[0m | \u001b[0m135.7    \u001b[0m | \u001b[0m77.96    \u001b[0m | \u001b[0m231.7    \u001b[0m | \u001b[0m195.3    \u001b[0m | \u001b[0m116.2    \u001b[0m | \u001b[0m0.2542   \u001b[0m | \u001b[0m112.4    \u001b[0m | \u001b[0m4.585    \u001b[0m | \u001b[0m38.95    \u001b[0m |\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m-6.577e+1\u001b[0m | \u001b[0m234.5    \u001b[0m | \u001b[0m0.2632   \u001b[0m | \u001b[0m344.6    \u001b[0m | \u001b[0m105.0    \u001b[0m | \u001b[0m249.8    \u001b[0m | \u001b[0m70.72    \u001b[0m | \u001b[0m197.2    \u001b[0m | \u001b[0m103.6    \u001b[0m | \u001b[0m0.148    \u001b[0m | \u001b[0m132.6    \u001b[0m | \u001b[0m4.014    \u001b[0m | \u001b[0m24.55    \u001b[0m |\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m-2.263e+1\u001b[0m | \u001b[0m264.6    \u001b[0m | \u001b[0m0.4063   \u001b[0m | \u001b[0m180.2    \u001b[0m | \u001b[0m171.6    \u001b[0m | \u001b[0m173.4    \u001b[0m | \u001b[0m122.8    \u001b[0m | \u001b[0m141.5    \u001b[0m | \u001b[0m109.0    \u001b[0m | \u001b[0m0.177    \u001b[0m | \u001b[0m256.0    \u001b[0m | \u001b[0m1.671    \u001b[0m | \u001b[0m34.26    \u001b[0m |\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m-2.349e+1\u001b[0m | \u001b[0m425.7    \u001b[0m | \u001b[0m0.02199  \u001b[0m | \u001b[0m171.9    \u001b[0m | \u001b[0m72.6     \u001b[0m | \u001b[0m108.4    \u001b[0m | \u001b[0m154.7    \u001b[0m | \u001b[0m130.6    \u001b[0m | \u001b[0m108.1    \u001b[0m | \u001b[0m0.6991   \u001b[0m | \u001b[0m74.0     \u001b[0m | \u001b[0m1.723    \u001b[0m | \u001b[0m29.67    \u001b[0m |\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m-2.218e+1\u001b[0m | \u001b[0m253.3    \u001b[0m | \u001b[0m0.5      \u001b[0m | \u001b[0m292.7    \u001b[0m | \u001b[0m235.4    \u001b[0m | \u001b[0m140.0    \u001b[0m | \u001b[0m136.0    \u001b[0m | \u001b[0m45.38    \u001b[0m | \u001b[0m211.4    \u001b[0m | \u001b[0m0.5406   \u001b[0m | \u001b[0m256.0    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m41.52    \u001b[0m |\n",
      "=========================================================================================================================================================================\n",
      "Best Hyperparameters: {'batch_size': 207.28477562056565, 'dropout_rate': 0.4753571532049581, 'epochs': 392.797576724562, 'layer_neurons_1': 166.0995004601362, 'layer_neurons_2': 66.94817545910578, 'layer_neurons_3': 66.94277255530939, 'layer_neurons_4': 45.010729125676676, 'layer_neurons_5': 226.02345665358547, 'learning_rate': 0.6051038616257767, 'neurons': 190.6082574263142, 'num_layers': 1.0823379771832098, 'patience': 49.097295564859834}\n"
     ]
    }
   ],
   "source": [
    "def dnn_model_score(neurons, dropout_rate, learning_rate, epochs, batch_size, patience, num_layers, **layer_neurons):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(int(neurons), activation='relu', input_shape = (X_train.shape[1],)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    for i in range(1, int(num_layers) + 1):\n",
    "        model.add(Dense(int(layer_neurons[f'layer_neurons_{i}']), activation='relu'))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(1, activation = 'linear'))\n",
    "\n",
    "    optimizer = Adam(learning_rate = learning_rate)\n",
    "    model.compile(loss = 'mean_squared_error', optimizer = optimizer)\n",
    "\n",
    "    es = EarlyStopping(monitor = 'val_loss', patience = int(patience), restore_best_weights = True)\n",
    "\n",
    "    model.fit(X_train, y_train, validation_split = .2, epochs = int(epochs), batch_size = int(batch_size), callbacks = es, verbose = 0)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    return -mse\n",
    "\n",
    "pbounds = {'neurons': (32, 256),\n",
    "           'dropout_rate': (0.0, 0.5),\n",
    "           'learning_rate': (0.01, 1),\n",
    "           'epochs' : (100, 500),\n",
    "           'batch_size' : (32, 500),\n",
    "           'patience' : (20, 50),\n",
    "           'num_layers': (1, 5)}\n",
    "\n",
    "for i in range(1, 6):\n",
    "    pbounds[f'layer_neurons_{i}'] = (32, 256)\n",
    "\n",
    "optimizer = BayesianOptimization(f = dnn_model_score, pbounds = pbounds, random_state = 42)\n",
    "\n",
    "optimizer.maximize(init_points = 5, n_iter = 10)\n",
    "\n",
    "best_params = optimizer.max['params']\n",
    "print(\"Best Hyperparameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = int(best_params['batch_size'])\n",
    "dropout_rate = best_params['dropout_rate']\n",
    "epochs = int(best_params['epochs'])\n",
    "neurons = []\n",
    "neurons.append(int(best_params['neurons']))\n",
    "neurons.append(int(best_params['layer_neurons_1']))\n",
    "neurons.append(int(best_params['layer_neurons_2']))\n",
    "neurons.append(int(best_params['layer_neurons_3']))\n",
    "neurons.append(int(best_params['layer_neurons_4']))\n",
    "neurons.append(int(best_params['layer_neurons_5']))\n",
    "learning_rate = best_params['learning_rate']\n",
    "num_layers = int(best_params['num_layers'])\n",
    "patience = int(best_params['patience'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_15\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_15\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ Dense1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">190</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">14,820</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Dropout1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">190</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Dense2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">166</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">31,706</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Dropout2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">166</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">167</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ Dense1 (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m190\u001b[0m)            │        \u001b[38;5;34m14,820\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Dropout1 (\u001b[38;5;33mDropout\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m190\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Dense2 (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m166\u001b[0m)            │        \u001b[38;5;34m31,706\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Dropout2 (\u001b[38;5;33mDropout\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m166\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Output (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m167\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">46,693</span> (182.39 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m46,693\u001b[0m (182.39 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">46,693</span> (182.39 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m46,693\u001b[0m (182.39 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(name = 'Dense1', units = neurons[0], input_dim = X_train.shape[1], activation = 'relu'))\n",
    "model.add(Dropout(name = 'Dropout1', rate = dropout_rate))\n",
    "\n",
    "for i in range(1, num_layers + 1):\n",
    "    model.add(Dense(name = f'Dense{i + 1}', units = neurons[i], activation = 'relu'))\n",
    "    model.add(Dropout(name = f'Dropout{i + 1}', rate = dropout_rate))\n",
    "\n",
    "model.add(Dense(name = 'Output', units = 1, activation = 'linear'))\n",
    "\n",
    "optimizer = Adam(learning_rate = learning_rate)\n",
    "\n",
    "model.compile(optimizer = optimizer, loss = 'mean_squared_error')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/392\n",
      "16/16 - 4s - 245ms/step - loss: 48168855142400.0000 - val_loss: 32053166866432.0000\n",
      "Epoch 2/392\n",
      "16/16 - 0s - 12ms/step - loss: 32669312221184.0000 - val_loss: 28785554489344.0000\n",
      "Epoch 3/392\n",
      "16/16 - 0s - 16ms/step - loss: 30067356860416.0000 - val_loss: 26202389610496.0000\n",
      "Epoch 4/392\n",
      "16/16 - 0s - 19ms/step - loss: 28963168583680.0000 - val_loss: 25250651701248.0000\n",
      "Epoch 5/392\n",
      "16/16 - 0s - 24ms/step - loss: 27783900168192.0000 - val_loss: 24963673227264.0000\n",
      "Epoch 6/392\n",
      "16/16 - 0s - 13ms/step - loss: 27591247396864.0000 - val_loss: 25080568479744.0000\n",
      "Epoch 7/392\n",
      "16/16 - 0s - 21ms/step - loss: 27414763667456.0000 - val_loss: 24696877744128.0000\n",
      "Epoch 8/392\n",
      "16/16 - 0s - 17ms/step - loss: 26610128388096.0000 - val_loss: 24780031918080.0000\n",
      "Epoch 9/392\n",
      "16/16 - 0s - 24ms/step - loss: 26745212239872.0000 - val_loss: 24828731981824.0000\n",
      "Epoch 10/392\n",
      "16/16 - 0s - 19ms/step - loss: 26856914944000.0000 - val_loss: 23967656050688.0000\n",
      "Epoch 11/392\n",
      "16/16 - 1s - 32ms/step - loss: 25305389465600.0000 - val_loss: 23815845314560.0000\n",
      "Epoch 12/392\n",
      "16/16 - 0s - 13ms/step - loss: 25509719179264.0000 - val_loss: 23550687707136.0000\n",
      "Epoch 13/392\n",
      "16/16 - 0s - 14ms/step - loss: 24676625547264.0000 - val_loss: 23264002834432.0000\n",
      "Epoch 14/392\n",
      "16/16 - 0s - 27ms/step - loss: 24005742428160.0000 - val_loss: 23142254772224.0000\n",
      "Epoch 15/392\n",
      "16/16 - 0s - 26ms/step - loss: 23316947533824.0000 - val_loss: 22858568826880.0000\n",
      "Epoch 16/392\n",
      "16/16 - 0s - 30ms/step - loss: 23179466637312.0000 - val_loss: 22523232124928.0000\n",
      "Epoch 17/392\n",
      "16/16 - 0s - 15ms/step - loss: 22761707667456.0000 - val_loss: 22688106020864.0000\n",
      "Epoch 18/392\n",
      "16/16 - 0s - 14ms/step - loss: 23407330590720.0000 - val_loss: 22879487918080.0000\n",
      "Epoch 19/392\n",
      "16/16 - 0s - 15ms/step - loss: 24586464788480.0000 - val_loss: 22235674836992.0000\n",
      "Epoch 20/392\n",
      "16/16 - 0s - 14ms/step - loss: 22952458321920.0000 - val_loss: 22401626669056.0000\n",
      "Epoch 21/392\n",
      "16/16 - 0s - 14ms/step - loss: 21885207183360.0000 - val_loss: 21407643729920.0000\n",
      "Epoch 22/392\n",
      "16/16 - 0s - 14ms/step - loss: 21651764805632.0000 - val_loss: 21465315409920.0000\n",
      "Epoch 23/392\n",
      "16/16 - 0s - 14ms/step - loss: 20910767603712.0000 - val_loss: 20932422795264.0000\n",
      "Epoch 24/392\n",
      "16/16 - 0s - 14ms/step - loss: 20490932453376.0000 - val_loss: 21346505457664.0000\n",
      "Epoch 25/392\n",
      "16/16 - 0s - 14ms/step - loss: 20078886125568.0000 - val_loss: 20705085227008.0000\n",
      "Epoch 26/392\n",
      "16/16 - 0s - 14ms/step - loss: 19534408843264.0000 - val_loss: 20746340401152.0000\n",
      "Epoch 27/392\n",
      "16/16 - 0s - 14ms/step - loss: 19662943289344.0000 - val_loss: 20981810724864.0000\n",
      "Epoch 28/392\n",
      "16/16 - 0s - 14ms/step - loss: 19036379283456.0000 - val_loss: 21742514864128.0000\n",
      "Epoch 29/392\n",
      "16/16 - 0s - 14ms/step - loss: 18760247279616.0000 - val_loss: 20851023937536.0000\n",
      "Epoch 30/392\n",
      "16/16 - 0s - 14ms/step - loss: 18509419511808.0000 - val_loss: 20896156745728.0000\n",
      "Epoch 31/392\n",
      "16/16 - 0s - 14ms/step - loss: 17502071422976.0000 - val_loss: 20259201351680.0000\n",
      "Epoch 32/392\n",
      "16/16 - 0s - 14ms/step - loss: 17394933170176.0000 - val_loss: 20862249992192.0000\n",
      "Epoch 33/392\n",
      "16/16 - 0s - 16ms/step - loss: 17249640382464.0000 - val_loss: 20671891505152.0000\n",
      "Epoch 34/392\n",
      "16/16 - 0s - 15ms/step - loss: 16570393821184.0000 - val_loss: 21616778018816.0000\n",
      "Epoch 35/392\n",
      "16/16 - 0s - 14ms/step - loss: 17345221230592.0000 - val_loss: 20555786878976.0000\n",
      "Epoch 36/392\n",
      "16/16 - 0s - 14ms/step - loss: 16862874173440.0000 - val_loss: 20263993344000.0000\n",
      "Epoch 37/392\n",
      "16/16 - 0s - 14ms/step - loss: 17175454679040.0000 - val_loss: 20467138166784.0000\n",
      "Epoch 38/392\n",
      "16/16 - 0s - 14ms/step - loss: 16735569707008.0000 - val_loss: 20183076831232.0000\n",
      "Epoch 39/392\n",
      "16/16 - 0s - 15ms/step - loss: 16259398762496.0000 - val_loss: 20234865999872.0000\n",
      "Epoch 40/392\n",
      "16/16 - 0s - 18ms/step - loss: 15796639105024.0000 - val_loss: 21489564778496.0000\n",
      "Epoch 41/392\n",
      "16/16 - 0s - 13ms/step - loss: 15683174793216.0000 - val_loss: 20671509823488.0000\n",
      "Epoch 42/392\n",
      "16/16 - 0s - 14ms/step - loss: 14693403983872.0000 - val_loss: 20534718889984.0000\n",
      "Epoch 43/392\n",
      "16/16 - 0s - 18ms/step - loss: 15353274957824.0000 - val_loss: 22101289336832.0000\n",
      "Epoch 44/392\n",
      "16/16 - 0s - 14ms/step - loss: 15137911078912.0000 - val_loss: 21352427814912.0000\n",
      "Epoch 45/392\n",
      "16/16 - 0s - 14ms/step - loss: 14846992056320.0000 - val_loss: 20423420936192.0000\n",
      "Epoch 46/392\n",
      "16/16 - 0s - 14ms/step - loss: 14853237374976.0000 - val_loss: 20425950101504.0000\n",
      "Epoch 47/392\n",
      "16/16 - 0s - 21ms/step - loss: 14660577263616.0000 - val_loss: 21456398319616.0000\n",
      "Epoch 48/392\n",
      "16/16 - 0s - 14ms/step - loss: 14697748234240.0000 - val_loss: 20765344792576.0000\n",
      "Epoch 49/392\n",
      "16/16 - 0s - 14ms/step - loss: 13600753188864.0000 - val_loss: 20836985602048.0000\n",
      "Epoch 50/392\n",
      "16/16 - 0s - 15ms/step - loss: 13590773891072.0000 - val_loss: 20363469651968.0000\n",
      "Epoch 51/392\n",
      "16/16 - 0s - 14ms/step - loss: 14065455857664.0000 - val_loss: 21204073185280.0000\n",
      "Epoch 52/392\n",
      "16/16 - 0s - 14ms/step - loss: 13631810961408.0000 - val_loss: 21679755493376.0000\n",
      "Epoch 53/392\n",
      "16/16 - 0s - 14ms/step - loss: 13867091492864.0000 - val_loss: 20429683032064.0000\n",
      "Epoch 54/392\n",
      "16/16 - 0s - 18ms/step - loss: 13172114194432.0000 - val_loss: 21171479248896.0000\n",
      "Epoch 55/392\n",
      "16/16 - 0s - 13ms/step - loss: 12731154432000.0000 - val_loss: 21332567785472.0000\n",
      "Epoch 56/392\n",
      "16/16 - 0s - 14ms/step - loss: 13741102989312.0000 - val_loss: 21765994577920.0000\n",
      "Epoch 57/392\n",
      "16/16 - 0s - 13ms/step - loss: 12980245757952.0000 - val_loss: 21072183296000.0000\n",
      "Epoch 58/392\n",
      "16/16 - 0s - 13ms/step - loss: 13734082772992.0000 - val_loss: 21032899444736.0000\n",
      "Epoch 59/392\n",
      "16/16 - 0s - 13ms/step - loss: 12455597047808.0000 - val_loss: 21805033062400.0000\n",
      "Epoch 60/392\n",
      "16/16 - 0s - 13ms/step - loss: 12485170036736.0000 - val_loss: 21475660660736.0000\n",
      "Epoch 61/392\n",
      "16/16 - 0s - 13ms/step - loss: 12997702451200.0000 - val_loss: 20406043934720.0000\n",
      "Epoch 62/392\n",
      "16/16 - 0s - 13ms/step - loss: 12516874780672.0000 - val_loss: 22020253286400.0000\n",
      "Epoch 63/392\n",
      "16/16 - 0s - 14ms/step - loss: 12527536701440.0000 - val_loss: 22258999361536.0000\n",
      "Epoch 64/392\n",
      "16/16 - 0s - 14ms/step - loss: 12457535864832.0000 - val_loss: 21695796609024.0000\n",
      "Epoch 65/392\n",
      "16/16 - 0s - 13ms/step - loss: 12185087508480.0000 - val_loss: 20751434383360.0000\n",
      "Epoch 66/392\n",
      "16/16 - 0s - 13ms/step - loss: 12834600648704.0000 - val_loss: 21252070703104.0000\n",
      "Epoch 67/392\n",
      "16/16 - 0s - 13ms/step - loss: 12578830942208.0000 - val_loss: 22162389860352.0000\n",
      "Epoch 68/392\n",
      "16/16 - 0s - 13ms/step - loss: 12484042817536.0000 - val_loss: 20929224638464.0000\n",
      "Epoch 69/392\n",
      "16/16 - 0s - 12ms/step - loss: 11666764857344.0000 - val_loss: 21859103932416.0000\n",
      "Epoch 70/392\n",
      "16/16 - 0s - 13ms/step - loss: 11798749118464.0000 - val_loss: 20762335379456.0000\n",
      "Epoch 71/392\n",
      "16/16 - 0s - 13ms/step - loss: 11183677505536.0000 - val_loss: 21509626134528.0000\n",
      "Epoch 72/392\n",
      "16/16 - 0s - 13ms/step - loss: 11369439035392.0000 - val_loss: 20846953365504.0000\n",
      "Epoch 73/392\n",
      "16/16 - 0s - 13ms/step - loss: 11910900613120.0000 - val_loss: 21583416524800.0000\n",
      "Epoch 74/392\n",
      "16/16 - 0s - 14ms/step - loss: 11659951210496.0000 - val_loss: 21607959494656.0000\n",
      "Epoch 75/392\n",
      "16/16 - 0s - 15ms/step - loss: 11342998142976.0000 - val_loss: 21286782763008.0000\n",
      "Epoch 76/392\n",
      "16/16 - 0s - 13ms/step - loss: 11345179181056.0000 - val_loss: 20855895621632.0000\n",
      "Epoch 77/392\n",
      "16/16 - 0s - 13ms/step - loss: 11117533331456.0000 - val_loss: 21350280331264.0000\n",
      "Epoch 78/392\n",
      "16/16 - 0s - 13ms/step - loss: 11077220827136.0000 - val_loss: 19818021388288.0000\n",
      "Epoch 79/392\n",
      "16/16 - 0s - 13ms/step - loss: 11660447186944.0000 - val_loss: 21079949049856.0000\n",
      "Epoch 80/392\n",
      "16/16 - 0s - 13ms/step - loss: 10999568531456.0000 - val_loss: 21233743691776.0000\n",
      "Epoch 81/392\n",
      "16/16 - 0s - 13ms/step - loss: 11073322221568.0000 - val_loss: 20298338402304.0000\n",
      "Epoch 82/392\n",
      "16/16 - 0s - 13ms/step - loss: 11087557689344.0000 - val_loss: 20456862121984.0000\n",
      "Epoch 83/392\n",
      "16/16 - 0s - 13ms/step - loss: 11020614500352.0000 - val_loss: 21531673493504.0000\n",
      "Epoch 84/392\n",
      "16/16 - 0s - 13ms/step - loss: 10721596276736.0000 - val_loss: 20760550703104.0000\n",
      "Epoch 85/392\n",
      "16/16 - 0s - 12ms/step - loss: 10924837568512.0000 - val_loss: 20515483811840.0000\n",
      "Epoch 86/392\n",
      "16/16 - 0s - 14ms/step - loss: 10580840677376.0000 - val_loss: 20530887393280.0000\n",
      "Epoch 87/392\n",
      "16/16 - 0s - 13ms/step - loss: 10551974428672.0000 - val_loss: 21078244065280.0000\n",
      "Epoch 88/392\n",
      "16/16 - 0s - 22ms/step - loss: 10986825187328.0000 - val_loss: 20459684888576.0000\n",
      "Epoch 89/392\n",
      "16/16 - 0s - 13ms/step - loss: 9992551792640.0000 - val_loss: 21285595774976.0000\n",
      "Epoch 90/392\n",
      "16/16 - 0s - 12ms/step - loss: 10897805279232.0000 - val_loss: 20398838120448.0000\n",
      "Epoch 91/392\n",
      "16/16 - 0s - 12ms/step - loss: 11405127319552.0000 - val_loss: 21856784482304.0000\n",
      "Epoch 92/392\n",
      "16/16 - 0s - 13ms/step - loss: 10771297730560.0000 - val_loss: 20810689413120.0000\n",
      "Epoch 93/392\n",
      "16/16 - 0s - 12ms/step - loss: 10094215430144.0000 - val_loss: 20140374622208.0000\n",
      "Epoch 94/392\n",
      "16/16 - 0s - 12ms/step - loss: 11040053002240.0000 - val_loss: 21061296979968.0000\n",
      "Epoch 95/392\n",
      "16/16 - 0s - 13ms/step - loss: 10480982687744.0000 - val_loss: 20716462276608.0000\n",
      "Epoch 96/392\n",
      "16/16 - 0s - 13ms/step - loss: 10374257573888.0000 - val_loss: 20381152837632.0000\n",
      "Epoch 97/392\n",
      "16/16 - 0s - 13ms/step - loss: 10218374168576.0000 - val_loss: 21425731665920.0000\n",
      "Epoch 98/392\n",
      "16/16 - 0s - 13ms/step - loss: 9912222482432.0000 - val_loss: 20778705747968.0000\n",
      "Epoch 99/392\n",
      "16/16 - 0s - 13ms/step - loss: 10136790761472.0000 - val_loss: 20658455052288.0000\n",
      "Epoch 100/392\n",
      "16/16 - 0s - 13ms/step - loss: 11101302423552.0000 - val_loss: 21303683710976.0000\n",
      "Epoch 101/392\n",
      "16/16 - 0s - 13ms/step - loss: 11429624152064.0000 - val_loss: 21285484625920.0000\n",
      "Epoch 102/392\n",
      "16/16 - 0s - 13ms/step - loss: 11202431287296.0000 - val_loss: 20384848019456.0000\n",
      "Epoch 103/392\n",
      "16/16 - 0s - 13ms/step - loss: 10119149518848.0000 - val_loss: 21712013885440.0000\n",
      "Epoch 104/392\n",
      "16/16 - 0s - 13ms/step - loss: 9875850526720.0000 - val_loss: 20637273817088.0000\n",
      "Epoch 105/392\n",
      "16/16 - 0s - 13ms/step - loss: 10448819716096.0000 - val_loss: 20778869325824.0000\n",
      "Epoch 106/392\n",
      "16/16 - 0s - 13ms/step - loss: 10301778952192.0000 - val_loss: 20492882804736.0000\n",
      "Epoch 107/392\n",
      "16/16 - 0s - 13ms/step - loss: 10027060428800.0000 - val_loss: 20551600963584.0000\n",
      "Epoch 108/392\n",
      "16/16 - 0s - 14ms/step - loss: 10151447756800.0000 - val_loss: 21464136810496.0000\n",
      "Epoch 109/392\n",
      "16/16 - 0s - 14ms/step - loss: 10250851713024.0000 - val_loss: 20379863089152.0000\n",
      "Epoch 110/392\n",
      "16/16 - 0s - 14ms/step - loss: 9816339644416.0000 - val_loss: 21807952297984.0000\n",
      "Epoch 111/392\n",
      "16/16 - 0s - 13ms/step - loss: 9592949964800.0000 - val_loss: 21161196912640.0000\n",
      "Epoch 112/392\n",
      "16/16 - 0s - 13ms/step - loss: 9609957867520.0000 - val_loss: 21093513428992.0000\n",
      "Epoch 113/392\n",
      "16/16 - 0s - 17ms/step - loss: 9663096553472.0000 - val_loss: 20083541803008.0000\n",
      "Epoch 114/392\n",
      "16/16 - 0s - 13ms/step - loss: 9945180274688.0000 - val_loss: 20230172573696.0000\n",
      "Epoch 115/392\n",
      "16/16 - 0s - 13ms/step - loss: 9074956566528.0000 - val_loss: 21012844380160.0000\n",
      "Epoch 116/392\n",
      "16/16 - 0s - 13ms/step - loss: 9588257587200.0000 - val_loss: 20140271861760.0000\n",
      "Epoch 117/392\n",
      "16/16 - 0s - 19ms/step - loss: 9627822456832.0000 - val_loss: 21574447005696.0000\n",
      "Epoch 118/392\n",
      "16/16 - 0s - 19ms/step - loss: 10070678044672.0000 - val_loss: 20403042910208.0000\n",
      "Epoch 119/392\n",
      "16/16 - 0s - 13ms/step - loss: 9519696445440.0000 - val_loss: 21233236180992.0000\n",
      "Epoch 120/392\n",
      "16/16 - 0s - 13ms/step - loss: 9109048918016.0000 - val_loss: 20973805895680.0000\n",
      "Epoch 121/392\n",
      "16/16 - 0s - 13ms/step - loss: 10246073352192.0000 - val_loss: 21220760223744.0000\n",
      "Epoch 122/392\n",
      "16/16 - 0s - 14ms/step - loss: 10455023091712.0000 - val_loss: 20201116532736.0000\n",
      "Epoch 123/392\n",
      "16/16 - 0s - 12ms/step - loss: 9661928439808.0000 - val_loss: 20041374367744.0000\n",
      "Epoch 124/392\n",
      "16/16 - 0s - 13ms/step - loss: 9527674011648.0000 - val_loss: 20132051025920.0000\n",
      "Epoch 125/392\n",
      "16/16 - 0s - 13ms/step - loss: 8698639417344.0000 - val_loss: 20980030242816.0000\n",
      "Epoch 126/392\n",
      "16/16 - 0s - 13ms/step - loss: 9088092078080.0000 - val_loss: 20278820208640.0000\n",
      "Epoch 127/392\n",
      "16/16 - 0s - 21ms/step - loss: 9428870889472.0000 - val_loss: 21262187364352.0000\n"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor = 'val_loss', patience = int(patience), restore_best_weights = True)\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_split = .2, batch_size = batch_size, epochs = epochs, callbacks = es, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    }
   ],
   "source": [
    "test_preds = model.predict(X_test)\n",
    "train_preds = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Neural Network Metrics:\n",
      "Train RMSE: 3085814.808687137\n",
      "Test RMSE: 4665029.528188211\n"
     ]
    }
   ],
   "source": [
    "train_rmse = mean_squared_error(y_train, train_preds)\n",
    "test_rmse = mean_squared_error(y_test, test_preds)\n",
    "\n",
    "print('Deep Neural Network Metrics:')\n",
    "print(f'Train RMSE: {np.sqrt(train_rmse)}')\n",
    "print(f'Test RMSE: {np.sqrt(test_rmse)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best_model/y_test.joblib']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the model, pca, and preprocessor so that new data can be fit using the same criteria\n",
    "#model.save('best_model/best_model.keras')\n",
    "#joblib.dump(pca, 'dnn/pca45.joblib')\n",
    "#joblib.dump(preprocessor, 'best_model/best_preprocssor.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
