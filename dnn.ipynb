{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "import shap\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseball = pd.read_csv('best_model/engineered_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = baseball.drop(['Salary'], axis = 1)\n",
    "y = baseball['Salary']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .25, random_state = 621)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = ['Tm', 'Lg', 'Acquired', 'Bat']\n",
    "num_columns = [col for col in X.columns if col not in cat_columns + ['Pos_C', 'Pos_1B', 'Pos_2B', 'Pos_3B', 'Pos_SS', 'Pos_OF']]\n",
    "\n",
    "cat_transformer = Pipeline(\n",
    "    steps = [\n",
    "        ('onehot', OneHotEncoder(handle_unknown = 'ignore'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "num_transformer = Pipeline(\n",
    "    steps = [\n",
    "        ('scale', StandardScaler())\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers = [\n",
    "        ('cont', num_transformer, num_columns),\n",
    "        ('cat', cat_transformer, cat_columns)\n",
    "    ], remainder = 'passthrough'\n",
    ")\n",
    "\n",
    "X_transform = preprocessor.fit_transform(X)\n",
    "\n",
    "selected_features = np.concatenate([\n",
    "    np.array(num_columns),\n",
    "    np.array(preprocessor.transformers_[1][1]['onehot'].get_feature_names_out(cat_columns)),\n",
    "    np.array(['Pos_C', 'Pos_1B', 'Pos_2B', 'Pos_3B', 'Pos_SS', 'Pos_OF'])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Dense\n",
    "from bayes_opt import BayesianOptimization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_transform, y, test_size = .2, random_state = 621)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | batch_... | dropou... |  epochs   | layer_... | layer_... | layer_... | layer_... | layer_... | learni... |  neurons  | num_la... | patience  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 37\u001b[0m\n\u001b[1;32m     33\u001b[0m     pbounds[\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlayer_neurons_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m (\u001b[39m32\u001b[39m, \u001b[39m256\u001b[39m)\n\u001b[1;32m     35\u001b[0m optimizer \u001b[39m=\u001b[39m BayesianOptimization(f \u001b[39m=\u001b[39m dnn_model_score, pbounds \u001b[39m=\u001b[39m pbounds, random_state \u001b[39m=\u001b[39m \u001b[39m42\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m optimizer\u001b[39m.\u001b[39mmaximize(init_points \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m, n_iter \u001b[39m=\u001b[39m \u001b[39m15\u001b[39m)\n\u001b[1;32m     39\u001b[0m best_params \u001b[39m=\u001b[39m optimizer\u001b[39m.\u001b[39mmax[\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     40\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBest Hyperparameters:\u001b[39m\u001b[39m\"\u001b[39m, best_params)\n",
      "File \u001b[0;32m~/miniconda3/envs/pythonEnv/lib/python3.11/site-packages/bayes_opt/bayesian_optimization.py:310\u001b[0m, in \u001b[0;36mBayesianOptimization.maximize\u001b[0;34m(self, init_points, n_iter, acquisition_function, acq, kappa, kappa_decay, kappa_decay_delay, xi, **gp_params)\u001b[0m\n\u001b[1;32m    308\u001b[0m     x_probe \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msuggest(util)\n\u001b[1;32m    309\u001b[0m     iteration \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 310\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprobe(x_probe, lazy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    312\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bounds_transformer \u001b[39mand\u001b[39;00m iteration \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    313\u001b[0m     \u001b[39m# The bounds transformer should only modify the bounds after\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[39m# the init_points points (only for the true iterations)\u001b[39;00m\n\u001b[1;32m    315\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_bounds(\n\u001b[1;32m    316\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bounds_transformer\u001b[39m.\u001b[39mtransform(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_space))\n",
      "File \u001b[0;32m~/miniconda3/envs/pythonEnv/lib/python3.11/site-packages/bayes_opt/bayesian_optimization.py:208\u001b[0m, in \u001b[0;36mBayesianOptimization.probe\u001b[0;34m(self, params, lazy)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_queue\u001b[39m.\u001b[39madd(params)\n\u001b[1;32m    207\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 208\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_space\u001b[39m.\u001b[39mprobe(params)\n\u001b[1;32m    209\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch(Events\u001b[39m.\u001b[39mOPTIMIZATION_STEP)\n",
      "File \u001b[0;32m~/miniconda3/envs/pythonEnv/lib/python3.11/site-packages/bayes_opt/target_space.py:236\u001b[0m, in \u001b[0;36mTargetSpace.probe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    234\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_as_array(params)\n\u001b[1;32m    235\u001b[0m params \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_keys, x))\n\u001b[0;32m--> 236\u001b[0m target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_func(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[1;32m    238\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constraint \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    239\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mregister(x, target)\n",
      "Cell \u001b[0;32mIn[32], line 17\u001b[0m, in \u001b[0;36mdnn_model_score\u001b[0;34m(neurons, dropout_rate, learning_rate, epochs, batch_size, patience, num_layers, **layer_neurons)\u001b[0m\n\u001b[1;32m     13\u001b[0m model\u001b[39m.\u001b[39mcompile(loss \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmean_squared_error\u001b[39m\u001b[39m'\u001b[39m, optimizer \u001b[39m=\u001b[39m optimizer)\n\u001b[1;32m     15\u001b[0m es \u001b[39m=\u001b[39m EarlyStopping(monitor \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m, patience \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(patience), restore_best_weights \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 17\u001b[0m model\u001b[39m.\u001b[39mfit(X_train, y_train, validation_split \u001b[39m=\u001b[39m \u001b[39m.2\u001b[39m, epochs \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(epochs), batch_size \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(batch_size), callbacks \u001b[39m=\u001b[39m es, verbose \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[1;32m     19\u001b[0m y_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test)\n\u001b[1;32m     20\u001b[0m mse \u001b[39m=\u001b[39m mean_squared_error(y_test, y_pred)\n",
      "File \u001b[0;32m~/miniconda3/envs/pythonEnv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    118\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/pythonEnv/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:329\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[39mfor\u001b[39;00m step, iterator \u001b[39min\u001b[39;00m epoch_iterator\u001b[39m.\u001b[39menumerate_epoch():\n\u001b[1;32m    328\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 329\u001b[0m     logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_function(iterator)\n\u001b[1;32m    330\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_end(\n\u001b[1;32m    331\u001b[0m         step, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pythonify_logs(logs)\n\u001b[1;32m    332\u001b[0m     )\n\u001b[1;32m    333\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_training:\n",
      "File \u001b[0;32m~/miniconda3/envs/pythonEnv/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/pythonEnv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def dnn_model_score(neurons, dropout_rate, learning_rate, epochs, batch_size, patience, num_layers, **layer_neurons):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(int(neurons), activation='relu', input_shape = (X_train.shape[1],)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    for i in range(1, int(num_layers) + 1):\n",
    "        model.add(Dense(int(layer_neurons[f'layer_neurons_{i}']), activation='relu'))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(1, activation = 'linear'))\n",
    "\n",
    "    optimizer = Adam(learning_rate = learning_rate)\n",
    "    model.compile(loss = 'mean_squared_error', optimizer = optimizer)\n",
    "\n",
    "    es = EarlyStopping(monitor = 'val_loss', patience = int(patience), restore_best_weights = True)\n",
    "\n",
    "    model.fit(X_train, y_train, validation_split = .2, epochs = int(epochs), batch_size = int(batch_size), callbacks = es, verbose = 0)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    return -mse\n",
    "\n",
    "pbounds = {'neurons': (32, 256),\n",
    "           'dropout_rate': (0.0, 0.5),\n",
    "           'learning_rate': (0.01, 1),\n",
    "           'epochs' : (100, 500),\n",
    "           'batch_size' : (32, 500),\n",
    "           'patience' : (20, 50),\n",
    "           'num_layers': (1, 5)}\n",
    "\n",
    "for i in range(1, 6):\n",
    "    pbounds[f'layer_neurons_{i}'] = (32, 256)\n",
    "\n",
    "optimizer = BayesianOptimization(f = dnn_model_score, pbounds = pbounds, random_state = 42)\n",
    "\n",
    "optimizer.maximize(init_points = 5, n_iter = 10)\n",
    "\n",
    "best_params = optimizer.max['params']\n",
    "print(\"Best Hyperparameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = int(best_params['batch_size'])\n",
    "dropout_rate = best_params['dropout_rate']\n",
    "epochs = int(best_params['epochs'])\n",
    "neurons = []\n",
    "neurons.append(int(best_params['neurons']))\n",
    "neurons.append(int(best_params['layer_neurons_1']))\n",
    "neurons.append(int(best_params['layer_neurons_2']))\n",
    "neurons.append(int(best_params['layer_neurons_3']))\n",
    "neurons.append(int(best_params['layer_neurons_4']))\n",
    "neurons.append(int(best_params['layer_neurons_5']))\n",
    "learning_rate = best_params['learning_rate']\n",
    "num_layers = int(best_params['num_layers'])\n",
    "patience = int(best_params['patience'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_36\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_36\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ Dense1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">19,750</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Dropout1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Dense2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">154</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">38,654</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Dropout2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">154</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">155</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ Dense1 (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m)            │        \u001b[38;5;34m19,750\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Dropout1 (\u001b[38;5;33mDropout\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Dense2 (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m154\u001b[0m)            │        \u001b[38;5;34m38,654\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Dropout2 (\u001b[38;5;33mDropout\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m154\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Output (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m155\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">58,559</span> (228.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m58,559\u001b[0m (228.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">58,559</span> (228.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m58,559\u001b[0m (228.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(name = 'Dense1', units = neurons[0], input_dim = X_train.shape[1], activation = 'relu'))\n",
    "model.add(Dropout(name = 'Dropout1', rate = dropout_rate))\n",
    "\n",
    "for i in range(1, num_layers + 1):\n",
    "    model.add(Dense(name = f'Dense{i + 1}', units = neurons[i], activation = 'relu'))\n",
    "    model.add(Dropout(name = f'Dropout{i + 1}', rate = dropout_rate))\n",
    "\n",
    "model.add(Dense(name = 'Output', units = 1, activation = 'linear'))\n",
    "\n",
    "optimizer = Adam(learning_rate = learning_rate)\n",
    "\n",
    "model.compile(optimizer = optimizer, loss = 'mean_squared_error')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/262\n",
      "13/13 - 4s - 322ms/step - loss: 64334885552128.0000 - val_loss: 44592078520320.0000\n",
      "Epoch 2/262\n",
      "13/13 - 1s - 42ms/step - loss: 39064162009088.0000 - val_loss: 31156061863936.0000\n",
      "Epoch 3/262\n",
      "13/13 - 0s - 28ms/step - loss: 33310143152128.0000 - val_loss: 29430982377472.0000\n",
      "Epoch 4/262\n",
      "13/13 - 0s - 22ms/step - loss: 31566260600832.0000 - val_loss: 28177405575168.0000\n",
      "Epoch 5/262\n",
      "13/13 - 0s - 21ms/step - loss: 30229854683136.0000 - val_loss: 28030160338944.0000\n",
      "Epoch 6/262\n",
      "13/13 - 0s - 21ms/step - loss: 28853468659712.0000 - val_loss: 26432805797888.0000\n",
      "Epoch 7/262\n",
      "13/13 - 0s - 21ms/step - loss: 28282703577088.0000 - val_loss: 25963570135040.0000\n",
      "Epoch 8/262\n",
      "13/13 - 0s - 21ms/step - loss: 27693846364160.0000 - val_loss: 25452410306560.0000\n",
      "Epoch 9/262\n",
      "13/13 - 0s - 28ms/step - loss: 27265503068160.0000 - val_loss: 25740097617920.0000\n",
      "Epoch 10/262\n",
      "13/13 - 1s - 41ms/step - loss: 26875596374016.0000 - val_loss: 25089389101056.0000\n",
      "Epoch 11/262\n",
      "13/13 - 1s - 39ms/step - loss: 26795025891328.0000 - val_loss: 24839874150400.0000\n",
      "Epoch 12/262\n",
      "13/13 - 0s - 21ms/step - loss: 26416630464512.0000 - val_loss: 24733699538944.0000\n",
      "Epoch 13/262\n",
      "13/13 - 0s - 18ms/step - loss: 26017450164224.0000 - val_loss: 24678338920448.0000\n",
      "Epoch 14/262\n",
      "13/13 - 0s - 18ms/step - loss: 26081927102464.0000 - val_loss: 24369059332096.0000\n",
      "Epoch 15/262\n",
      "13/13 - 0s - 18ms/step - loss: 26002172411904.0000 - val_loss: 24028267937792.0000\n",
      "Epoch 16/262\n",
      "13/13 - 0s - 20ms/step - loss: 25723083423744.0000 - val_loss: 23858295865344.0000\n",
      "Epoch 17/262\n",
      "13/13 - 0s - 17ms/step - loss: 25284933844992.0000 - val_loss: 23873372291072.0000\n",
      "Epoch 18/262\n",
      "13/13 - 0s - 18ms/step - loss: 25193634332672.0000 - val_loss: 23860940374016.0000\n",
      "Epoch 19/262\n",
      "13/13 - 0s - 17ms/step - loss: 24901998084096.0000 - val_loss: 23713156169728.0000\n",
      "Epoch 20/262\n",
      "13/13 - 0s - 18ms/step - loss: 24356577083392.0000 - val_loss: 23225767559168.0000\n",
      "Epoch 21/262\n",
      "13/13 - 0s - 17ms/step - loss: 24324639555584.0000 - val_loss: 24055587536896.0000\n",
      "Epoch 22/262\n",
      "13/13 - 0s - 17ms/step - loss: 24048553689088.0000 - val_loss: 23513039634432.0000\n",
      "Epoch 23/262\n",
      "13/13 - 0s - 17ms/step - loss: 23939346595840.0000 - val_loss: 23072776126464.0000\n",
      "Epoch 24/262\n",
      "13/13 - 0s - 18ms/step - loss: 23322943291392.0000 - val_loss: 22858375888896.0000\n",
      "Epoch 25/262\n",
      "13/13 - 0s - 18ms/step - loss: 23424724369408.0000 - val_loss: 23315097845760.0000\n",
      "Epoch 26/262\n",
      "13/13 - 0s - 17ms/step - loss: 22784908460032.0000 - val_loss: 23150416887808.0000\n",
      "Epoch 27/262\n",
      "13/13 - 0s - 18ms/step - loss: 22449146036224.0000 - val_loss: 22514692521984.0000\n",
      "Epoch 28/262\n",
      "13/13 - 0s - 17ms/step - loss: 22231528767488.0000 - val_loss: 22622815387648.0000\n",
      "Epoch 29/262\n",
      "13/13 - 0s - 18ms/step - loss: 22028285378560.0000 - val_loss: 22261257994240.0000\n",
      "Epoch 30/262\n",
      "13/13 - 0s - 18ms/step - loss: 21823863390208.0000 - val_loss: 22129896587264.0000\n",
      "Epoch 31/262\n",
      "13/13 - 0s - 18ms/step - loss: 21364262043648.0000 - val_loss: 21895156072448.0000\n",
      "Epoch 32/262\n",
      "13/13 - 0s - 17ms/step - loss: 20923935621120.0000 - val_loss: 22905813467136.0000\n",
      "Epoch 33/262\n",
      "13/13 - 0s - 17ms/step - loss: 21290071097344.0000 - val_loss: 21860045553664.0000\n",
      "Epoch 34/262\n",
      "13/13 - 0s - 18ms/step - loss: 20617808052224.0000 - val_loss: 21817150406656.0000\n",
      "Epoch 35/262\n",
      "13/13 - 0s - 17ms/step - loss: 20507692892160.0000 - val_loss: 21529169494016.0000\n",
      "Epoch 36/262\n",
      "13/13 - 0s - 17ms/step - loss: 20090212843520.0000 - val_loss: 21359816081408.0000\n",
      "Epoch 37/262\n",
      "13/13 - 0s - 17ms/step - loss: 19291034353664.0000 - val_loss: 20867369140224.0000\n",
      "Epoch 38/262\n",
      "13/13 - 0s - 18ms/step - loss: 19175613399040.0000 - val_loss: 21094373261312.0000\n",
      "Epoch 39/262\n",
      "13/13 - 0s - 18ms/step - loss: 18782969921536.0000 - val_loss: 20854146596864.0000\n",
      "Epoch 40/262\n",
      "13/13 - 0s - 18ms/step - loss: 18802465046528.0000 - val_loss: 20745327476736.0000\n",
      "Epoch 41/262\n",
      "13/13 - 0s - 18ms/step - loss: 18577147035648.0000 - val_loss: 21055571755008.0000\n",
      "Epoch 42/262\n",
      "13/13 - 0s - 18ms/step - loss: 17873229578240.0000 - val_loss: 20928786333696.0000\n",
      "Epoch 43/262\n",
      "13/13 - 0s - 19ms/step - loss: 18265495568384.0000 - val_loss: 20312995397632.0000\n",
      "Epoch 44/262\n",
      "13/13 - 0s - 18ms/step - loss: 17931779964928.0000 - val_loss: 21138702860288.0000\n",
      "Epoch 45/262\n",
      "13/13 - 0s - 18ms/step - loss: 18104094556160.0000 - val_loss: 21450790535168.0000\n",
      "Epoch 46/262\n",
      "13/13 - 0s - 17ms/step - loss: 18416660381696.0000 - val_loss: 20139265228800.0000\n",
      "Epoch 47/262\n",
      "13/13 - 0s - 18ms/step - loss: 16787847512064.0000 - val_loss: 19933362651136.0000\n",
      "Epoch 48/262\n",
      "13/13 - 0s - 18ms/step - loss: 16638359371776.0000 - val_loss: 20560555802624.0000\n",
      "Epoch 49/262\n",
      "13/13 - 0s - 17ms/step - loss: 16454420267008.0000 - val_loss: 20066315796480.0000\n",
      "Epoch 50/262\n",
      "13/13 - 0s - 27ms/step - loss: 16406905094144.0000 - val_loss: 20821854650368.0000\n",
      "Epoch 51/262\n",
      "13/13 - 0s - 18ms/step - loss: 16353049182208.0000 - val_loss: 20389394644992.0000\n",
      "Epoch 52/262\n",
      "13/13 - 0s - 17ms/step - loss: 16228383981568.0000 - val_loss: 20223677693952.0000\n",
      "Epoch 53/262\n",
      "13/13 - 0s - 18ms/step - loss: 15620169007104.0000 - val_loss: 19981242728448.0000\n",
      "Epoch 54/262\n",
      "13/13 - 0s - 18ms/step - loss: 15448077762560.0000 - val_loss: 19876787781632.0000\n",
      "Epoch 55/262\n",
      "13/13 - 0s - 18ms/step - loss: 15149862748160.0000 - val_loss: 20116238499840.0000\n",
      "Epoch 56/262\n",
      "13/13 - 0s - 18ms/step - loss: 15037526704128.0000 - val_loss: 20034608955392.0000\n",
      "Epoch 57/262\n",
      "13/13 - 0s - 18ms/step - loss: 15142271057920.0000 - val_loss: 20388541104128.0000\n",
      "Epoch 58/262\n",
      "13/13 - 0s - 17ms/step - loss: 15205035671552.0000 - val_loss: 20878572126208.0000\n",
      "Epoch 59/262\n",
      "13/13 - 0s - 18ms/step - loss: 14587006025728.0000 - val_loss: 20102867058688.0000\n",
      "Epoch 60/262\n",
      "13/13 - 0s - 18ms/step - loss: 13526393421824.0000 - val_loss: 19985032282112.0000\n",
      "Epoch 61/262\n",
      "13/13 - 0s - 17ms/step - loss: 13210359955456.0000 - val_loss: 19670191046656.0000\n",
      "Epoch 62/262\n",
      "13/13 - 0s - 17ms/step - loss: 13419860197376.0000 - val_loss: 20030165090304.0000\n",
      "Epoch 63/262\n",
      "13/13 - 0s - 17ms/step - loss: 13630347149312.0000 - val_loss: 19611875540992.0000\n",
      "Epoch 64/262\n",
      "13/13 - 0s - 17ms/step - loss: 12935130775552.0000 - val_loss: 20317766418432.0000\n",
      "Epoch 65/262\n",
      "13/13 - 0s - 35ms/step - loss: 12710354878464.0000 - val_loss: 19624995323904.0000\n",
      "Epoch 66/262\n",
      "13/13 - 0s - 19ms/step - loss: 12237076955136.0000 - val_loss: 19779792404480.0000\n",
      "Epoch 67/262\n",
      "13/13 - 0s - 19ms/step - loss: 12434731433984.0000 - val_loss: 20089921339392.0000\n",
      "Epoch 68/262\n",
      "13/13 - 0s - 20ms/step - loss: 12279650189312.0000 - val_loss: 19569850712064.0000\n",
      "Epoch 69/262\n",
      "13/13 - 0s - 19ms/step - loss: 12442704805888.0000 - val_loss: 20199002603520.0000\n",
      "Epoch 70/262\n",
      "13/13 - 0s - 19ms/step - loss: 12281109807104.0000 - val_loss: 19961634357248.0000\n",
      "Epoch 71/262\n",
      "13/13 - 0s - 18ms/step - loss: 11562668523520.0000 - val_loss: 19896448581632.0000\n",
      "Epoch 72/262\n",
      "13/13 - 0s - 19ms/step - loss: 11275819024384.0000 - val_loss: 20414856167424.0000\n",
      "Epoch 73/262\n",
      "13/13 - 0s - 19ms/step - loss: 11189766586368.0000 - val_loss: 19614614421504.0000\n",
      "Epoch 74/262\n",
      "13/13 - 0s - 19ms/step - loss: 11280992698368.0000 - val_loss: 21269760180224.0000\n",
      "Epoch 75/262\n",
      "13/13 - 0s - 18ms/step - loss: 11442625445888.0000 - val_loss: 19909849382912.0000\n",
      "Epoch 76/262\n",
      "13/13 - 0s - 18ms/step - loss: 10957315112960.0000 - val_loss: 19943999406080.0000\n",
      "Epoch 77/262\n",
      "13/13 - 0s - 18ms/step - loss: 10617809272832.0000 - val_loss: 20190519623680.0000\n",
      "Epoch 78/262\n",
      "13/13 - 0s - 17ms/step - loss: 11187109494784.0000 - val_loss: 20109554876416.0000\n",
      "Epoch 79/262\n",
      "13/13 - 0s - 17ms/step - loss: 10996308508672.0000 - val_loss: 20135104479232.0000\n",
      "Epoch 80/262\n",
      "13/13 - 0s - 18ms/step - loss: 10571712823296.0000 - val_loss: 20285554163712.0000\n",
      "Epoch 81/262\n",
      "13/13 - 0s - 18ms/step - loss: 10787940728832.0000 - val_loss: 20284023242752.0000\n",
      "Epoch 82/262\n",
      "13/13 - 0s - 18ms/step - loss: 11128666062848.0000 - val_loss: 20307500859392.0000\n",
      "Epoch 83/262\n",
      "13/13 - 0s - 18ms/step - loss: 9907200851968.0000 - val_loss: 19915304075264.0000\n",
      "Epoch 84/262\n",
      "13/13 - 0s - 18ms/step - loss: 9989026480128.0000 - val_loss: 19663702458368.0000\n",
      "Epoch 85/262\n",
      "13/13 - 0s - 18ms/step - loss: 9739441274880.0000 - val_loss: 19793809768448.0000\n",
      "Epoch 86/262\n",
      "13/13 - 0s - 18ms/step - loss: 9509657378816.0000 - val_loss: 20527091548160.0000\n",
      "Epoch 87/262\n",
      "13/13 - 0s - 18ms/step - loss: 9374787436544.0000 - val_loss: 20188974022656.0000\n",
      "Epoch 88/262\n",
      "13/13 - 0s - 18ms/step - loss: 9259380113408.0000 - val_loss: 20137690267648.0000\n",
      "Epoch 89/262\n",
      "13/13 - 0s - 19ms/step - loss: 9383913193472.0000 - val_loss: 20312783585280.0000\n",
      "Epoch 90/262\n",
      "13/13 - 0s - 18ms/step - loss: 9456651862016.0000 - val_loss: 20215987437568.0000\n",
      "Epoch 91/262\n",
      "13/13 - 0s - 26ms/step - loss: 9462359261184.0000 - val_loss: 20206321664000.0000\n"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor = 'val_loss', patience = int(patience), restore_best_weights = True)\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_split = .2, batch_size = batch_size, epochs = epochs, callbacks = es, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n"
     ]
    }
   ],
   "source": [
    "test_preds = model.predict(X_test)\n",
    "train_preds = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Neural Network Metrics:\n",
      "Train RMSE: 3462586.252572907\n",
      "Test RMSE: 4789954.3738712855\n"
     ]
    }
   ],
   "source": [
    "train_rmse = mean_squared_error(y_train, train_preds)\n",
    "test_rmse = mean_squared_error(y_test, test_preds)\n",
    "\n",
    "print('Deep Neural Network Metrics:')\n",
    "print(f'Train RMSE: {np.sqrt(train_rmse)}')\n",
    "print(f'Test RMSE: {np.sqrt(test_rmse)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['best_model/best_preprocssor.joblib']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the model, pca, and preprocessor so that new data can be fit using the same criteria\n",
    "model.save('best_model/best_model.keras')\n",
    "#joblib.dump(pca, 'dnn/pca45.joblib')\n",
    "joblib.dump(preprocessor, 'best_model/best_preprocssor.joblib')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
