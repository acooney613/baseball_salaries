{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseball = pd.read_csv('best_model/engineered_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = baseball.drop(['Salary'], axis = 1)\n",
    "y = baseball['Salary']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .25, random_state = 621)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = ['Tm', 'Lg', 'Acquired', 'Bat']\n",
    "num_columns = [col for col in X.columns if col not in cat_columns + ['Pos_C', 'Pos_1B', 'Pos_2B', 'Pos_3B', 'Pos_SS', 'Pos_OF']]\n",
    "\n",
    "cat_transformer = Pipeline(\n",
    "    steps = [\n",
    "        ('onehot', OneHotEncoder(handle_unknown = 'ignore'))\n",
    "    ]\n",
    ")\n",
    "\n",
    "num_transformer = Pipeline(\n",
    "    steps = [\n",
    "        ('scale', StandardScaler())\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers = [\n",
    "        ('cont', num_transformer, num_columns),\n",
    "        ('cat', cat_transformer, cat_columns)\n",
    "    ], remainder = 'passthrough'\n",
    ")\n",
    "\n",
    "X_transform = preprocessor.fit_transform(X)\n",
    "\n",
    "selected_features = np.concatenate([\n",
    "    np.array(num_columns),\n",
    "    np.array(preprocessor.transformers_[1][1]['onehot'].get_feature_names_out(cat_columns)),\n",
    "    np.array(['Pos_C', 'Pos_1B', 'Pos_2B', 'Pos_3B', 'Pos_SS', 'Pos_OF'])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Dense\n",
    "from bayes_opt import BayesianOptimization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "import warnings\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_transform, y, test_size = .2, random_state = 621)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | batch_... | dropou... |  epochs   | layer_... | layer_... | layer_... | layer_... | layer_... | learni... |  neurons  | num_la... | patience  |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "| \u001b[0m1        \u001b[0m | \u001b[0m-2.276e+1\u001b[0m | \u001b[0m207.3    \u001b[0m | \u001b[0m0.4754   \u001b[0m | \u001b[0m392.8    \u001b[0m | \u001b[0m166.1    \u001b[0m | \u001b[0m66.95    \u001b[0m | \u001b[0m66.94    \u001b[0m | \u001b[0m45.01    \u001b[0m | \u001b[0m226.0    \u001b[0m | \u001b[0m0.6051   \u001b[0m | \u001b[0m190.6    \u001b[0m | \u001b[0m1.082    \u001b[0m | \u001b[0m49.1     \u001b[0m |\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "| \u001b[0m2        \u001b[0m | \u001b[0m-2.398e+1\u001b[0m | \u001b[0m421.6    \u001b[0m | \u001b[0m0.1062   \u001b[0m | \u001b[0m172.7    \u001b[0m | \u001b[0m73.08    \u001b[0m | \u001b[0m100.2    \u001b[0m | \u001b[0m149.5    \u001b[0m | \u001b[0m128.8    \u001b[0m | \u001b[0m97.24    \u001b[0m | \u001b[0m0.6157   \u001b[0m | \u001b[0m63.25    \u001b[0m | \u001b[0m2.169    \u001b[0m | \u001b[0m30.99    \u001b[0m |\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "| \u001b[0m3        \u001b[0m | \u001b[0m-2.405e+1\u001b[0m | \u001b[0m245.4    \u001b[0m | \u001b[0m0.3926   \u001b[0m | \u001b[0m179.9    \u001b[0m | \u001b[0m147.2    \u001b[0m | \u001b[0m164.7    \u001b[0m | \u001b[0m42.4     \u001b[0m | \u001b[0m168.1    \u001b[0m | \u001b[0m70.2     \u001b[0m | \u001b[0m0.0744   \u001b[0m | \u001b[0m244.6    \u001b[0m | \u001b[0m4.863    \u001b[0m | \u001b[0m44.25    \u001b[0m |\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "| \u001b[0m4        \u001b[0m | \u001b[0m-2.39e+13\u001b[0m | \u001b[0m174.6    \u001b[0m | \u001b[0m0.04884  \u001b[0m | \u001b[0m373.7    \u001b[0m | \u001b[0m130.6    \u001b[0m | \u001b[0m59.34    \u001b[0m | \u001b[0m142.9    \u001b[0m | \u001b[0m39.7     \u001b[0m | \u001b[0m235.7    \u001b[0m | \u001b[0m0.2662   \u001b[0m | \u001b[0m180.4    \u001b[0m | \u001b[0m2.247    \u001b[0m | \u001b[0m35.6     \u001b[0m |\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "| \u001b[0m5        \u001b[0m | \u001b[0m-2.278e+1\u001b[0m | \u001b[0m287.9    \u001b[0m | \u001b[0m0.09243  \u001b[0m | \u001b[0m487.8    \u001b[0m | \u001b[0m205.6    \u001b[0m | \u001b[0m242.4    \u001b[0m | \u001b[0m232.4    \u001b[0m | \u001b[0m165.9    \u001b[0m | \u001b[0m238.5    \u001b[0m | \u001b[0m0.09761  \u001b[0m | \u001b[0m75.9     \u001b[0m | \u001b[0m1.181    \u001b[0m | \u001b[0m29.76    \u001b[0m |\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "| \u001b[95m6        \u001b[0m | \u001b[95m-2.128e+1\u001b[0m | \u001b[95m259.3    \u001b[0m | \u001b[95m0.354    \u001b[0m | \u001b[95m263.0    \u001b[0m | \u001b[95m154.4    \u001b[0m | \u001b[95m218.6    \u001b[0m | \u001b[95m143.3    \u001b[0m | \u001b[95m158.1    \u001b[0m | \u001b[95m126.6    \u001b[0m | \u001b[95m0.1988   \u001b[0m | \u001b[95m250.1    \u001b[0m | \u001b[95m1.082    \u001b[0m | \u001b[95m23.73    \u001b[0m |\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "| \u001b[0m7        \u001b[0m | \u001b[0m-2.303e+1\u001b[0m | \u001b[0m353.1    \u001b[0m | \u001b[0m0.3643   \u001b[0m | \u001b[0m410.4    \u001b[0m | \u001b[0m232.1    \u001b[0m | \u001b[0m166.2    \u001b[0m | \u001b[0m254.1    \u001b[0m | \u001b[0m59.43    \u001b[0m | \u001b[0m206.1    \u001b[0m | \u001b[0m0.06139  \u001b[0m | \u001b[0m62.54    \u001b[0m | \u001b[0m3.695    \u001b[0m | \u001b[0m35.34    \u001b[0m |\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "| \u001b[0m8        \u001b[0m | \u001b[0m-2.354e+1\u001b[0m | \u001b[0m439.1    \u001b[0m | \u001b[0m0.3421   \u001b[0m | \u001b[0m194.0    \u001b[0m | \u001b[0m49.28    \u001b[0m | \u001b[0m243.3    \u001b[0m | \u001b[0m170.7    \u001b[0m | \u001b[0m131.0    \u001b[0m | \u001b[0m83.59    \u001b[0m | \u001b[0m0.1348   \u001b[0m | \u001b[0m86.74    \u001b[0m | \u001b[0m4.043    \u001b[0m | \u001b[0m20.82    \u001b[0m |\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "| \u001b[0m9        \u001b[0m | \u001b[0m-6.577e+1\u001b[0m | \u001b[0m451.2    \u001b[0m | \u001b[0m0.05307  \u001b[0m | \u001b[0m186.9    \u001b[0m | \u001b[0m114.3    \u001b[0m | \u001b[0m94.19    \u001b[0m | \u001b[0m165.8    \u001b[0m | \u001b[0m54.16    \u001b[0m | \u001b[0m225.9    \u001b[0m | \u001b[0m0.5165   \u001b[0m | \u001b[0m36.21    \u001b[0m | \u001b[0m4.075    \u001b[0m | \u001b[0m36.53    \u001b[0m |\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "| \u001b[0m10       \u001b[0m | \u001b[0m-2.21e+13\u001b[0m | \u001b[0m225.3    \u001b[0m | \u001b[0m0.3572   \u001b[0m | \u001b[0m311.2    \u001b[0m | \u001b[0m154.7    \u001b[0m | \u001b[0m138.4    \u001b[0m | \u001b[0m102.8    \u001b[0m | \u001b[0m107.7    \u001b[0m | \u001b[0m162.6    \u001b[0m | \u001b[0m0.2999   \u001b[0m | \u001b[0m218.7    \u001b[0m | \u001b[0m1.951    \u001b[0m | \u001b[0m37.25    \u001b[0m |\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "| \u001b[0m11       \u001b[0m | \u001b[0m-2.254e+1\u001b[0m | \u001b[0m335.5    \u001b[0m | \u001b[0m0.2401   \u001b[0m | \u001b[0m222.5    \u001b[0m | \u001b[0m76.78    \u001b[0m | \u001b[0m145.2    \u001b[0m | \u001b[0m145.0    \u001b[0m | \u001b[0m181.8    \u001b[0m | \u001b[0m32.0     \u001b[0m | \u001b[0m0.5517   \u001b[0m | \u001b[0m145.9    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m23.8     \u001b[0m |\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "| \u001b[0m12       \u001b[0m | \u001b[0m-6.577e+1\u001b[0m | \u001b[0m234.5    \u001b[0m | \u001b[0m0.2632   \u001b[0m | \u001b[0m344.6    \u001b[0m | \u001b[0m105.0    \u001b[0m | \u001b[0m249.8    \u001b[0m | \u001b[0m70.72    \u001b[0m | \u001b[0m197.2    \u001b[0m | \u001b[0m103.6    \u001b[0m | \u001b[0m0.148    \u001b[0m | \u001b[0m132.6    \u001b[0m | \u001b[0m4.014    \u001b[0m | \u001b[0m24.55    \u001b[0m |\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "| \u001b[0m13       \u001b[0m | \u001b[0m-2.191e+1\u001b[0m | \u001b[0m289.4    \u001b[0m | \u001b[0m0.344    \u001b[0m | \u001b[0m224.1    \u001b[0m | \u001b[0m136.1    \u001b[0m | \u001b[0m125.1    \u001b[0m | \u001b[0m143.4    \u001b[0m | \u001b[0m127.0    \u001b[0m | \u001b[0m103.3    \u001b[0m | \u001b[0m0.4165   \u001b[0m | \u001b[0m231.6    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m32.58    \u001b[0m |\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "| \u001b[0m14       \u001b[0m | \u001b[0m-2.329e+1\u001b[0m | \u001b[0m425.6    \u001b[0m | \u001b[0m0.02058  \u001b[0m | \u001b[0m172.4    \u001b[0m | \u001b[0m72.45    \u001b[0m | \u001b[0m109.6    \u001b[0m | \u001b[0m154.5    \u001b[0m | \u001b[0m131.5    \u001b[0m | \u001b[0m109.4    \u001b[0m | \u001b[0m0.711    \u001b[0m | \u001b[0m73.33    \u001b[0m | \u001b[0m1.62     \u001b[0m | \u001b[0m29.4     \u001b[0m |\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "| \u001b[0m15       \u001b[0m | \u001b[0m-2.248e+1\u001b[0m | \u001b[0m438.7    \u001b[0m | \u001b[0m0.0      \u001b[0m | \u001b[0m103.6    \u001b[0m | \u001b[0m40.63    \u001b[0m | \u001b[0m131.6    \u001b[0m | \u001b[0m169.5    \u001b[0m | \u001b[0m201.1    \u001b[0m | \u001b[0m44.3     \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m169.6    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m20.71    \u001b[0m |\n",
      "=========================================================================================================================================================================\n",
      "Best Hyperparameters: {'batch_size': 259.3101930843625, 'dropout_rate': 0.3540292961915878, 'epochs': 262.977231252349, 'layer_neurons_1': 154.38449655080007, 'layer_neurons_2': 218.55366767053115, 'layer_neurons_3': 143.25039750874907, 'layer_neurons_4': 158.1390452675157, 'layer_neurons_5': 126.56498333121105, 'learning_rate': 0.1987586896871326, 'neurons': 250.05524415819025, 'num_layers': 1.0822858400287743, 'patience': 23.72615181804609}\n"
     ]
    }
   ],
   "source": [
    "def dnn_model_score(neurons, dropout_rate, learning_rate, epochs, batch_size, patience, num_layers, **layer_neurons):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(int(neurons), activation='relu', input_shape = (X_train.shape[1],)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    for i in range(1, int(num_layers) + 1):\n",
    "        model.add(Dense(int(layer_neurons[f'layer_neurons_{i}']), activation='relu'))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(1, activation = 'linear'))\n",
    "\n",
    "    optimizer = Adam(learning_rate = learning_rate)\n",
    "    model.compile(loss = 'mean_squared_error', optimizer = optimizer)\n",
    "\n",
    "    es = EarlyStopping(monitor = 'val_loss', patience = int(patience), restore_best_weights = True)\n",
    "\n",
    "    model.fit(X_train, y_train, validation_split = .2, epochs = int(epochs), batch_size = int(batch_size), callbacks = es, verbose = 0)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    return -mse\n",
    "\n",
    "pbounds = {'neurons': (32, 256),\n",
    "           'dropout_rate': (0.0, 0.5),\n",
    "           'learning_rate': (0.01, 1),\n",
    "           'epochs' : (100, 500),\n",
    "           'batch_size' : (32, 500),\n",
    "           'patience' : (20, 50),\n",
    "           'num_layers': (1, 5)}\n",
    "\n",
    "for i in range(1, 6):\n",
    "    pbounds[f'layer_neurons_{i}'] = (32, 256)\n",
    "\n",
    "optimizer = BayesianOptimization(f = dnn_model_score, pbounds = pbounds, random_state = 42)\n",
    "\n",
    "optimizer.maximize(init_points = 5, n_iter = 10)\n",
    "\n",
    "best_params = optimizer.max['params']\n",
    "print(\"Best Hyperparameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = int(best_params['batch_size'])\n",
    "dropout_rate = best_params['dropout_rate']\n",
    "epochs = int(best_params['epochs'])\n",
    "neurons = []\n",
    "neurons.append(int(best_params['neurons']))\n",
    "neurons.append(int(best_params['layer_neurons_1']))\n",
    "neurons.append(int(best_params['layer_neurons_2']))\n",
    "neurons.append(int(best_params['layer_neurons_3']))\n",
    "neurons.append(int(best_params['layer_neurons_4']))\n",
    "neurons.append(int(best_params['layer_neurons_5']))\n",
    "learning_rate = best_params['learning_rate']\n",
    "num_layers = int(best_params['num_layers'])\n",
    "patience = int(best_params['patience'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_15\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_15\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ Dense1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">19,750</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Dropout1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">250</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Dense2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">154</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">38,654</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Dropout2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">154</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">155</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ Dense1 (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m)            │        \u001b[38;5;34m19,750\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Dropout1 (\u001b[38;5;33mDropout\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m250\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Dense2 (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m154\u001b[0m)            │        \u001b[38;5;34m38,654\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Dropout2 (\u001b[38;5;33mDropout\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m154\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Output (\u001b[38;5;33mDense\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m155\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">58,559</span> (228.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m58,559\u001b[0m (228.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">58,559</span> (228.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m58,559\u001b[0m (228.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(name = 'Dense1', units = neurons[0], input_dim = X_train.shape[1], activation = 'relu'))\n",
    "model.add(Dropout(name = 'Dropout1', rate = dropout_rate))\n",
    "\n",
    "for i in range(1, num_layers + 1):\n",
    "    model.add(Dense(name = f'Dense{i + 1}', units = neurons[i], activation = 'relu'))\n",
    "    model.add(Dropout(name = f'Dropout{i + 1}', rate = dropout_rate))\n",
    "\n",
    "model.add(Dense(name = 'Output', units = 1, activation = 'linear'))\n",
    "\n",
    "optimizer = Adam(learning_rate = learning_rate)\n",
    "\n",
    "model.compile(optimizer = optimizer, loss = 'mean_squared_error')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/262\n",
      "13/13 - 3s - 199ms/step - loss: 63482414235648.0000 - val_loss: 42525863706624.0000\n",
      "Epoch 2/262\n",
      "13/13 - 0s - 18ms/step - loss: 39605755707392.0000 - val_loss: 30419678396416.0000\n",
      "Epoch 3/262\n",
      "13/13 - 0s - 19ms/step - loss: 33617648549888.0000 - val_loss: 29283718266880.0000\n",
      "Epoch 4/262\n",
      "13/13 - 0s - 18ms/step - loss: 31119057616896.0000 - val_loss: 27809168752640.0000\n",
      "Epoch 5/262\n",
      "13/13 - 0s - 18ms/step - loss: 29824003342336.0000 - val_loss: 27760309305344.0000\n",
      "Epoch 6/262\n",
      "13/13 - 0s - 20ms/step - loss: 28683423186944.0000 - val_loss: 26364637872128.0000\n",
      "Epoch 7/262\n",
      "13/13 - 0s - 20ms/step - loss: 28391474462720.0000 - val_loss: 25859113091072.0000\n",
      "Epoch 8/262\n",
      "13/13 - 0s - 19ms/step - loss: 28075643371520.0000 - val_loss: 25343484231680.0000\n",
      "Epoch 9/262\n",
      "13/13 - 0s - 22ms/step - loss: 27096418091008.0000 - val_loss: 25264018948096.0000\n",
      "Epoch 10/262\n",
      "13/13 - 0s - 18ms/step - loss: 26878295408640.0000 - val_loss: 24960674299904.0000\n",
      "Epoch 11/262\n",
      "13/13 - 0s - 22ms/step - loss: 26420826865664.0000 - val_loss: 24780583469056.0000\n",
      "Epoch 12/262\n",
      "13/13 - 0s - 19ms/step - loss: 25947086520320.0000 - val_loss: 24619589304320.0000\n",
      "Epoch 13/262\n",
      "13/13 - 0s - 18ms/step - loss: 25989417533440.0000 - val_loss: 24324354342912.0000\n",
      "Epoch 14/262\n",
      "13/13 - 0s - 20ms/step - loss: 25695967248384.0000 - val_loss: 24724539179008.0000\n",
      "Epoch 15/262\n",
      "13/13 - 0s - 20ms/step - loss: 25720199839744.0000 - val_loss: 23984584261632.0000\n",
      "Epoch 16/262\n",
      "13/13 - 0s - 19ms/step - loss: 25270945841152.0000 - val_loss: 24242223579136.0000\n",
      "Epoch 17/262\n",
      "13/13 - 0s - 17ms/step - loss: 25203459489792.0000 - val_loss: 24081520918528.0000\n",
      "Epoch 18/262\n",
      "13/13 - 0s - 29ms/step - loss: 24699075559424.0000 - val_loss: 23720793997312.0000\n",
      "Epoch 19/262\n",
      "13/13 - 0s - 34ms/step - loss: 24634871250944.0000 - val_loss: 23347842777088.0000\n",
      "Epoch 20/262\n",
      "13/13 - 0s - 34ms/step - loss: 24287199100928.0000 - val_loss: 23410121900032.0000\n",
      "Epoch 21/262\n",
      "13/13 - 0s - 22ms/step - loss: 24106212786176.0000 - val_loss: 23274102718464.0000\n",
      "Epoch 22/262\n",
      "13/13 - 0s - 19ms/step - loss: 23976906588160.0000 - val_loss: 23158289596416.0000\n",
      "Epoch 23/262\n",
      "13/13 - 0s - 26ms/step - loss: 23231287263232.0000 - val_loss: 23274188701696.0000\n",
      "Epoch 24/262\n",
      "13/13 - 0s - 27ms/step - loss: 23296951189504.0000 - val_loss: 22993619124224.0000\n",
      "Epoch 25/262\n",
      "13/13 - 0s - 18ms/step - loss: 23213757169664.0000 - val_loss: 23485590011904.0000\n",
      "Epoch 26/262\n",
      "13/13 - 0s - 18ms/step - loss: 23143055884288.0000 - val_loss: 22764031311872.0000\n",
      "Epoch 27/262\n",
      "13/13 - 0s - 17ms/step - loss: 22680350752768.0000 - val_loss: 22716253995008.0000\n",
      "Epoch 28/262\n",
      "13/13 - 0s - 16ms/step - loss: 22521315328000.0000 - val_loss: 22620355428352.0000\n",
      "Epoch 29/262\n",
      "13/13 - 0s - 17ms/step - loss: 21823131484160.0000 - val_loss: 22247068663808.0000\n",
      "Epoch 30/262\n",
      "13/13 - 0s - 24ms/step - loss: 21805819494400.0000 - val_loss: 22112100155392.0000\n",
      "Epoch 31/262\n",
      "13/13 - 0s - 16ms/step - loss: 21228685361152.0000 - val_loss: 21883370078208.0000\n",
      "Epoch 32/262\n",
      "13/13 - 0s - 17ms/step - loss: 21359275016192.0000 - val_loss: 21988915544064.0000\n",
      "Epoch 33/262\n",
      "13/13 - 0s - 17ms/step - loss: 20721136828416.0000 - val_loss: 21883336523776.0000\n",
      "Epoch 34/262\n",
      "13/13 - 0s - 19ms/step - loss: 20090093305856.0000 - val_loss: 21681368203264.0000\n",
      "Epoch 35/262\n",
      "13/13 - 0s - 17ms/step - loss: 19944056029184.0000 - val_loss: 21806184398848.0000\n",
      "Epoch 36/262\n",
      "13/13 - 0s - 16ms/step - loss: 19810907848704.0000 - val_loss: 21490372182016.0000\n",
      "Epoch 37/262\n",
      "13/13 - 0s - 16ms/step - loss: 19273634283520.0000 - val_loss: 21212788948992.0000\n",
      "Epoch 38/262\n",
      "13/13 - 0s - 18ms/step - loss: 19011381231616.0000 - val_loss: 21034147250176.0000\n",
      "Epoch 39/262\n",
      "13/13 - 0s - 18ms/step - loss: 18866589663232.0000 - val_loss: 21204670873600.0000\n",
      "Epoch 40/262\n",
      "13/13 - 0s - 17ms/step - loss: 18293775663104.0000 - val_loss: 20551829553152.0000\n",
      "Epoch 41/262\n",
      "13/13 - 0s - 17ms/step - loss: 17816157683712.0000 - val_loss: 21285279105024.0000\n",
      "Epoch 42/262\n",
      "13/13 - 0s - 17ms/step - loss: 17837563314176.0000 - val_loss: 20525118128128.0000\n",
      "Epoch 43/262\n",
      "13/13 - 0s - 17ms/step - loss: 17969931354112.0000 - val_loss: 20732866199552.0000\n",
      "Epoch 44/262\n",
      "13/13 - 0s - 17ms/step - loss: 17798883442688.0000 - val_loss: 21341140942848.0000\n",
      "Epoch 45/262\n",
      "13/13 - 0s - 18ms/step - loss: 16754992480256.0000 - val_loss: 20426436640768.0000\n",
      "Epoch 46/262\n",
      "13/13 - 0s - 17ms/step - loss: 16710783467520.0000 - val_loss: 20586698899456.0000\n",
      "Epoch 47/262\n",
      "13/13 - 0s - 18ms/step - loss: 16513666908160.0000 - val_loss: 20472993415168.0000\n",
      "Epoch 48/262\n",
      "13/13 - 0s - 17ms/step - loss: 16332427886592.0000 - val_loss: 20522605740032.0000\n",
      "Epoch 49/262\n",
      "13/13 - 0s - 17ms/step - loss: 15950408581120.0000 - val_loss: 20011531894784.0000\n",
      "Epoch 50/262\n",
      "13/13 - 0s - 18ms/step - loss: 16148893532160.0000 - val_loss: 20262862979072.0000\n",
      "Epoch 51/262\n",
      "13/13 - 0s - 16ms/step - loss: 15587536273408.0000 - val_loss: 20217262505984.0000\n",
      "Epoch 52/262\n",
      "13/13 - 0s - 17ms/step - loss: 15705859686400.0000 - val_loss: 20050637488128.0000\n",
      "Epoch 53/262\n",
      "13/13 - 0s - 17ms/step - loss: 15307812896768.0000 - val_loss: 20038440452096.0000\n",
      "Epoch 54/262\n",
      "13/13 - 0s - 17ms/step - loss: 15117249937408.0000 - val_loss: 20223845466112.0000\n",
      "Epoch 55/262\n",
      "13/13 - 0s - 17ms/step - loss: 14694511280128.0000 - val_loss: 20609392181248.0000\n",
      "Epoch 56/262\n",
      "13/13 - 0s - 17ms/step - loss: 15135129206784.0000 - val_loss: 20386737553408.0000\n",
      "Epoch 57/262\n",
      "13/13 - 0s - 17ms/step - loss: 14341642387456.0000 - val_loss: 20366818803712.0000\n",
      "Epoch 58/262\n",
      "13/13 - 0s - 16ms/step - loss: 14130251563008.0000 - val_loss: 20962273656832.0000\n",
      "Epoch 59/262\n",
      "13/13 - 0s - 17ms/step - loss: 14045468950528.0000 - val_loss: 19760335028224.0000\n",
      "Epoch 60/262\n",
      "13/13 - 0s - 17ms/step - loss: 13856238731264.0000 - val_loss: 20536748933120.0000\n",
      "Epoch 61/262\n",
      "13/13 - 0s - 17ms/step - loss: 13677707132928.0000 - val_loss: 20717733150720.0000\n",
      "Epoch 62/262\n",
      "13/13 - 0s - 16ms/step - loss: 13790036885504.0000 - val_loss: 20116955725824.0000\n",
      "Epoch 63/262\n",
      "13/13 - 0s - 17ms/step - loss: 13127941881856.0000 - val_loss: 20812620890112.0000\n",
      "Epoch 64/262\n",
      "13/13 - 0s - 17ms/step - loss: 12801664876544.0000 - val_loss: 20253838934016.0000\n",
      "Epoch 65/262\n",
      "13/13 - 0s - 17ms/step - loss: 12410354139136.0000 - val_loss: 19969878261760.0000\n",
      "Epoch 66/262\n",
      "13/13 - 0s - 17ms/step - loss: 12533490515968.0000 - val_loss: 19600657874944.0000\n",
      "Epoch 67/262\n",
      "13/13 - 0s - 24ms/step - loss: 11894876274688.0000 - val_loss: 20067282583552.0000\n",
      "Epoch 68/262\n",
      "13/13 - 0s - 16ms/step - loss: 11892925923328.0000 - val_loss: 20391112212480.0000\n",
      "Epoch 69/262\n",
      "13/13 - 0s - 16ms/step - loss: 12141426900992.0000 - val_loss: 20274340691968.0000\n",
      "Epoch 70/262\n",
      "13/13 - 0s - 17ms/step - loss: 11998453563392.0000 - val_loss: 20222633312256.0000\n",
      "Epoch 71/262\n",
      "13/13 - 0s - 19ms/step - loss: 11123346636800.0000 - val_loss: 20327012761600.0000\n",
      "Epoch 72/262\n",
      "13/13 - 0s - 22ms/step - loss: 11218446188544.0000 - val_loss: 20785053827072.0000\n",
      "Epoch 73/262\n",
      "13/13 - 0s - 17ms/step - loss: 11210989764608.0000 - val_loss: 20941618806784.0000\n",
      "Epoch 74/262\n",
      "13/13 - 0s - 16ms/step - loss: 10776546902016.0000 - val_loss: 20208248946688.0000\n",
      "Epoch 75/262\n",
      "13/13 - 0s - 16ms/step - loss: 10688072253440.0000 - val_loss: 20423278329856.0000\n",
      "Epoch 76/262\n",
      "13/13 - 0s - 17ms/step - loss: 10808714067968.0000 - val_loss: 20458762141696.0000\n",
      "Epoch 77/262\n",
      "13/13 - 0s - 16ms/step - loss: 10876223488000.0000 - val_loss: 20058917044224.0000\n",
      "Epoch 78/262\n",
      "13/13 - 0s - 16ms/step - loss: 10832942465024.0000 - val_loss: 20452621680640.0000\n",
      "Epoch 79/262\n",
      "13/13 - 0s - 16ms/step - loss: 10314937532416.0000 - val_loss: 21185658093568.0000\n",
      "Epoch 80/262\n",
      "13/13 - 0s - 15ms/step - loss: 10121605283840.0000 - val_loss: 20389184929792.0000\n",
      "Epoch 81/262\n",
      "13/13 - 0s - 17ms/step - loss: 10141359407104.0000 - val_loss: 20699445985280.0000\n",
      "Epoch 82/262\n",
      "13/13 - 0s - 16ms/step - loss: 9951008260096.0000 - val_loss: 20556493619200.0000\n",
      "Epoch 83/262\n",
      "13/13 - 0s - 16ms/step - loss: 10052824989696.0000 - val_loss: 20130966798336.0000\n",
      "Epoch 84/262\n",
      "13/13 - 0s - 16ms/step - loss: 9945961463808.0000 - val_loss: 20795766079488.0000\n",
      "Epoch 85/262\n",
      "13/13 - 0s - 17ms/step - loss: 9906450071552.0000 - val_loss: 20986476888064.0000\n",
      "Epoch 86/262\n",
      "13/13 - 0s - 16ms/step - loss: 9575555137536.0000 - val_loss: 20321123958784.0000\n",
      "Epoch 87/262\n",
      "13/13 - 0s - 17ms/step - loss: 8837442568192.0000 - val_loss: 21143610195968.0000\n",
      "Epoch 88/262\n",
      "13/13 - 0s - 16ms/step - loss: 9023584731136.0000 - val_loss: 19589763170304.0000\n",
      "Epoch 89/262\n",
      "13/13 - 0s - 16ms/step - loss: 9164965281792.0000 - val_loss: 20131900030976.0000\n",
      "Epoch 90/262\n",
      "13/13 - 0s - 16ms/step - loss: 8659700547584.0000 - val_loss: 21442055897088.0000\n",
      "Epoch 91/262\n",
      "13/13 - 0s - 17ms/step - loss: 8981612331008.0000 - val_loss: 21521965776896.0000\n",
      "Epoch 92/262\n",
      "13/13 - 0s - 17ms/step - loss: 8885073084416.0000 - val_loss: 21127011237888.0000\n",
      "Epoch 93/262\n",
      "13/13 - 0s - 16ms/step - loss: 8741268750336.0000 - val_loss: 21502411931648.0000\n",
      "Epoch 94/262\n",
      "13/13 - 0s - 17ms/step - loss: 8530295783424.0000 - val_loss: 20987678556160.0000\n",
      "Epoch 95/262\n",
      "13/13 - 0s - 16ms/step - loss: 8348244639744.0000 - val_loss: 20683165794304.0000\n",
      "Epoch 96/262\n",
      "13/13 - 0s - 17ms/step - loss: 8204857114624.0000 - val_loss: 21382326910976.0000\n",
      "Epoch 97/262\n",
      "13/13 - 0s - 17ms/step - loss: 8188394995712.0000 - val_loss: 20446170841088.0000\n",
      "Epoch 98/262\n",
      "13/13 - 0s - 16ms/step - loss: 7989738602496.0000 - val_loss: 20534687432704.0000\n",
      "Epoch 99/262\n",
      "13/13 - 0s - 16ms/step - loss: 8118612787200.0000 - val_loss: 20787109036032.0000\n",
      "Epoch 100/262\n",
      "13/13 - 0s - 17ms/step - loss: 8093135011840.0000 - val_loss: 21049106235392.0000\n",
      "Epoch 101/262\n",
      "13/13 - 0s - 24ms/step - loss: 8207492186112.0000 - val_loss: 20627803078656.0000\n",
      "Epoch 102/262\n",
      "13/13 - 0s - 16ms/step - loss: 7825012555776.0000 - val_loss: 21330634211328.0000\n",
      "Epoch 103/262\n",
      "13/13 - 0s - 17ms/step - loss: 7999478824960.0000 - val_loss: 20777778806784.0000\n",
      "Epoch 104/262\n",
      "13/13 - 0s - 17ms/step - loss: 7960608636928.0000 - val_loss: 20868218486784.0000\n",
      "Epoch 105/262\n",
      "13/13 - 0s - 16ms/step - loss: 7630678392832.0000 - val_loss: 20713230565376.0000\n",
      "Epoch 106/262\n",
      "13/13 - 0s - 17ms/step - loss: 7995562393600.0000 - val_loss: 21031490158592.0000\n",
      "Epoch 107/262\n",
      "13/13 - 0s - 16ms/step - loss: 7537088790528.0000 - val_loss: 21686516711424.0000\n",
      "Epoch 108/262\n",
      "13/13 - 0s - 19ms/step - loss: 7541997699072.0000 - val_loss: 21641415360512.0000\n",
      "Epoch 109/262\n",
      "13/13 - 0s - 17ms/step - loss: 7782740262912.0000 - val_loss: 22255960588288.0000\n",
      "Epoch 110/262\n",
      "13/13 - 0s - 16ms/step - loss: 7495491780608.0000 - val_loss: 20234660478976.0000\n",
      "Epoch 111/262\n",
      "13/13 - 0s - 16ms/step - loss: 7487183912960.0000 - val_loss: 21778852216832.0000\n"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor = 'val_loss', patience = int(patience), restore_best_weights = True)\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_split = .2, batch_size = batch_size, epochs = epochs, callbacks = es, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n"
     ]
    }
   ],
   "source": [
    "test_preds = model.predict(X_test)\n",
    "train_preds = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Neural Network Metrics:\n",
      "Train RMSE: 3015039.9241755\n",
      "Test RMSE: 4843637.290906582\n"
     ]
    }
   ],
   "source": [
    "train_rmse = mean_squared_error(y_train, train_preds)\n",
    "test_rmse = mean_squared_error(y_test, test_preds)\n",
    "\n",
    "print('Deep Neural Network Metrics:')\n",
    "print(f'Train RMSE: {np.sqrt(train_rmse)}')\n",
    "print(f'Test RMSE: {np.sqrt(test_rmse)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model, pca, and preprocessor so that new data can be fit using the same criteria\n",
    "#model.save('best_model/best_model.keras')\n",
    "#joblib.dump(pca, 'dnn/pca45.joblib')\n",
    "#joblib.dump(preprocessor, 'best_model/best_preprocssor.joblib')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
